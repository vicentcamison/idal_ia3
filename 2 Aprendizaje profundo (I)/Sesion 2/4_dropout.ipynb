{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/2%20Aprendizaje%20profundo%20(I)/Sesion%202/4_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Di92jBvswB"
      },
      "source": [
        "# Implementation of Batch Normalization and Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Zq7BEhv2S4"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Función Sigmoide\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivate(sig_output):\n",
        "    return sig_output*(1-sig_output)\n",
        "\n",
        "# Entrada y salida: Tamaño entrada = (4, 3), Tamaño salida = (4, 1)\n",
        "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Variables\n",
        "alpha = .5\n",
        "hidden_dim = 4\n",
        "dropout_percent = .2\n",
        "do_dropout = True\n",
        "weights_layer0 = 2 * np.random.random((3, hidden_dim)) - 1  # Tamaño (3, 4)\n",
        "weights_layer1 = 2 * np.random.random((hidden_dim, 1)) - 1  # Tamaño (4, 1)\n",
        "\n",
        "for j in range(100):\n",
        "    # Fordward pass Capa 1\n",
        "    layer_1 = sigmoid(np.dot(X, weights_layer0))\n",
        "\n",
        "\n",
        "    if do_dropout:\n",
        "        # Aplicamos dropout\n",
        "        drop_out_mask1 = np.ones((len(X), hidden_dim), dtype=int) # Tamaño (4, 4)\n",
        "        drop_out_mask1 = np.random.binomial(drop_out_mask1, 1-dropout_percent)\n",
        "        scale = 1/(1-dropout_percent)\n",
        "        layer_1 *= drop_out_mask1 * scale #esta 'renormalización' de la capa sirve para reescalar los valores numéricos,\n",
        "        # los cuales serían más pequeños de normal que si no estuviera la capa (es decir, compensar los 0 haciendo que el\n",
        "        # resto de pesos sea mayor proporcionalmente)\n",
        "\n",
        "\n",
        "    # Forward pass Capa 2\n",
        "    layer_2 = sigmoid(np.dot(layer1, weights_layer1))\n",
        "\n",
        "\n",
        "    # Retro-propagación\n",
        "    error = layer_2 - y\n",
        "    layer_2_delta = error * sigmoid_derivate(layer2)\n",
        "    layer_1_delta = np.dot(layer_2_delta, weights_layer1.T) * sigmoid_derivate(layer1)\n",
        "  \n",
        "    \n",
        "    # Actualización de los pesos\n",
        "    weights_layer1 -= alpha * np.dot(layer_1.T, layer2_delta)\n",
        "    weights_layer0 -= alpha * np.dot(X.T, layer_1_delta)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}