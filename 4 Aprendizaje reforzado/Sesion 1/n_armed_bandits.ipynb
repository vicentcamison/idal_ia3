{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "n_armed_bandits.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/4%20Aprendizaje%20reforzado/Sesion%201/n_armed_bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pN4I8rRhAIK"
      },
      "source": [
        "# N-Armed Bandit\n",
        "\n",
        "Vamos a analizar en este cuaderno el comportamiento del compromiso entre exploración y explotación.\n",
        "Para ello vamos a implementar un N-armed Bandit y estudiaremos diferentes estrategias de apuesta:\n",
        "Una estrategia greedy\n",
        "Dos estrategias epsilon-greedy\n",
        "\n",
        "Visualizaremos los resultados de las gráficas de recompensa para ver su evolución\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IzspQaChAIW"
      },
      "source": [
        "# import modules \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "%matplotlib inline\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgcGsNMhAIa"
      },
      "source": [
        "#### Ejercicio 1.1:  \n",
        "Completa el código con la línea en '??'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjoJWzCRhAIc"
      },
      "source": [
        "class eps_bandit:\n",
        "    '''\n",
        "    epsilon-greedy k-bandit problem\n",
        "    \n",
        "    Inputs\n",
        "    =====================================================\n",
        "    k: number of arms (int)\n",
        "    eps: probability of random action 0 < eps < 1 (float)\n",
        "    iters: number of steps (int)\n",
        "    mu: set the average rewards for each of the k-arms.\n",
        "        Set to \"random\" for the rewards to be selected from\n",
        "        a normal distribution with mean = 0. \n",
        "        Set to \"sequence\" for the means to be ordered from \n",
        "        0 to k-1.\n",
        "        Pass a list or array of length = k for user-defined\n",
        "        values.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, k, eps, iters, mu='random'):\n",
        "        # Number of arms\n",
        "        self.k = k\n",
        "        # Search probability\n",
        "        self.eps = eps\n",
        "        # Number of iterations\n",
        "        self.iters = iters\n",
        "        # Step count\n",
        "        self.n = 0\n",
        "        # Step count for each arm\n",
        "        self.k_n = np.zeros(k)\n",
        "        # Total mean reward\n",
        "        self.mean_reward = 0\n",
        "        self.reward = np.zeros(iters)\n",
        "        \n",
        "        #ESTA ES LA DEFINICION DE LA FUNCION Q(a) PARA CADA A (PARA CADA MAQUINA)\n",
        "        # Mean reward for each arm\n",
        "        self.k_reward = np.zeros(k)\n",
        "        \n",
        "        if type(mu) == list or type(mu).__module__ == np.__name__:\n",
        "            # User-defined averages            \n",
        "            self.mu = np.array(mu)\n",
        "        elif mu == 'random':\n",
        "            # Draw means from probability distribution\n",
        "            self.mu = np.random.normal(0, 1, k)\n",
        "        elif mu == 'sequence':\n",
        "            # Increase the mean for each arm by one\n",
        "            self.mu = np.linspace(0, k-1, k)    #<<<---------Ejercicio 1.4\n",
        "  \n",
        "  \n",
        "    #EJECUTA UNA ACCION\n",
        "  \n",
        "    def pull(self):\n",
        "        # Generate random number\n",
        "        p = np.random.rand()\n",
        "        \n",
        "        #DEFINICION DE LAS POLITICAS\n",
        "        #PARA LA POLITICA GREEDY, SE ELIJE EN LA PRIMERA ITERACION\n",
        "        if self.eps == 0 and self.n == 0:\n",
        "            a = np.random.choice(self.k)\n",
        "        elif p < self.eps:\n",
        "            # Randomly select an action\n",
        "            a = np.random.choice(self.k)\n",
        "        else:\n",
        "            # Take greedy action\n",
        "            a = np.argmax(self.k_reward)\n",
        "         \n",
        "        #Calculo de la recompensa\n",
        "        reward = np.random.normal(self.mu[a], 1)\n",
        "        \n",
        "        # Update counts\n",
        "        self.n += 1\n",
        "        self.k_n[a] += 1\n",
        "        \n",
        "        # Update total\n",
        "        self.mean_reward = self.mean_reward + (\n",
        "            reward - self.mean_reward) / self.n\n",
        "        \n",
        "        #ESTA INSTRUCCION ES LA ACTUALIZACION DE  LA FUNCION Q(a)\n",
        "        #USANDO LA FORMULA ITERATIVA DE CALCULAR LA MEDIA\n",
        "        # Update results for a_k\n",
        "        self.k_reward[a] = ??\n",
        "        \n",
        "    #ESTA FUNCION EJECUTA LA ANTERIOR EL NUMERO DE ITERACIONES ESTABLECIDO\n",
        "    def run(self):\n",
        "        for i in range(self.iters):\n",
        "            self.pull()\n",
        "            self.reward[i] = self.mean_reward\n",
        "            \n",
        "    def reset(self):\n",
        "        # Resets results while keeping settings\n",
        "        self.n = 0\n",
        "        self.k_n = np.zeros(k)\n",
        "        self.mean_reward = 0\n",
        "        self.reward = np.zeros(iters)\n",
        "        self.k_reward = np.zeros(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PWCUaqDhAIe"
      },
      "source": [
        "#VAMOS A HACER TRES EXPERIMENTOS: \n",
        "#1. Política Greedy\n",
        "#2. Política Epsilon-Greedy con epsilon = 0.01\n",
        "#3. Política epsilon-greedy con epsilon = 0.1\n",
        "#LOS VALORES DE LAS MEDIAS SE ELIGEN ENTRE 0 y 1 CON UNA gausiana\n",
        "\n",
        "k = 10\n",
        "iters = 1000\n",
        "\n",
        "#GENERAMOS ESTRUCTURAS PARA LOS TRES EXPERIMENTOS QUE VAMOS A HACER\n",
        "#ESTAS VARIABLES ACUMULARAN LA MEDIA DE LAREWARD OBTENIDA EN CADA ITERACION\n",
        "#ES DECIR eps_0_rewards[5] ACUMULA LA MEDIA DE LOS VALORES DE REWARD \n",
        "#EN LA ITERACION 5 CALCULADA SOBRE 1000 EJECUCIONES\n",
        "#ESTOS VALORES SON LOS QUE SE REPRESENTAN EN LAS GRAFICAS\n",
        "eps_0_rewards = np.zeros(iters)\n",
        "eps_01_rewards = np.zeros(iters)\n",
        "eps_1_rewards = np.zeros(iters)\n",
        "\n",
        "#SE HACEN 1000 EJECUCIONES (episodes)  DE LOS EXPERIMENTOS \n",
        "#CON 1000 INTENTOS (ACCIONES) CADA UNO\n",
        "episodes = 1000\n",
        "# Run experiments\n",
        "for i in range(episodes):\n",
        "    # Initialize bandits\n",
        "    eps_0 = eps_bandit(k, 0, iters)\n",
        "    eps_01 = eps_bandit(k, 0.01, iters, eps_0.mu.copy())\n",
        "    eps_1 = eps_bandit(k, 0.1, iters, eps_0.mu.copy())\n",
        "    \n",
        "    # Run experiments\n",
        "    eps_0.run()\n",
        "    eps_01.run()\n",
        "    eps_1.run()\n",
        "    \n",
        "    # Update long-term averages\n",
        "    #Son vectores de longitud 'iters' que almacenan  el valor medio de cada iteracion para 1000 episodes\n",
        "    #donde cada episode es un conjunto de 'iters' acciones\n",
        "    #ES DECIR eps_0_rewards[5] ACUMULA LA MEDIA DE LOS VALORES DE REWARD \n",
        "    #EN LA ITERACION 5 CALCULADA SOBRE 1000 EJECUCIONES\n",
        "    #ESTOS VALORES SON LOS QUE SE REPRESENTAN EN LAS GRAFICAS\n",
        "    eps_0_rewards = eps_0_rewards + (\n",
        "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
        "    eps_01_rewards = eps_01_rewards + (\n",
        "        eps_01.reward - eps_01_rewards) / (i + 1)\n",
        "    eps_1_rewards = eps_1_rewards + (\n",
        "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
        "    \n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(eps_0_rewards, label=\"$\\epsilon=0$ (greedy)\")\n",
        "plt.plot(eps_01_rewards, label=\"$\\epsilon=0.01$\")\n",
        "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
        "plt.legend(bbox_to_anchor=(1.3, 0.5))\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Average $\\epsilon-greedy$ Rewards after \" + str(episodes) \n",
        "    + \" Episodes\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jte3dyH7hAIf"
      },
      "source": [
        "#### Ejercicio 1.2: \n",
        "Fíjate en la gráfica de epsilon =0. ¿por qué no es una línea horizontal desde el primer momento si siempre en un episodio tiene el mismo valor?. Responde a continuación en esta caja.\n",
        "\n",
        "\n",
        "\n",
        "#### Ejercicio 1.3: \n",
        "¿qué sería necesario que cambiara para que la politica epsilon =0.01 tuviese mejor resultado que la epsilon=0.1. Responde a continuación\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8HHMZSRhAIg"
      },
      "source": [
        "#AHORA LAS REWARDS MEDIAS DE CADA ACCION VAN DE 0 a 9 \n",
        "#(0, para la accion 0, 1 para la  accion 1 etc.)\n",
        "k = 10\n",
        "iters = 1000\n",
        "\n",
        "#ESTAS VARIABLES ACUMULARAN LA MEDIA DE LA REWARD OBTENIDA EN CADA ITERACION\n",
        "#ES DECIR eps_0_rewards[5] ACUMULA LA MEDIA DE LOS VALORES DE REWARD \n",
        "#EN LA ITERACION 5 CALCULADA SOBRE 1000 EJECUCIONES\n",
        "#ESTOS VALORES SON LOS QUE SE REPRESENTAN EN LAS GRAFICAS\n",
        "eps_0_rewards = np.zeros(iters)\n",
        "eps_01_rewards = np.zeros(iters)\n",
        "eps_1_rewards = np.zeros(iters)\n",
        "\n",
        "\n",
        "eps_0_selection = np.zeros(k)\n",
        "eps_01_selection = np.zeros(k)\n",
        "eps_1_selection = np.zeros(k)\n",
        "\n",
        "#SE HACEN 1000 EJECUCIONES (episodes)  DE LOS EXPERIMENTOS \n",
        "#CON 1000 INTENTOS (ACCIONES) CADA UNO\n",
        "episodes = 1000\n",
        "# Run experiments\n",
        "for i in range(episodes):\n",
        "    # Initialize bandits\n",
        "    eps_0 = eps_bandit(k, 0, iters, mu='sequence')\n",
        "    eps_01 = eps_bandit(k, 0.01, iters, eps_0.mu.copy())\n",
        "    eps_1 = eps_bandit(k, 0.1, iters, eps_0.mu.copy())\n",
        "    \n",
        "    # Run experiments\n",
        "    eps_0.run()\n",
        "    eps_01.run()\n",
        "    eps_1.run()\n",
        "    \n",
        "    # Update long-term averages\n",
        "    eps_0_rewards = eps_0_rewards + (\n",
        "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
        "    eps_01_rewards = eps_01_rewards + (\n",
        "        eps_01.reward - eps_01_rewards) / (i + 1)\n",
        "    eps_1_rewards = eps_1_rewards + (\n",
        "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
        "    \n",
        "    # Average actions per episode\n",
        "    eps_0_selection = eps_0_selection + (\n",
        "        eps_0.k_n - eps_0_selection) / (i + 1)\n",
        "    eps_01_selection = eps_01_selection + (\n",
        "        eps_01.k_n - eps_01_selection) / (i + 1)\n",
        "    eps_1_selection = eps_1_selection + (\n",
        "        eps_1.k_n - eps_1_selection) / (i + 1)\n",
        "    \n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(eps_0_rewards, label=\"$\\epsilon=0$ (greedy)\")\n",
        "plt.plot(eps_01_rewards, label=\"$\\epsilon=0.01$\")\n",
        "plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
        "for i in range(k):\n",
        "    plt.hlines(eps_0.mu[i], xmin=0,\n",
        "              xmax=iters, alpha=0.5,\n",
        "              linestyle=\"--\")\n",
        "plt.legend(bbox_to_anchor=(1.3, 0.5))\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Average $\\epsilon-greedy$ Rewards after \" + \n",
        "     str(episodes) + \" Episodes\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AJHFIrthAIh"
      },
      "source": [
        "#### Ejercicio 1.4: \n",
        "¿En qué influyen el valor de las recompensas en la forma de las gráficas? ¿Por qué?\n",
        "\n",
        "\n",
        "#### Ejercicio 1.5:\n",
        "Modifica el valor de las recompensas cambiando los argumentos de la función señalada en el código de la segunda caja de python (señalada con <<<-------Ejercicio 1.5) para tener unos valores más grandes de recompensas y vuelve a ejecutar el codigo para obtener otrras gráficas en este apartado. ¿Cambia la situación, en qué sentido? ¿Qué puedes concluir acerca del valor absoluto de las recompensas usadas? Responde a continuación:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHW0U4XshAIh"
      },
      "source": [
        "#### Ejercicio 1.6:\n",
        "Implementa la función de exploración Softmax. Experimenta con diferentes temperaturas (0.5,1,5,10,...) y observa las gráficas que salen. \n",
        "Compáralas con las obtenidas para epsilon-greedy. Escribe tus conclusiones a continuación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUhbJxK8hAIj"
      },
      "source": [
        "#Implementa aquí el problema con Softmax. Copia y pega del codigo anterior \n",
        "#todo salvo la implementación y uso de SoftMax."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}