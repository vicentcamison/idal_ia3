{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_v2_adv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/4%20Aprendizaje%20reforzado/Sesion%203/DQN_v2_adv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWsxkki8jh8A"
      },
      "source": [
        "# DQN v2 advantage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSeTTLadjXBg"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import random\n",
        "from collections import deque\n",
        "import datetime"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9iGqQ5XTTg2"
      },
      "source": [
        "# Abrimos el archivo utils.py desde el github\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/vicentcamison/idal_ia3/main/4%20Aprendizaje%20reforzado/Sesion%203/utils.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('utils.py', 'w') as f:\n",
        "    f.write(r.text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIUYXs_YTWcT"
      },
      "source": [
        "# Abrimos el archivo utils.py desde el github\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/vicentcamison/idal_ia3/main/4%20Aprendizaje%20reforzado/Sesion%203/dqn_approximators.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('dqn_approximators.py', 'w') as f:\n",
        "    f.write(r.text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "7jRH5V0ZUEFu",
        "outputId": "85c1e66d-2f7a-4d64-d3dd-460da8970566"
      },
      "source": [
        "from utils import discrete_input\n",
        "from dqn_approximators import DqnAdv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1e7bb045c5fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiscrete_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdqn_approximators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDqnAdv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/dqn_approximators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnoisy_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoisyLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoisyLayer2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'noisy_layer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn0-uTtqkPeC"
      },
      "source": [
        "Disable GPU computation for local devices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-dueBMmj8QY"
      },
      "source": [
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gvh2VHsje3w"
      },
      "source": [
        "## Gym selection and basic configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A722SC0km68"
      },
      "source": [
        "gym_name_list = [\n",
        "    {\n",
        "        'name': 'CartPole-v0',\n",
        "        'goal': 180,\n",
        "        'v_min': 0,\n",
        "        'v_max': 210,\n",
        "        'ep': 50\n",
        "    },\n",
        "    {\n",
        "        'name': 'MountainCar-v0',\n",
        "        'goal': -150, \n",
        "        'v_min': -210,\n",
        "        'v_max': 0,\n",
        "        'ep': 20\n",
        "    },\n",
        "    {\n",
        "        'name': 'Blackjack-v0',\n",
        "        'goal': 0.10,\n",
        "        'v_min': -20,\n",
        "        'v_max': 20,\n",
        "        'ep': 1000\n",
        "    }\n",
        "]\n",
        "\n",
        "env_i = 0 #@param {type:\"slider\", min:0, max:2, step:1}\n",
        "\n",
        "save_model: bool = False # @param {type:\"boolean\"}\n",
        "show_plots: bool = True # @param {type:\"boolean\"}\n",
        "render_env: bool = False # @param {type:\"boolean\"}\n",
        "seed = 42 # @param {type:\"integer\"}\n",
        "\n",
        "max_steps_per_episode = 200 # @param {type:\"integer\"}\n",
        "\n",
        "stopping_reward_criteria = gym_name_list[env_i]['goal']\n",
        "\n",
        "gym_name = gym_name_list[env_i]['name']\n",
        "\n",
        "env = gym.make(gym_name)  # Create the environment\n",
        "env.seed(seed)\n",
        "\n",
        "if isinstance(env.observation_space, gym.spaces.tuple.Tuple):\n",
        "    env = gym.wrappers.TransformObservation(env, lambda obs: discrete_input(obs, env.observation_space))\n",
        "    num_inputs = sum([x.n for x in env.observation_space])  # 4\n",
        "else:\n",
        "    num_inputs = env.observation_space.shape[0]  # 4\n",
        "num_actions = env.action_space.n  # 2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWXm85B4lIJ7"
      },
      "source": [
        "## Algorithm hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqDFkMCflFY-"
      },
      "source": [
        "# Factor of the ema that displays that tracks the averaged rewards\n",
        "ema_ratio = 0.01  # @param {type:\"number\"}\n",
        "\n",
        "# Ratio between generating experiences and sampling for training\n",
        "training_ratio: int = 4 # @param  {type:\"integer\"}\n",
        "\n",
        "# Size of the batch when sampling experiences\n",
        "batch_size: int = 32 # @param {type:\"integer\"}\n",
        "\n",
        "# Size of the buffer that stores the experiences\n",
        "mem_length: int = 2048 # @param {type:\"integer\"}\n",
        "\n",
        "# Discount factor for estimaing the futures rewards\n",
        "gamma: float = 0.99  # @param {type:\"number\"}\n",
        "\n",
        "# Initial and last probability for choosing exploration instead of explotation\n",
        "epsilon: float = 1.0 # @param {type:\"number\"}\n",
        "epsilon_min: float = 0.05 # @param {type:\"number\"}\n",
        "\n",
        "# This is an estimation of the training iterations to tune the epsilon decay\n",
        "approx_iterations: float = 1e6 # @param {type:\"number\"}\n",
        "\n",
        "# The epsilon_decay reduce the exploration probability after each iteration\n",
        "epsilon_decay: float = (epsilon_min / epsilon) ** (1 / approx_iterations)\n",
        "\n",
        "# Factor of the ema that controls the updating weights of the target network\n",
        "tau: float = 0.125 # @param {type:\"number\"}\n",
        "\n",
        "# The usual factor that controls the amount of change the weights are updated\n",
        "learning_rate = 0.05 # @param {type:\"number\"}\n",
        "\n",
        "# For enabling the double dqn learning when choosing next Q-values\n",
        "double_dqn_learning: bool = True # @param {type:\"boolean\"}\n",
        "\n",
        "# Factor to define heuristically the size of the hidden layer.\n",
        "hidden_size_factor = 16 # @param {type:\"integer\"}\n",
        "num_hidden = num_inputs * num_actions * hidden_size_factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT3aznNck22R"
      },
      "source": [
        "## Load DQN models as Q-table approximators\n",
        "We start with double DQN, where:\n",
        "*   q_model estimates the Q-values used for action selection.\n",
        "*   t_model is responsible for estimating the target Q values on training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq3oaCbNk17Y"
      },
      "source": [
        "q_model = DqnAdv(num_inputs=num_inputs,\n",
        "                 num_actions=num_actions,\n",
        "                 num_hidden=num_hidden,\n",
        "                 name=\"q_model\")\n",
        "\n",
        "t_model = DqnAdv(num_inputs=num_inputs,\n",
        "                 num_actions=num_actions,\n",
        "                 num_hidden=num_hidden,\n",
        "                 name=\"t_model\")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "# loss_function = keras.losses.Huber()\n",
        "loss_function = keras.losses.MeanSquaredError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2H04rFNqdNg"
      },
      "source": [
        "## Tensorboard configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyP6U0yKq-Ez"
      },
      "source": [
        "implementation = \"DQN_v2\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = os.path.join(\"logs\", gym_name, implementation, \"T_\" + current_time)\n",
        "summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "VkJuXmNph9wb",
        "outputId": "458de9b0-3d76-4b2b-893a-fc8cad44ffb7"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# from tensorboard import notebook\n",
        "# notebook.list() # View open TensorBoard instances\n",
        "\n",
        "# # Control TensorBoard display. If no port is provided, \n",
        "# # the most recently launched TensorBoard is used\n",
        "# notebook.display(port=6006, height=1000) \n",
        "\n",
        "%tensorboard --logdir ./logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Budp8owKeX"
      },
      "source": [
        "## Main learning loop\n",
        "It consist in main two steps:\n",
        "\n",
        "1.   Collect experiences from the environment in episodes, with exploitation/exploration trade-off\n",
        "2.   Sample past experiences to train the model using the Bellman equation.\n",
        "\n",
        "Other sections are the mean reward tracking, stop-learning trigger, display status.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "QG_ZMJ0NwVNl",
        "outputId": "c5bfcebe-9aa1-4c1c-ab94-91358260823d"
      },
      "source": [
        "memory = deque(maxlen=mem_length)\n",
        "running_reward = None\n",
        "episode_count = 0\n",
        "epoch = 0\n",
        "historic_reward = []\n",
        "while True:  # Run until solved\n",
        "\n",
        "    state = env.reset()\n",
        "    state = tf.convert_to_tensor(state)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    episode_reward = 0\n",
        "    for time_step in range(1, max_steps_per_episode):\n",
        "        # env.render(); Adding this line would show the attempts\n",
        "        # of the agent in a pop up window.\n",
        "\n",
        "        epsilon *= epsilon_decay\n",
        "        epsilon = max(epsilon_min, epsilon)\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_model(state, training=False))\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        next_state = tf.convert_to_tensor(next_state)\n",
        "        next_state = tf.expand_dims(next_state, 0)\n",
        "\n",
        "        memory.append([state, action, reward, next_state, done])\n",
        "        episode_reward += reward\n",
        "\n",
        "        # ##### TRAIN MODEL ###############\n",
        "        if len(memory) >= 2 * batch_size and time_step % training_ratio == 0:\n",
        "            samples = random.sample(memory, batch_size)\n",
        "\n",
        "            state_batch = tf.concat([s for s, a, r, n_s, d in samples], axis=0)\n",
        "            action_batch = tf.concat([a for s, a, r, n_s, d in samples], axis=0)\n",
        "            reward_batch = tf.concat([r for s, a, r, n_s, d in samples], axis=0)\n",
        "            next_state_batch = tf.concat([n_s for s, a, r, n_s, d in samples], axis=0)\n",
        "            not_done_batch = tf.concat([float(not d) for s, a, r, n_s, d in samples], axis=0)\n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_batch, num_actions)\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            future_t = t_model(next_state_batch)\n",
        "\n",
        "            if double_dqn_learning:\n",
        "                future_q = q_model(next_state_batch)\n",
        "                best_future_action = tf.argmax(future_q, axis=-1)\n",
        "                next_action_mask = tf.one_hot(best_future_action, num_actions)\n",
        "                future_q_action = tf.reduce_sum(tf.multiply(future_t, next_action_mask), axis=1)\n",
        "            else:\n",
        "                future_q_action = tf.reduce_max(future_t, axis=1)\n",
        "\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = reward_batch + gamma * tf.multiply(future_q_action, not_done_batch)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                q_values = q_model(state_batch)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                loss = loss_function(q_action, updated_q_values)\n",
        "\n",
        "                # Backpropagation\n",
        "                grads = tape.gradient(loss, q_model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, q_model.trainable_variables))\n",
        "\n",
        "            # Transfer weights to target model\n",
        "            weights = q_model.get_weights()\n",
        "            target_weights = t_model.get_weights()\n",
        "            for j in range(len(target_weights)):\n",
        "                target_weights[j] = weights[j] * tau + target_weights[j] * (1 - tau)\n",
        "            t_model.set_weights(target_weights)\n",
        "            # ###############################################\n",
        "\n",
        "            if running_reward is not None:\n",
        "                with summary_writer.as_default():\n",
        "                    tf.summary.scalar('loss', loss.numpy(), step=epoch)\n",
        "                    tf.summary.scalar('ema_reward', running_reward, step=epoch)\n",
        "                    tf.summary.scalar('epsilon', epsilon, step=epoch)\n",
        "                epoch += 1\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if running_reward is None:\n",
        "        running_reward = episode_reward\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    running_reward = ema_ratio * episode_reward + (1 - ema_ratio) * running_reward\n",
        "    historic_reward.append(running_reward)\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % gym_name_list[env_i]['ep'] == 0 and 'loss' in locals():\n",
        "        template = \"running reward: {:.2f} at episode {} with epsilon {:.2f} and loss {:.2f}\"\n",
        "        print(template.format(running_reward, episode_count, epsilon, loss))\n",
        "\n",
        "    if running_reward > stopping_reward_criteria:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break\n",
        "\n",
        "    if show_plots and episode_count % 10000000 == 0:\n",
        "        plt.plot(historic_reward)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running reward: 66.70 at episode 24850 with epsilon 0.05 and loss 205.92\n",
            "running reward: 63.69 at episode 24900 with epsilon 0.05 and loss 3.44\n",
            "running reward: 54.40 at episode 24950 with epsilon 0.05 and loss 19.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bbb6c9b5662b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;31m# Apply the masks to the Q-values to get the Q-value for action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mq_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdated_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m--> 154\u001b[0;31m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m_get_reduction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;34m\"\"\"Handles `AUTO` reduction cases and returns the reduction value.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     if (not self._allow_sum_over_batch_size and\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         (self.reduction == losses_utils.ReductionV2.AUTO or\n\u001b[1;32m    193\u001b[0m          self.reduction == losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE)):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribution_strategy_context.py\u001b[0m in \u001b[0;36mhas_strategy\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minside\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_get_default_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribution_strategy_context.py\u001b[0m in \u001b[0;36mget_strategy\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrategy\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_get_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribution_strategy_context.py\u001b[0m in \u001b[0;36m_get_per_thread_mode\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_default_replica_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   6020\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6021\u001b[0m   \"\"\"\n\u001b[0;32m-> 6022\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RoY6P3OzRyy"
      },
      "source": [
        "## Save model weights for a later use (optional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We2l8xxlznvz"
      },
      "source": [
        "if save_model:\n",
        "    model_folder = os.path.join(\"./models\", gym_name, implementation, \"Ep_\" + str(episode_count).zfill(5), \"model\")\n",
        "    if not os.path.exists(model_folder):\n",
        "        os.makedirs(model_folder)\n",
        "    q_model.save_weights(filepath=model_folder, save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwgdrS-SzuCZ"
      },
      "source": [
        "## Play with the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akp6d5gdz2zj"
      },
      "source": [
        "episodes = 100 # @param {type:\"integer\"}\n",
        "agent_rewards = []\n",
        "for env_i in range(episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    for time_step in range(1, max_steps_per_episode):\n",
        "        if render_env and gym_name != 'Blackjack-v0':\n",
        "            env.render()  # Show the attempts of the agent in a pop up window.\n",
        "\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "\n",
        "        action = np.argmax(q_model(state))\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    agent_rewards.append(episode_reward)\n",
        "  \n",
        "print(f\"After 100 episodes the mean reward is {np.mean(agent_rewards)}\")\n",
        "\n",
        "if show_plots:\n",
        "    num_bins = 50\n",
        "    x = np.array(agent_rewards)\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # the histogram of the data\n",
        "    n, bins, patches = ax.hist(x, num_bins, density=1)\n",
        "\n",
        "    ax.set_xlabel('Episode rewards')\n",
        "    ax.set_ylabel('Probability density')\n",
        "    ax.set_title(f'Mean {np.mean(x).round(2)} +/- {np.std(x).round(2)}')\n",
        "\n",
        "    # Tweak spacing to prevent clipping of ylabel\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"End of script!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}