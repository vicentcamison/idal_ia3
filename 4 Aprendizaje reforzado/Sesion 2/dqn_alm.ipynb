{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "dqn_alm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/4%20Aprendizaje%20reforzado/Sesion%202/dqn_alm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq4UlJfOyoxk"
      },
      "source": [
        "# DEEP Q_LEARNING: DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrRnPuMDyoxp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "7219fa97-1d54-4163-b326-d4dd35c78137"
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxCJEjz8yoxu"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"XXXXXXX\"\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        \"XXXXXX\"\n",
        "\n",
        "    def action(self, state):\n",
        "        \"XXXXX\"\n",
        "        return \"XXXXX\"\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9x0R8tUyoxx"
      },
      "source": [
        "env = gym.make('Acrobot-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "# agent.load(\"./save/cartpole-dqn.h5\")\n",
        "done = False\n",
        "batch_size = 32\n",
        "EPISODES=20\n",
        "rewards = []\n",
        "for e in range(EPISODES):\n",
        "    print(e)\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time in range(100):\n",
        "        print(time)\n",
        "        # env.render()\n",
        "        action = agent.action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        reward = reward if not done else -10\n",
        "        rewards.append(reward)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.store(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "       #if done:\n",
        "       #    print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "       #          .format(e, EPISODES, time, agent.epsilon))\n",
        "       #    break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.experience_replay(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dc1FrG_yox0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}