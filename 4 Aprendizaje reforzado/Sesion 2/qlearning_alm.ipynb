{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "qlearning_alm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/4%20Aprendizaje%20reforzado/Sesion%202/qlearning_alm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18rEPvbZ5hit"
      },
      "source": [
        "\n",
        "## Q-learning\n",
        "\n",
        "En este cuaderno se debe implementar QLearningAgent (sigue las instrucciones de cada método) y usarlo en una serie de pruebas a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntgR4SDF5hix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81ec0d1-aa93-46b4-ac38-ccdb283165f9"
      },
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "        \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: ../xvfb: No such file or directory\n",
            "env: DISPLAY=:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZX0Kqc5hi1"
      },
      "source": [
        "La implementación del agente se realizará a traves de la construcción de una clase llamada QLearningAgent. Esto se debe a que necesitaremos funciones intermedias que se comuniquen entre ellas aunque no es estrictamente necesario darle esta estructura. La clase tendra que tener las siguientes funciones.\n",
        "\n",
        "+ get_qvalue\n",
        "\n",
        "+ set_qvalue\n",
        "\n",
        "+ get_value\n",
        "\n",
        "+ update\n",
        "\n",
        "+ get_best_action\n",
        "\n",
        "+ get_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r5rX_weaJam"
      },
      "source": [
        "Antes de comenzar, conviene mencionar un pequeño 'truco'.\n",
        "\n",
        "Para construir una tabla muy grande, pero que no vayamos a utilizar/rellenar en ese momento, podemos utilizar el truco siguiente:\n",
        "\n",
        "```\n",
        "dic = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "```\n",
        "\n",
        "Esto genera una tabla vacía, que podemos ir rellenando cuando queramos y como queramos, sin que arroje ningún error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZv396SgaD-a",
        "outputId": "891afcb6-9554-475f-d61e-f2871e239639",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "dic = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "dic['clase'] = 7\n",
        "dic"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.<lambda>>, {'clase': 7})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WtbzP87awg7"
      },
      "source": [
        "Tal y como podemos ver, hemos inicializado un **defaultdict** vacío, y hemos podido posteriormente añadirle el elemento 'clase' con valor 7, sin que arroje ningún error.\n",
        "\n",
        "Por último, hemos comprobado que en el interior del defaultdict tenemos, efectivamente, la pareja de valores 'clase' = 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKM4iwMr5hi2"
      },
      "source": [
        "from collections import defaultdict\n",
        "import random, math\n",
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "\n",
        "        \"\"\"\n",
        "        Agente de Q-Learning\n",
        "        Variables de instancia a las que tiene acceso\n",
        "          - self.epsilon (problema de exploración)\n",
        "          - self.alpha (tasa de aprendizaje)\n",
        "          - self.discount (tasa de descuento aka gamma)\n",
        "\n",
        "        Funciones que debes usar\n",
        "          - self.get_legal_actions (state) {estado, hashable -> lista de acciones, cada una es hashable}\n",
        "            que devuelve acciones legales para un estado\n",
        "          - self.get_qvalue (estado, acción)\n",
        "            que devuelve Q (estado, acción)\n",
        "          - self.set_qvalue (estado, acción, valor)\n",
        "            que establece Q (estado, acción): = valor\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return \"XXXXX\"\n",
        "\n",
        "    def set_qvalue(self,state,action,value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        \"XXXXXX\"\n",
        "\n",
        "        \n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Calcula la estimación de V (s) del agente utilizando los valores q actuales\n",
        "        V (s) = max_over_action Q (estado, acción) sobre posibles acciones.\n",
        "        Nota: tenger en cuenta que los valores q pueden ser negativos.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #SI NO HAY ACCIONES POSIBLES DEVOLVEMOS 0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        #QUEDATE CON EL VALOR DE LA ACCION QUE MAXIMICE Q-VALUE\n",
        "        #TU CÓDIGO AQUI\n",
        "        value = \"XXXXXXXXX\"\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        ACTIALIZA EL Q-VALOR SEGUN LA FORMULA QUE SE PRESENTA:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * max(Q(s',a')))\n",
        "        \"\"\"\n",
        "\n",
        "        #PARAMETROS\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        #IMPLEMENTA LA FUNCION PRESENTADA EN LA DESCRIPCION DE ARRIBA\n",
        "        value = \"XXXXX\"\n",
        "        \n",
        "        self.set_qvalue(state, action, value)\n",
        "\n",
        "    \n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Calcula la mejor acción para tomar en un estado (utilizando los valores q actuales).\n",
        "        \"\"\"\n",
        "        #SELECCIONA LAS POSIBLES ACCIONES DADO EL ESTADO\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        #CALCULA LA MEJOR ACCION DADO EL ESTADO\n",
        "        best_action = None\n",
        "        best_q = float(\"-inf\")\n",
        "        for action in possible_actions:\n",
        "            cur_q = self.get_qvalue(state,action)\n",
        "            if cur_q > best_q:\n",
        "                best_q = cur_q\n",
        "                best_action  = action\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Calcula la acción a tomar en el estado actual, incluida la exploración.\n",
        "        Con probabilidad self.epsilon, deberíamos realizar una acción aleatoria.\n",
        "        de lo contrario - la mejor acción política (self.getPolicy).\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        #PARAMETROS\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        #SELECCIONA UNA ACCION TENIENDO EN CUENTA LA EXPLORACIÓN CON EL EPSILON\n",
        "        #TU CODIGO AQUI\n",
        "        \"XXXXXXXX\"\n",
        "\n",
        "        return chosen_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hKMwXeF5hi5"
      },
      "source": [
        "\n",
        "# Vamos a ver como funciona nuestro algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWhgM3mT5hi6"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\") #gym.make(\"CliffWalking-v0\")\n",
        "\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AosRIYnF5hi9"
      },
      "source": [
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFNsESnL5hjA"
      },
      "source": [
        "Originalmente no hemos explorado el entorno por lo que tenemos el diccionario de pares (estado,accion)-->(q-valor) vacion y mientras mas exploremos el entorno mas podremos llenarlo mejorando el comportamiento de nuestro algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGzfeH2_5hjB"
      },
      "source": [
        "agent._qvalues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6X18v575hjF"
      },
      "source": [
        "Inicializamos nuestro entorno en un estado aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZ1ogKz5hjG"
      },
      "source": [
        "s = env.reset()\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPuad9jG5hjJ"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CSrcC_F5hjM"
      },
      "source": [
        "Tomamos la accion a tomar ya sea de manera aleatoria o tomando la mejor accion posible. Como nuestro Epsilon inicial va a ser 1 las primeras acciones serán aleatorias y a medida que exploremos dichas acciones se iran volviendo greedy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BFXwPJn5hjN"
      },
      "source": [
        "a = agent.get_action(s)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf6P5MZ_5hjQ"
      },
      "source": [
        "Aplicamos nuestra accion al entorno y observamos lo que ocurre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU48jPQj5hjR"
      },
      "source": [
        "next_s, r, done, _ = env.step(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieGY4nje5hjU",
        "outputId": "9e7e7b92-030a-490b-b889-b1d86f1c69df"
      },
      "source": [
        "print(\"reward: \",r, \"accion: \", a, \"Nuevo Estado: \", next_s)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward:  -1 accion:  0 Nuevo Estado:  393\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yS_eiEi5hjX"
      },
      "source": [
        "**Vamos a actualizar nuestra tabla de q-valores y el entorno dado la acción elegida y ver lo que ocurre con nuestra tabla de q-valores. Hemos explorado estando en el estado incial y como desconocemos el valor hasta realizar la accion todas las acciones tiene el valor 0 y despues de tomar la accion se promediara con el reward obenido.**\n",
        "\n",
        "**Por último, se inicializa el estado de acciones del nuevo estado con todo 0 hasta que se realice la accion.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHDfY5k95hjY"
      },
      "source": [
        "agent._qvalues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKZAQfG5hjb"
      },
      "source": [
        "agent.update(s, a, r, next_s)\n",
        "agent._qvalues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAN82k5L5hje"
      },
      "source": [
        "**Así pues, asi es como va a funcionar nuestro algoritmo vamos a guardar los valores de las acciones que vamos realizando con el objetivo de tomar decisiones en relacion a estos y optimizar la política de nuestro agente.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41CpSkGD5hjf"
      },
      "source": [
        "# JUGEMOS  EN TAXI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUWthIAC5hjg"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\") #gym.make(\"CliffWalking-v0\")\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46_o_XZn5hjj"
      },
      "source": [
        "def play_and_train(env,agent,t_max=10**4):\n",
        "    \"\"\"\n",
        "    Esta función debería\n",
        "    - Ejecutar un juego completo, acciones dadas por la política greedy del agente.\n",
        "    - agente update usando agent.update (...) siempre que sea posible\n",
        "    - Devolver la recompensa total\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        # PIDELE AL AGENTE UNA ACCION A TOMAR\n",
        "        #TU CODIGO AQUI\n",
        "        a = \"XXXXXX\"\n",
        "        \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        # ACTUALIZA TU ALGORITMO AGENT\n",
        "        #TU CODIGO AQUI\n",
        "        \"XXXXXXX\"\n",
        "        \n",
        "        #si quieres ver el juego desbloquea esto:\n",
        "        #env.render()\n",
        "        \n",
        "        #ACTUALIZA EL ESTADO Y GUARDA EL REWARD\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done: break\n",
        "        \n",
        "    return total_reward\n",
        "    \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxjANnyZ5hjm"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "env.reset()\n",
        "rewards = []\n",
        "diccios_intermedios=[]\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent,t_max=10**(5)))#\n",
        "    agent.epsilon *= 0.99\n",
        "    diccios_intermedios.append(agent._qvalues)\n",
        "    \n",
        "    if i %100 ==0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL-24LwJ5hjp"
      },
      "source": [
        "play_and_train(env, agent,t_max=10**(5))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}