{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Expected_sarsa_alm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/4%20Aprendizaje%20reforzado/Sesion%202/Expected_sarsa_alm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9TQzNr6-Qqt"
      },
      "source": [
        "## Espected Sarsa\n",
        "\n",
        "\n",
        "Este cuaderno se basa en `qlearning.ipynb` para implementar el Valor esperado SARSA.\n",
        "\n",
        "La política que usaremos es la política de épsilon-greedy, donde el agente toma medidas óptimas con probabilidad $ (1-\\epsilon)$, de lo contrario, muestra la acción al azar. Ten en cuenta que el agente __puede ocasionalmente muestrear una acción óptima durante el muestreo aleatorio por pura casualidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3n3Kdqp-Qqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1f2bb3fb-5c10-4717-dd7b-d02eee2ed47c"
      },
      "source": [
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "        \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: ../xvfb: No such file or directory\n",
            "env: DISPLAY=:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAizCB4S-Qqy"
      },
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "\n",
        "        \"\"\"\n",
        "        Agente de Q-Learning\n",
        "        Variables de instancia a las que tiene acceso\n",
        "          - self.epsilon (problema de exploración)\n",
        "          - self.alpha (tasa de aprendizaje)\n",
        "          - self.discount (tasa de descuento aka gamma)\n",
        "\n",
        "        Funciones que debes usar\n",
        "          - self.get_legal_actions (state) {estado, hashable -> lista de acciones, cada una es hashable}\n",
        "            que devuelve acciones legales para un estado\n",
        "          - self.get_qvalue (estado, acción)\n",
        "            que devuelve Q (estado, acción)\n",
        "          - self.set_qvalue (estado, acción, valor)\n",
        "            que establece Q (estado, acción): = valor\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self,state,action,value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "        \n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Calcula la estimación de V (s) del agente utilizando los valores q actuales\n",
        "        V (s) = max_over_action Q (estado, acción) sobre posibles acciones.\n",
        "        Nota: tenger en cuenta que los valores q pueden ser negativos.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #SI NO HAY ACCIONES POSIBLES DEVOLVEMOS 0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        #QUEDATE CON EL VALOR DE LA ACCION QUE MAXIMICE Q-VALUE\n",
        "        #TU CÓDIGO AQUI\n",
        "        value = max(self.get_qvalue(state,action) for action in possible_actions)\n",
        "        \n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        ACTIALIZA EL Q-VALOR SEGUN LA FORMULA QUE SE PRESENTA:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        #PARAMETROS\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        #IMPLEMENTA LA FUNCION PRESENTADA EN LA DESCRIPCION DE ARRIBA\n",
        "        value = (1-learning_rate) * self.get_qvalue(state,action) + learning_rate * (reward + gamma * self.get_value(next_state))\n",
        "        \n",
        "        self.set_qvalue(state, action, value)\n",
        "\n",
        "    \n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Calcula la mejor acción para tomar en un estado (utilizando los valores q actuales).\n",
        "        \"\"\"\n",
        "        #SELECCIONA LAS POSIBLES ACCIONES DADO EL ESTADO\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        #CALCULA LA MEJOR ACCION DADO EL ESTADO\n",
        "        #TU CODIGO AQUI\n",
        "        best_action = None\n",
        "        best_q = float(\"-inf\")\n",
        "        for action in possible_actions:\n",
        "            cur_q = self.get_qvalue(state,action)\n",
        "            if cur_q > best_q:\n",
        "                best_q = cur_q\n",
        "                best_action  = action\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Calcula la acción a tomar en el estado actual, incluida la exploración.\n",
        "        Con probabilidad self.epsilon, deberíamos realizar una acción aleatoria.\n",
        "        de lo contrario - la mejor acción política (self.getPolicy).\n",
        "        \"\"\"\n",
        "\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        #PARAMETROS\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        #SELECCIONA UNA ACCION TENIENDO EN CUENTA LA EXPLORACIÓN CON EL EPSILON\n",
        "        #TU CODIGO AQUI\n",
        "        if random.random() > epsilon:\n",
        "            chosen_action = self.get_best_action(state)\n",
        "        else:\n",
        "            action = random.choice(possible_actions)\n",
        "            chosen_action = action\n",
        "        \n",
        "        return chosen_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJC2brrC-Qq1"
      },
      "source": [
        "\n",
        "\n",
        "class EVSarsaAgent(QLearningAgent):\n",
        "    \n",
        "    def get_value(self, state):\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        #IMPLEMENTA EL VALOR MEDIO DE LAS ACCIONES.\n",
        "        # TU CODIGO AQUI\n",
        "        state_value = \"XXXXXXXXXXXXXX\"\n",
        "        \n",
        "        return state_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIDMH_s7-Qq4"
      },
      "source": [
        "# PROBEMOS NUESTRO MODELO SOBRE UN ENTORNO CONOCIDO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YICbJgs-Qq5"
      },
      "source": [
        "import gym, gym.envs.toy_text\n",
        "env = gym.envs.toy_text.CliffWalkingEnv()\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(env.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rK2nT9R-Qq-"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IoeDObm-QrB"
      },
      "source": [
        "\n",
        "def play_and_train(env,agent,t_max=10**4):\n",
        "    \"\"\"This function should \n",
        "    - run a full game, actions given by agent.getAction(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        a = agent.get_action(s)\n",
        "        \n",
        "        next_s,r,done,_ = env.step(a)\n",
        "        agent.update(s, a, r, next_s)\n",
        "        \n",
        "        s = next_s\n",
        "        total_reward +=r\n",
        "        if done:break\n",
        "        \n",
        "    return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvWxWTsP-QrE"
      },
      "source": [
        "from collections import defaultdict\n",
        "import random, math\n",
        "import numpy as np\n",
        "\n",
        "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJBdgh8R-QrH"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from pandas import DataFrame\n",
        "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(10000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "    \n",
        "    if i %100 ==0:\n",
        "        clear_output(True)\n",
        "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
        "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
        "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
        "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9G9PP3h-QrK"
      },
      "source": [
        "# VEAMOS LAS DIFERENCISA CON Q-LEARNING: Cliff Walking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ain4Scyq-QrL"
      },
      "source": [
        "import gym, gym.envs.toy_text\n",
        "env = gym.envs.toy_text.CliffWalkingEnv()\n",
        "n_actions = env.action_space.n\n",
        "#print(env.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd3WokCi-QrO"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiYB0Upf-QrR"
      },
      "source": [
        "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pjR-5h2-QrT"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from pandas import DataFrame\n",
        "moving_average = lambda x, span=100: DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(10000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "    #Note: agent.epsilon stays constant\n",
        "    \n",
        "    if i %100 ==0:\n",
        "        clear_output(True)\n",
        "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
        "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
        "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
        "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHn21v_1-QrW"
      },
      "source": [
        "def draw_policy(env, agent):\n",
        "    n_rows, n_cols = env._cliff.shape\n",
        "    \n",
        "    actions = '^>v<'\n",
        "    \n",
        "    for yi in range(n_rows):\n",
        "        for xi in range(n_cols):\n",
        "            if env._cliff[yi, xi]:\n",
        "                print(\" C \", end='')\n",
        "            elif (yi * n_cols + xi) == env.start_state_index:\n",
        "                print(\" X \", end='')\n",
        "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
        "                print(\" T \", end='')\n",
        "            else:\n",
        "                print(\" %s \" % actions[agent.get_best_action(yi * n_cols + xi)], end='')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jTzqVKC-QrZ"
      },
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(env, agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(env, agent_sarsa)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}