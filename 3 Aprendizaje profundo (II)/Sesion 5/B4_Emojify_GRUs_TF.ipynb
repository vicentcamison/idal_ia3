{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "RNnEs",
      "launcher_item_id": "acNYU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "4-Emojify_GRUs_TF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cTQ30GvJtKFq",
        "KaVwFmT9tKFq",
        "OF_kORZLtKF3"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/Sesion%205/B4_Emojify_GRUs_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0lccR8uznTK"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \r\n",
        "\r\n",
        "#**Máster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDb9804BzstC"
      },
      "source": [
        "#<strong><center>Emojify!</center></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idxi5zI6tKFJ"
      },
      "source": [
        "En este documento vamos a utilizar representaciones vectoriales de palabras para construir un Emojificador. \n",
        "\n",
        "¿Alguna vez has querido hacer tus mensajes de texto más expresivos? Tu aplicación emojificadora te ayudará a hacerlo. La finalidad es que en lugar de escribir \"Congratulations on the promotion! Lets get coffee and talk. Love you!\" , el emojificador puede convertirlo automáticamente en \"Congratulations on the promotion! 👍 Lets get coffee and talk. ☕️ Love you! ❤️\"\n",
        "\n",
        "*NOTA: Debido a que el vector de entrenamiento de emmbedings es en inglés, hemos de trabajar con frases en inglés. Siempre es bueno practicar idiomas!*\n",
        "\n",
        "Implementarás un modelo que introduzca una frase (como \"¡Vamos a ver el partido de béisbol esta noche!\") y encuentre el emoji más apropiado para ser utilizado con esta frase (⚾️). En muchas interfaces de emoji, tienes que recordar que ❤️ es el símbolo del \"corazón\" y no el del \"amor\". Pero si utilizas vectores de palabras, verás que incluso si tu conjunto de entrenamiento relaciona explícitamente sólo unas pocas palabras con un emoji en particular, tu algoritmo será capaz de generalizar y asociar palabras en el conjunto de prueba al mismo emoji incluso si esas palabras ni siquiera aparecen en el conjunto de entrenamiento. Esto te permite construir un clasificador preciso de frases a emojis, incluso utilizando un conjunto de entrenamiento pequeño. \n",
        "\n",
        "En el ejercicio, empezaremos con un modelo de referencia (Emojifier-V1) que utiliza *word embeddings*, y luego construiremos un modelo más sofisticado (Emojifier-V2) que incorpora una GRU. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55BqdKU8t0PP",
        "outputId": "503989c2-c59a-47cb-9faf-d1f125b1fb31"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4quQWfpFtKFU"
      },
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLKC_gO21OtK"
      },
      "source": [
        "Para el ejercicio necesitamos acceso a varios documentos, de forma que lo mejor es realizar la conexión con nuestro googel Drive donde debemos tener los siguientes archivos: \r\n",
        "* train_emoji.csv\r\n",
        "* tesss.csv\r\n",
        "* glove.6B.50d.txt\r\n",
        "\r\n",
        "Tambien es necesario poner el fichero \"emo_utils.py\" en el directorio \"/content\" de la ejecucion temporal del notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJNZFplt0_iI",
        "outputId": "b499d5db-cb08-4778-b81e-5d38e0c8f9c7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7aS6_WqtKFV"
      },
      "source": [
        "## 1 - Baseline model: Emojifier-V1\n",
        "\n",
        "### 1.1 - Dataset EMOJISET\n",
        "Empezamos por construir un clasificador de referencia sencillo. \n",
        "\n",
        "Se tiene un pequeño conjunto de datos (X, Y) donde\n",
        "- X contiene 127 frases (cadenas)\n",
        "- Y contiene una etiqueta entera entre 0 y 4 que corresponde a un emoji para cada frase\n",
        "![Emojiset](https://i.imgur.com/KhFJKMv.png)\n",
        "<caption><center> **Figura 1**: EMOJISET - un problema de clasificación con 5 clases. Aquí se dan algunos ejemplos de frases. </center></caption>\n",
        "\n",
        "Vamos a cargar el conjunto de datos utilizando el código siguiente. Dividimos el conjunto de datos entre entrenamiento (127 ejemplos) y prueba (56 ejemplos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oh5fnFSptKFW"
      },
      "source": [
        "X_train, Y_train = read_csv('/content/gdrive/MyDrive/Colab Notebooks/IA3_data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('/content/gdrive/MyDrive/Colab Notebooks/IA3_data/tesss.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mpyUuCHztKFX"
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kn-_p_5tKFX"
      },
      "source": [
        "Ejecute la siguiente celda para imprimir las frases de X_train y las etiquetas correspondientes de Y_train. Cambie el `índice` para ver diferentes ejemplos. Debido al tipo de letra que utiliza el cuaderno iPython, el emoji del corazón puede ser de color negro en lugar de rojo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUoBv-fntKFY",
        "outputId": "d1c26261-4d4c-42ae-d0d4-0c71f3ecf165"
      },
      "source": [
        "index = 1\n",
        "print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am proud of your achievements 😄\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1YEyJ83tKFZ"
      },
      "source": [
        "### 1.2 - Visión general del Emojifier-V1\n",
        "\n",
        "En esta parte, vas a implementar un modelo de base llamado \"Emojifier-v1\".  \n",
        "<center>\n",
        "\n",
        "![Emojifier v1](https://i.imgur.com/Lxj2QM0.png)\n",
        "<caption><center> **Figura 2**: Modelo de referencia (Emojifier-V1).</center></caption>.\n",
        "</center>\n",
        "\n",
        "La entrada del modelo es una cadena correspondiente a una frase (por ejemplo, \"Te quiero\"). En el código, la salida será un vector de probabilidad de forma (1,5), que luego se pasa en una capa argmax para extraer el índice de la salida más probable del emoji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMW-ZnSTtKFa"
      },
      "source": [
        "Para obtener nuestras etiquetas en un formato adecuado para el entrenamiento de un clasificador softmax, vamos a convertir $Y$ de su forma actual $(m, 1)$ en una \"representación de un solo disparo\" $(m, 5)$, donde cada fila es un vector de un solo disparo que da la etiqueta de un ejemplo. Aquí, `Y_oh` significa \"Y-one-hot\" en los nombres de las variables `Y_oh_train` y `Y_oh_test`: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xVF6KZwvtKFb"
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7D98Xu4tKFb"
      },
      "source": [
        "Veamos lo que hizo `convert_to_one_hot()`. Siéntase libre de cambiar `index` para imprimir diferentes valores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sztWKqOytKFb",
        "outputId": "269e8e7f-1238-4bd7-831d-438e2b696459"
      },
      "source": [
        "index = 50\n",
        "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 is converted into one hot [1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBLwBMK0tKFc"
      },
      "source": [
        "Todos los datos están listos para ser introducidos en el modelo Emojify-V1. Vamos a implementar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qKCvi--tKFd"
      },
      "source": [
        "### 1.3 - Implementación del Emojifier-V1\n",
        "\n",
        "Como se muestra en la Figura (2), el primer paso es convertir una frase de entrada en la representación de vectores de palabras, que luego se promedian juntos. Al igual que en el ejercicio anterior, utilizaremos incrustaciones GloVe preentrenadas de 50 dimensiones. Ejecute la siguiente celda para cargar el `word_to_vec_map`, que contiene todas las representaciones vectoriales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "p4ppq-uEtKFd"
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/gdrive/MyDrive/Colab Notebooks/IA3_data/glove.6B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn00eQZVtKFe"
      },
      "source": [
        "Has cargado:\n",
        "- `word_to_index`: mapeo del diccionario desde las palabras a sus índices en el vocabulario (400.001 palabras, con los índices válidos que van de 0 a 400.000)\n",
        "- `index_to_word`: diccionario que relaciona los índices con las palabras correspondientes en el vocabulario.\n",
        "- `word_to_vec_map`: diccionario que asigna las palabras a su representación vectorial en GloVe.\n",
        "\n",
        "Ejecute la siguiente celda para comprobar si funciona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqffT3T7tKFe",
        "outputId": "1127d248-0785-4c6d-bea3-25ba1f736913"
      },
      "source": [
        "word = \"cucumber\"\n",
        "index = 289846\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the index of cucumber in the vocabulary is 113317\n",
            "the 289846th word in the vocabulary is potatos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibv9NmCNtKFf"
      },
      "source": [
        "Vamos a implementar `sentence_to_avg()`. Deberá realizar dos pasos:\n",
        "1. Convertir cada frase a minúsculas, y luego dividir la frase en una lista de palabras. Puede ser útil utilizar `X.lower()` y `X.split()`. \n",
        "2. Para cada palabra de la frase, acceda a su representación GloVe. A continuación, promedia todos estos valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TpR1sqJUtKFf"
      },
      "source": [
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Convierte una frase (cadena) en una lista de palabras (cadenas). Extrae la representación GloVe de cada palabra\n",
        "    y promedia su valor en un único vector que codifica el significado de la frase.\n",
        "    \n",
        "    Argumentos:\n",
        "    frase -- cadena, un ejemplo de entrenamiento de X\n",
        "    word_to_vec_map -- diccionario que mapea cada palabra de un vocabulario en su representación vectorial de 50 dimensiones\n",
        "    \n",
        "    Devuelve:\n",
        "    avg -- vector medio que codifica la información sobre la frase, matriz numpy de forma (50,)\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Split sentence into list of lower case words (≈ 1 line)\n",
        "    words = [i.lower() for i in sentence.split()]\n",
        "\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros((50,))\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    for w in words:\n",
        "        avg += word_to_vec_map[w]\n",
        "    avg = avg / len(words)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0VQQTy7tKFg",
        "outputId": "ca2213dd-8cd3-49f8-9f65-5a9be7ef3886"
      },
      "source": [
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "print(\"avg = \", avg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
            " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
            "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
            "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
            "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
            "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
            " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
            " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
            "  0.1445417   0.09808667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2mtMMxftKFg"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **avg= **\n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
        " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
        "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
        "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
        "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
        "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
        " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
        " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
        "  0.1445417   0.09808667]\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jtihxKr6tKFh"
      },
      "source": [
        "#### Modelo\n",
        "\n",
        "Ahora tenemos todas las piezas para terminar de implementar la función `model()`. Después de usar `sentence_to_avg()` necesitas pasar el promedio a través de la propagación hacia adelante, calcular el costo, y luego retropropagar para actualizar los parámetros del softmax. \n",
        "\n",
        "Vamso ahora a implementar la función `model()` descrita en la Figura (2). Asumiendo aquí que $Yoh$ (\"Y one hot\") es la codificación one-hot de las etiquetas de salida, las ecuaciones que necesitas implementar en el forward pass y para calcular el coste de la entropía cruzada son\n",
        "\n",
        "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
        "$$ a^{(i)} = softmax(z^{(i)})$$\n",
        "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$\n",
        "\n",
        "Es posible llegar a una implementación vectorizada más eficiente. Pero ya que estamos utilizando un bucle for para convertir las sentencias de una en una en la representación $avg^{(i)}$ de todos modos, no vamos a molestarnos esta vez. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMZl_8DitKFh"
      },
      "source": [
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
        "    \"\"\"\n",
        "    Modelo para entrenar representaciones de vectores de palabras en numpy.\n",
        "    \n",
        "    Argumentos:\n",
        "    X -- datos de entrada, array numpy de frases como cadenas, de forma (m, 1)\n",
        "    Y -- etiquetas, matriz numpy de enteros entre 0 y 7, matriz numpy de forma (m, 1)\n",
        "    word_to_vec_map -- diccionario que mapea cada palabra de un vocabulario en su representación vectorial de 50 dimensiones\n",
        "    learning_rate -- tasa de aprendizaje para el algoritmo de descenso de gradiente estocástico\n",
        "    num_iterations -- número de iteraciones\n",
        "    \n",
        "    Devuelve:\n",
        "    pred -- vector de predicciones, numpy-array de forma (m, 1)\n",
        "    W -- matriz de pesos de la capa softmax, de forma (n_y, n_h)\n",
        "    b -- sesgo de la capa softmax, de forma (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                          # number of training examples\n",
        "    n_y = 5                                 # number of classes  \n",
        "    n_h = 50                                # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
        "        for i in range(m):                                # Loop over the training examples\n",
        "            \n",
        "  \n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer\n",
        "            z = np.dot(W, avg) + b\n",
        "            a = softmax(z)\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
        "\n",
        "            \n",
        "            # Compute gradients \n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
        "\n",
        "    return pred, W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDPXz4rbtKFj",
        "outputId": "739ad297-047d-49c8-f4d2-214bad3448ad"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(X_train[0])\n",
        "print(type(X_train))\n",
        "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
        "print(Y.shape)\n",
        "\n",
        "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
        " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
        " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
        " 'You totally deserve this prize', 'Let us go play football',\n",
        " 'Are you down for football this afternoon', 'Work hard play harder',\n",
        " 'It is suprising how people can be dumb sometimes',\n",
        " 'I am very disappointed','It is the best day in my life',\n",
        " 'I think I will end up alone','My life is so boring','Good job',\n",
        " 'Great so awesome'])\n",
        "\n",
        "print(X.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(type(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(132,)\n",
            "(132,)\n",
            "(132, 5)\n",
            "never talk to me again\n",
            "<class 'numpy.ndarray'>\n",
            "(20,)\n",
            "(20,)\n",
            "(132, 5)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcr9Qn7ltKFk"
      },
      "source": [
        "Ejecute la siguiente celda para entrenar su modelo y aprender los parámetros softmax (W,b). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEja-X_jtKFl",
        "outputId": "6e4fcdef-cd77-45d6-b539-b44417aa7a69"
      },
      "source": [
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
        "#print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 --- cost = 1.9520498812810076\n",
            "Accuracy: 0.3484848484848485\n",
            "Epoch: 100 --- cost = 0.07971818726014807\n",
            "Accuracy: 0.9318181818181818\n",
            "Epoch: 200 --- cost = 0.04456369243681402\n",
            "Accuracy: 0.9545454545454546\n",
            "Epoch: 300 --- cost = 0.03432267378786059\n",
            "Accuracy: 0.9696969696969697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jip-JhRctKFl"
      },
      "source": [
        "**Salida esperada** (on a subset of iterations):\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 0**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 1.95204988128\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.348484848485\n",
        "        </td>\n",
        "    </tr>\n",
        "\n",
        "\n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 100**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0797181872601\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.931818181818\n",
        "        </td>\n",
        "    </tr>\n",
        "    \n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 200**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0445636924368\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.954545454545\n",
        "        </td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 300**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0343226737879\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.969696969697\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fzDJn0tKFm"
      },
      "source": [
        "Genial. El modelo tiene una precisión bastante alta en el conjunto de entrenamiento. Veamos ahora cómo lo hace en el conjunto de pruebas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "gaBjaMvitKFm"
      },
      "source": [
        "### 1.4 - Examinar el rendimiento del conjunto de pruebas \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAaKFbnytKFm",
        "outputId": "1c5222e3-fb56-45f3-f993-883b34a0cb5e"
      },
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print('Test set:')\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set:\n",
            "Accuracy: 0.9772727272727273\n",
            "Test set:\n",
            "Accuracy: 0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBOwQUortKFn"
      },
      "source": [
        "La adivinación aleatoria habría tenido un 20% de precisión dado que hay 5 clases. Se trata de un rendimiento bastante bueno después de entrenar con sólo 127 ejemplos. \n",
        "\n",
        "En el conjunto de entrenamiento, el algoritmo vio la frase \"*I love you*\" con la etiqueta ❤️. Sin embargo, puedes comprobar que la palabra \"adore\" no aparece en el conjunto de entrenamiento. No obstante, veamos qué ocurre si se escribe \"i adore you*\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ3KNPVLtKFo",
        "outputId": "edf41161-8ee1-4997-dd9e-0f68b1ef2eaa"
      },
      "source": [
        "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "print_predictions(X_my_sentences, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "\n",
            "i adore you ❤️\n",
            "i love you ❤️\n",
            "funny lol 😄\n",
            "lets play with a ball ⚾\n",
            "food is ready 🍴\n",
            "not feeling happy 😄\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG3TSDU7tKFo"
      },
      "source": [
        "Sorprendente. Como *adore* tiene una embedding similar a la de *love*, el algoritmo ha generalizado correctamente incluso con una palabra que nunca había visto antes. Palabras como *heart*, *dear*, *beloved* or *adore* tienen vectores de embedding similares a *love*, por lo que también podrían funcionar; siéntase libre de modificar las entradas anteriores y probar una variedad de frases de entrada. ¿Qué tal funciona?\n",
        "\n",
        "Observa que no acierta con \"not feeling happy\". Este algoritmo ignora el orden de las palabras, por lo que no es bueno para entender frases como \"no happy\". \n",
        "\n",
        "Imprimir la matriz de confusión también puede ayudar a entender qué clases son más difíciles para tu modelo. Una matriz de confusión muestra la frecuencia con la que un ejemplo cuya etiqueta es de una clase (clase \"real\") es etiquetado erróneamente por el algoritmo con una clase diferente (clase \"predicha\"). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "EFPIq8_itKFo",
        "outputId": "0fda061b-ad52-475f-cf48-b57512f86f1e"
      },
      "source": [
        "print(Y_test.shape)\n",
        "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56,)\n",
            "           ❤️    ⚾    😄    😞   🍴\n",
            "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
            "Actual                                 \n",
            "0            6    0    0    1    0    7\n",
            "1            0    8    0    0    0    8\n",
            "2            2    0   16    0    0   18\n",
            "3            1    1    2   12    0   16\n",
            "4            0    0    1    0    6    7\n",
            "All          9    9   19   13    6   56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ChmSA4dUtKFp"
      },
      "source": [
        "<font color='blue'>\n",
        "**Lo MAS IMPORTANTE de esta parte**:\n",
        "\n",
        "- Incluso con 127 ejemplos de entrenamiento, puedes obtener un modelo razonablemente bueno para la emojificación. Esto se debe al poder de generalización que le dan los vectores de palabras. \n",
        "- Emojify-V1 tendrá un mal rendimiento en frases como *\"Esta película no es buena y no es agradable \"* porque no entiende las combinaciones de palabras -simplemente promedia todos los vectores de embedding de las palabras, sin prestar atención al orden de las palabras. En la siguiente parte construirás un algoritmo mejor. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyD9auntKFp"
      },
      "source": [
        "## 2 - Emojifier-V2: Usando GRUs en Keras: \n",
        "\n",
        "Vamos a construir un modelo con GRUS que tome como entrada secuencias de palabras. Este modelo será capaz de tener en cuenta el orden de las palabras. Emojifier-V2 continuará utilizando incrustaciones de palabras pre-entrenadas para representar las palabras, pero las alimentará en un GRU, cuyo trabajo es predecir el emoji más apropiado. \n",
        "\n",
        "Ejecuta la siguiente celda para cargar los paquetes Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGokrRLatKFp"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, GRU, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQ30GvJtKFq"
      },
      "source": [
        "### 2.1 - Visión general del modelo\n",
        "\n",
        "Aquí está el Emojifier-v2 que implementarás:\n",
        "\n",
        "![Emojifier v2](https://i.imgur.com/zDC1Z2U.png)\n",
        "<caption><center> **Figura 3**: Emojifier-V2. Un clasificador de secuencias RGU de 2 capas. </center></caption>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaVwFmT9tKFq"
      },
      "source": [
        "### 2.2 Keras y el mini-batching \n",
        "\n",
        "En este ejercicio, queremos entrenar a Keras usando mini-lotes. Sin embargo, la mayoría de los marcos de aprendizaje profundo requieren que todas las secuencias en el mismo mini-batch tengan la misma longitud. Esto es lo que permite que la vectorización funcione: Si tuviéramos una frase de 3 palabras y otra de 4, los cálculos necesarios para ellas son diferentes (una requiere 3 pasos de un GRU, otra requiere 4 pasos), por lo que no es posible hacer ambas al mismo tiempo.\n",
        "\n",
        "La solución común a esto es utilizar el relleno. En concreto, se establece una longitud máxima de secuencia y se rellenan todas las secuencias con la misma longitud. Por ejemplo, si la longitud máxima de la secuencia es 20, podríamos rellenar cada frase con \"0\" para que cada frase de entrada tenga una longitud de 20. Así, una frase \"te quiero\" se representaría como $(e_{i}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$. En este ejemplo, cualquier frase de más de 20 palabras tendría que ser truncada. Una forma sencilla de elegir la longitud máxima de la secuencia es simplemente elegir la longitud de la frase más larga del conjunto de entrenamiento. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSuTdszRtKFq"
      },
      "source": [
        "### 2.3 - La capa de embedding\n",
        "\n",
        "En Keras, la matriz de embedding se representa como una \"capa\", y mapea enteros positivos (índices correspondientes a las palabras) en vectores densos de tamaño fijo (los vectores de embedding). Puede ser entrenada o inicializada con una embedding preentrenada. En esta parte, aprenderás a crear una capa [Embedding()](https://keras.io/layers/embeddings/) en Keras, inicializándola con los vectores GloVe de 50 dimensiones cargados anteriormente en el cuaderno. Como nuestro conjunto de entrenamiento es bastante pequeño, no actualizaremos las incrustaciones de palabras, sino que dejaremos sus valores fijos. Pero en el código siguiente, mostraremos cómo Keras permite entrenar o dejar fija esta capa.  \n",
        "\n",
        "La capa `Embedding()` toma como entrada una matriz entera de tamaño (tamaño del lote, longitud máxima de la entrada). Esta corresponde a sentencias convertidas en listas de índices (enteros), como se muestra en la siguiente figura.\n",
        "![Embedding1](https://i.imgur.com/g4xvysH.png)\n",
        "\n",
        "<caption><center> **Figura 4**: Capa de embedding. Este ejemplo muestra la propagación de dos ejemplos a través de la capa de embedding. Ambos han sido rellenados con cero hasta una longitud de `max_len=5`. La dimensión final de la representación es `(2,max_len,50)` porque las incrustaciones de palabras que estamos utilizando son de 50 dimensiones. </center></caption>\n",
        "\n",
        "El mayor número entero (es decir, el índice de la palabra) en la entrada no debe ser mayor que el tamaño del vocabulario. La capa da salida a un array de forma (tamaño del lote, longitud máxima de la entrada, dimensión de los vectores de palabras).\n",
        "\n",
        "El primer paso es convertir todas las frases de entrenamiento en listas de índices, y luego poner a cero todas estas listas para que su longitud sea la de la frase más larga. \n",
        "\n",
        "Implementamos la función siguiente para convertir X (matriz de sentencias como cadenas) en una matriz de índices correspondientes a las palabras de las sentencias. La forma de salida debe ser tal que se pueda dar a `Embedding()` (descrito en la Figura 4). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0dyrKqEctKFr"
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Convierte una matriz de frases (cadenas) en una matriz de índices correspondientes a las palabras de las frases.\n",
        "    La forma de salida debe ser tal que se pueda dar a `Embedding()` (descrito en la Figura 4). \n",
        "    \n",
        "    Argumentos:\n",
        "    X -- matriz de frases (cadenas), de forma (m, 1)\n",
        "    word_to_index -- un diccionario que contiene cada palabra asignada a su índice\n",
        "    max_len -- número máximo de palabras en una frase. Se puede asumir que cada frase en X no es más larga que esto. \n",
        "    \n",
        "    Devuelve:\n",
        "    X_indices -- array de índices correspondientes a las palabras de las sentencias de X, de forma (m, max_len)\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = X.shape[0]                                   \n",
        "\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    # loop over training examples\n",
        "    for i in range(m):                               \n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = [w.lower() for w in X[i].split()]\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            # Increment j to j + 1\n",
        "            j += 1\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kEWahKetKFr"
      },
      "source": [
        "Chequeamos `sentences_to_indices()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABX5Bh9BtKFr",
        "outputId": "82563781-61f0-418d-d934-e16ff2c2fcbf"
      },
      "source": [
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
            "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
            " [220930. 286375.  69714.      0.      0.]\n",
            " [151204. 192973. 302254. 151349. 394475.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qts9amsVtKFs"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **X1 =**\n",
        "        </td>\n",
        "        <td>\n",
        "           ['funny lol' 'lets play football' 'food is ready for you']\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **X1_indices =**\n",
        "        </td>\n",
        "        <td>\n",
        "           [[ 155345.  225122.       0.       0.       0.] <br>\n",
        "            [ 220930.  286375.  151266.       0.       0.] <br>\n",
        "            [ 151204.  192973.  302254.  151349.  394475.]]\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f35MPre8tKFs"
      },
      "source": [
        "Vamos a construir la capa `Embedding()` en Keras, utilizando vectores de palabras pre-entrenados. Una vez construida esta capa, le pasaremos la salida de `sentences_to_indices()` como entrada, y la capa `Embedding()` devolverá las incrustaciones de palabras para una frase. \n",
        "\n",
        "Implementaremos la capa `pretrained_embedding()`. Deberás realizar los siguientes pasos:\n",
        "1. Inicializar la matriz de embedding como un array numpy de ceros con la forma correcta.\n",
        "2. Rellenar la matriz de embedding con todas las incrustaciones de palabras extraídas de `word_to_vec_map`.\n",
        "3. Definir la capa de embedding de Keras. Utilice [Embedding()](https://keras.io/layers/embeddings/). Asegúrese de hacer que esta capa no sea entrenable, estableciendo `trainable = False` cuando llame a `Embedding()`. Si establece `trainable = True`, entonces permitirá que el algoritmo de optimización modifique los valores de las incrustaciones de palabras. \n",
        "4. Establezca los pesos de embedding para que sean iguales a la matriz de embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "D-XY1AK2tKFs"
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Crea una capa de Keras Embedding() y carga en ella vectores preentrenados GloVe de 50 dimensiones.\n",
        "    \n",
        "    Argumentos:\n",
        "    word_to_vec_map -- diccionario que mapea las palabras a su representación vectorial en GloVe.\n",
        "    word_to_index -- diccionario que mapea las palabras a sus índices en el vocabulario (400.001 palabras)\n",
        "\n",
        "    Devuelve:\n",
        "    embedding_layer -- instancia de Keras de la capa preentrenada\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFESHM4itKFt",
        "outputId": "df55913d-0519-44bf-f5ab-3dfaf2290324"
      },
      "source": [
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights[0][1][3] = -0.3403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucGmUvVctKFt"
      },
      "source": [
        "** Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **weights[0][1][3] =**\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.3403\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkSIvkqFtKFu"
      },
      "source": [
        "## 2.3 Construir el Emojifier-V2\n",
        "\n",
        "Construyamos ahora el modelo Emojifier-V2. Lo harás utilizando la capa de embedding que has construido, y alimentarás su salida a una red GRU. \n",
        "![Emojifier v2](https://i.imgur.com/zDC1Z2U.png)\n",
        "<caption><center> **Figura 3**: Emojifier-v2. Un clasificador de secuencias GRU de 2 capas. </center></caption>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtR7WWzTFDu7"
      },
      "source": [
        "Implementaremos `Emojify_V2()`, que construye un gráfico Keras de la arquitectura mostrada en la Figura 3. El modelo toma como entrada un array de frases de forma (`m`, `max_len`, ) definido por `input_shape`. Debería dar como salida un vector de probabilidad softmax de forma (`m`, `C = 5`). Puede necesitar `Input(shape = ..., dtype = '...')`, [GRU()](https://keras.io/layers/recurrent/#GRU), [Dropout()](https://keras.io/layers/core/#dropout), [Dense()](https://keras.io/layers/core/#dense), y [Activation()](https://keras.io/activations/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aSckngTXtKFu"
      },
      "source": [
        "# Emojify_V2\n",
        "\n",
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Función que crea el gráfico del modelo Emojify-v2.\n",
        "    \n",
        "    Argumentos:\n",
        "    input_shape -- forma de la entrada, normalmente (max_len,)\n",
        "    word_to_vec_map -- diccionario que mapea cada palabra del vocabulario en su representación vectorial de 50 dimensiones\n",
        "    word_to_index -- diccionario que asigna las palabras a sus índices en el vocabulario (400.001 palabras)\n",
        "\n",
        "    Devuelve:\n",
        "    model -- una instancia del modelo en Keras\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
        "    sentence_indices = Input(input_shape, dtype='int32')\n",
        "    \n",
        "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "    embeddings = embedding_layer(sentence_indices)   \n",
        "    \n",
        "    # Propagate the embeddings through an GRU layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a batch of sequences.\n",
        "    X = GRU(128, return_sequences=True)(embeddings)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X trough another GRU layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = GRU(128, return_sequences=False)(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
        "    X = Dense(5)(X)\n",
        "    # Add a softmax activation\n",
        "    X = Activation('softmax')(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(inputs=sentence_indices, outputs=X)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IiM3hK4tKFu"
      },
      "source": [
        "Ejecute la siguiente celda para crear su modelo y comprobar su resumen. Como todas las frases del conjunto de datos tienen menos de 10 palabras, elegimos `max_len = 10`.  \r\n",
        "Si revisamos la arquitectura, vemos que utiliza \"20.168.887\" parámetros, de los cuales 20.000.050 (las incrustaciones de palabras) no son entrenables, y los 168.837 restantes sí. Como nuestro vocabulario tiene 400.001 palabras (con índices válidos de 0 a 400.000) hay 400.001*50 = 20.000.050 parámetros no entrenables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6aEIdUTtKFu",
        "outputId": "045394ef-56b6-4d9d-a0e6-d4fec7968b52"
      },
      "source": [
        "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 10)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 50)            20000050  \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 10, 128)           69120     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 20,168,887\n",
            "Trainable params: 168,837\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_r7LHSVtKFv"
      },
      "source": [
        "Como siempre, después de crear tu modelo en Keras, necesitas compilarlo y definir qué pérdida, optimizador y métrica quieres usar. Compila tu modelo usando la pérdida `categorical_crossentropy`, el optimizador `adam` y la métrica `['accuracy']`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PX_R2zuutKFv"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfcgaV6tKFw"
      },
      "source": [
        "Es hora de entrenar tu modelo. Tu `modelo` de Emojifier-V2 toma como entrada un array de formas (`m`, `max_len`) y da como salida vectores de probabilidad de formas (`m`, `número de clases`). Por lo tanto, tenemos que convertir X_train (matriz de frases como cadenas) en X_train_indices (matriz de frases como lista de índices de palabras), e Y_train (etiquetas como índices) en Y_train_oh (etiquetas como vectores de un solo golpe)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iQt1rKGUtKFw"
      },
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIYQcsFtKFw"
      },
      "source": [
        "Ajusta el modelo Keras en `X_train_indices` y `Y_train_oh`. Utilizaremos `epochs = 50` y `batch_size = 32`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6rKZD20tKF0",
        "outputId": "7922e661-f610-47c8-a0ca-afb5d3f5c1e1"
      },
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 8.0368e-04 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 9.8403e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f319b3bf198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9CLRk89tKF1"
      },
      "source": [
        "El modelo debe tener una precisión cercana al **100%** en el conjunto de entrenamiento. La precisión exacta que obtenga puede ser un poco diferente. Ejecuta la siguiente celda para evaluar tu modelo en el conjunto de pruebas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KMawwTltKF1",
        "outputId": "3040199e-e2a5-4a67-8973-335f9afb054f"
      },
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4234 - accuracy: 0.8571\n",
            "\n",
            "Test accuracy =  0.8571428656578064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeLlnsutKF2"
      },
      "source": [
        "Debería obtener una precisión de la prueba entre el 80% y el 95%. Ejecuta la celda de abajo para ver los ejemplos mal etiquetados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpYsP2yEtKF2",
        "outputId": "42b92047-1bbb-477b-8ca2-8234e8a0daf1"
      },
      "source": [
        "C = 5\n",
        "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected emoji:😞 prediction: This girl is messing with me\t❤️\n",
            "Expected emoji:❤️ prediction: I love taking breaks\t😞\n",
            "Expected emoji:😄 prediction: you brighten my day\t❤️\n",
            "Expected emoji:😞 prediction: she is a bully\t❤️\n",
            "Expected emoji:😞 prediction: My life is so boring\t❤️\n",
            "Expected emoji:😄 prediction: will you be my valentine\t❤️\n",
            "Expected emoji:⚾ prediction: he can pitch really well\t😄\n",
            "Expected emoji:❤️ prediction: family is all I have\t😞\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1lYq921tKF2"
      },
      "source": [
        "Ahora puedes probarlo con tu propio ejemplo. Escribe tu propia frase a continuación. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5UjT_OhtKF3",
        "outputId": "a8c45f50-c957-4c76-d7f8-eab915eba687"
      },
      "source": [
        "# Juega con diferentes frases para ver las predecciones. Asegurate que las palabras existen en Glove embeddings.  \n",
        "# Sentences= [\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "x_test = np.array(['congratulations for the job'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "congratulations for the job 😄\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liIQQR7BtKF3"
      },
      "source": [
        "Prueba diferentes frases y observa los resultados. Ten en cuenta varias cosas: \n",
        "* las salidas de Keras son ligeramente aleatorias cada vez, así que puede que no haya obtenido el mismo resultado)\n",
        "* El modelo actual todavía no es muy robusto a la hora de entender la negación (como \"no ser feliz\") porque el conjunto de entrenamiento es pequeño y, por tanto, no tiene muchos ejemplos de negación. Pero si el conjunto de entrenamiento fuera más grande, el modelo GRU sería mucho mejor que el modelo Emojify-V1 para entender esas frases complejas. \n",
        "\n",
        "¿Que posibilidades de mejora crees que tiene? ¿Podríamos usar alguna arquitectura mejor?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF_kORZLtKF3"
      },
      "source": [
        "**Lo que debes recordar**:\n",
        "- Si tiene una tarea de PLN en la que el conjunto de entrenamiento es pequeño, el uso de incrustaciones de palabras puede ayudar a su algoritmo de forma significativa. Las incrustaciones de palabras permiten que tu modelo trabaje con palabras en el conjunto de prueba que pueden no haber aparecido en tu conjunto de entrenamiento. \n",
        "- El entrenamiento de modelos de secuencia en Keras (y en la mayoría de los otros marcos de aprendizaje profundo) requiere algunos detalles importantes:\n",
        "    - Para utilizar minilotes, las secuencias deben ser rellenadas para que todos los ejemplos de un minilote tengan la misma longitud. \n",
        "    - Una capa `Embedding()` puede ser inicializada con valores pre-entrenados. Estos valores pueden ser fijos o entrenados posteriormente en su conjunto de datos. Sin embargo, si su conjunto de datos etiquetados es pequeño, normalmente no vale la pena intentar entrenar un gran conjunto preentrenado de incrustaciones.   \n",
        "    - `GRU()` tiene una bandera llamada `return_sequences` para decidir si quieres devolver todos los estados ocultos o sólo el último. \n",
        "    - Puedes usar `Dropout()` justo después de `GRU()` para regularizar tu red. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tssylD8C_pMJ"
      },
      "source": [
        "## Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJE1_qv6-_XT"
      },
      "source": [
        "* Documento inspirado en los cuadernos del curso Deep Learning de Andrew Ng en Coursera: https://es.coursera.org/\r\n",
        "* Reconocimiento tambien a Alison Darcy y al equipo de Woebot por el asesoramiento en la creación de esta tarea:  http://woebot.io\r\n",
        "* Doc oficial Pytorch https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\r\n",
        "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5icC_T58_rLU"
      },
      "source": [
        "##Fin del cuaderno"
      ]
    }
  ]
}