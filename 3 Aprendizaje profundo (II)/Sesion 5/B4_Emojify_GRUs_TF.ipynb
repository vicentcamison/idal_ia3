{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "RNnEs",
      "launcher_item_id": "acNYU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "4-Emojify_GRUs_TF.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cTQ30GvJtKFq",
        "KaVwFmT9tKFq",
        "OF_kORZLtKF3"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/Sesion%205/B4_Emojify_GRUs_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0lccR8uznTK"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \r\n",
        "\r\n",
        "#**M√°ster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDb9804BzstC"
      },
      "source": [
        "#<strong><center>Emojify!</center></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idxi5zI6tKFJ"
      },
      "source": [
        "En este documento vamos a utilizar representaciones vectoriales de palabras para construir un Emojificador. \n",
        "\n",
        "¬øAlguna vez has querido hacer tus mensajes de texto m√°s expresivos? Tu aplicaci√≥n emojificadora te ayudar√° a hacerlo. La finalidad es que en lugar de escribir \"Congratulations on the promotion! Lets get coffee and talk. Love you!\" , el emojificador puede convertirlo autom√°ticamente en \"Congratulations on the promotion! üëç Lets get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\"\n",
        "\n",
        "*NOTA: Debido a que el vector de entrenamiento de emmbedings es en ingl√©s, hemos de trabajar con frases en ingl√©s. Siempre es bueno practicar idiomas!*\n",
        "\n",
        "Implementar√°s un modelo que introduzca una frase (como \"¬°Vamos a ver el partido de b√©isbol esta noche!\") y encuentre el emoji m√°s apropiado para ser utilizado con esta frase (‚öæÔ∏è). En muchas interfaces de emoji, tienes que recordar que ‚ù§Ô∏è es el s√≠mbolo del \"coraz√≥n\" y no el del \"amor\". Pero si utilizas vectores de palabras, ver√°s que incluso si tu conjunto de entrenamiento relaciona expl√≠citamente s√≥lo unas pocas palabras con un emoji en particular, tu algoritmo ser√° capaz de generalizar y asociar palabras en el conjunto de prueba al mismo emoji incluso si esas palabras ni siquiera aparecen en el conjunto de entrenamiento. Esto te permite construir un clasificador preciso de frases a emojis, incluso utilizando un conjunto de entrenamiento peque√±o. \n",
        "\n",
        "En el ejercicio, empezaremos con un modelo de referencia (Emojifier-V1) que utiliza *word embeddings*, y luego construiremos un modelo m√°s sofisticado (Emojifier-V2) que incorpora una GRU. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55BqdKU8t0PP",
        "outputId": "503989c2-c59a-47cb-9faf-d1f125b1fb31"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4quQWfpFtKFU"
      },
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLKC_gO21OtK"
      },
      "source": [
        "Para el ejercicio necesitamos acceso a varios documentos, de forma que lo mejor es realizar la conexi√≥n con nuestro googel Drive donde debemos tener los siguientes archivos: \r\n",
        "* train_emoji.csv\r\n",
        "* tesss.csv\r\n",
        "* glove.6B.50d.txt\r\n",
        "\r\n",
        "Tambien es necesario poner el fichero \"emo_utils.py\" en el directorio \"/content\" de la ejecucion temporal del notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJNZFplt0_iI",
        "outputId": "b499d5db-cb08-4778-b81e-5d38e0c8f9c7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7aS6_WqtKFV"
      },
      "source": [
        "## 1 - Baseline model: Emojifier-V1\n",
        "\n",
        "### 1.1 - Dataset EMOJISET\n",
        "Empezamos por construir un clasificador de referencia sencillo. \n",
        "\n",
        "Se tiene un peque√±o conjunto de datos (X, Y) donde\n",
        "- X contiene 127 frases (cadenas)\n",
        "- Y contiene una etiqueta entera entre 0 y 4 que corresponde a un emoji para cada frase\n",
        "![Emojiset](https://i.imgur.com/KhFJKMv.png)\n",
        "<caption><center> **Figura 1**: EMOJISET - un problema de clasificaci√≥n con 5 clases. Aqu√≠ se dan algunos ejemplos de frases. </center></caption>\n",
        "\n",
        "Vamos a cargar el conjunto de datos utilizando el c√≥digo siguiente. Dividimos el conjunto de datos entre entrenamiento (127 ejemplos) y prueba (56 ejemplos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oh5fnFSptKFW"
      },
      "source": [
        "X_train, Y_train = read_csv('/content/gdrive/MyDrive/Colab Notebooks/IA3_data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('/content/gdrive/MyDrive/Colab Notebooks/IA3_data/tesss.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "mpyUuCHztKFX"
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kn-_p_5tKFX"
      },
      "source": [
        "Ejecute la siguiente celda para imprimir las frases de X_train y las etiquetas correspondientes de Y_train. Cambie el `√≠ndice` para ver diferentes ejemplos. Debido al tipo de letra que utiliza el cuaderno iPython, el emoji del coraz√≥n puede ser de color negro en lugar de rojo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUoBv-fntKFY",
        "outputId": "d1c26261-4d4c-42ae-d0d4-0c71f3ecf165"
      },
      "source": [
        "index = 1\n",
        "print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am proud of your achievements üòÑ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1YEyJ83tKFZ"
      },
      "source": [
        "### 1.2 - Visi√≥n general del Emojifier-V1\n",
        "\n",
        "En esta parte, vas a implementar un modelo de base llamado \"Emojifier-v1\".  \n",
        "<center>\n",
        "\n",
        "![Emojifier v1](https://i.imgur.com/Lxj2QM0.png)\n",
        "<caption><center> **Figura 2**: Modelo de referencia (Emojifier-V1).</center></caption>.\n",
        "</center>\n",
        "\n",
        "La entrada del modelo es una cadena correspondiente a una frase (por ejemplo, \"Te quiero\"). En el c√≥digo, la salida ser√° un vector de probabilidad de forma (1,5), que luego se pasa en una capa argmax para extraer el √≠ndice de la salida m√°s probable del emoji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMW-ZnSTtKFa"
      },
      "source": [
        "Para obtener nuestras etiquetas en un formato adecuado para el entrenamiento de un clasificador softmax, vamos a convertir $Y$ de su forma actual $(m, 1)$ en una \"representaci√≥n de un solo disparo\" $(m, 5)$, donde cada fila es un vector de un solo disparo que da la etiqueta de un ejemplo. Aqu√≠, `Y_oh` significa \"Y-one-hot\" en los nombres de las variables `Y_oh_train` y `Y_oh_test`: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xVF6KZwvtKFb"
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7D98Xu4tKFb"
      },
      "source": [
        "Veamos lo que hizo `convert_to_one_hot()`. Si√©ntase libre de cambiar `index` para imprimir diferentes valores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sztWKqOytKFb",
        "outputId": "269e8e7f-1238-4bd7-831d-438e2b696459"
      },
      "source": [
        "index = 50\n",
        "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 is converted into one hot [1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBLwBMK0tKFc"
      },
      "source": [
        "Todos los datos est√°n listos para ser introducidos en el modelo Emojify-V1. Vamos a implementar el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qKCvi--tKFd"
      },
      "source": [
        "### 1.3 - Implementaci√≥n del Emojifier-V1\n",
        "\n",
        "Como se muestra en la Figura (2), el primer paso es convertir una frase de entrada en la representaci√≥n de vectores de palabras, que luego se promedian juntos. Al igual que en el ejercicio anterior, utilizaremos incrustaciones GloVe preentrenadas de 50 dimensiones. Ejecute la siguiente celda para cargar el `word_to_vec_map`, que contiene todas las representaciones vectoriales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "p4ppq-uEtKFd"
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/gdrive/MyDrive/Colab Notebooks/IA3_data/glove.6B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn00eQZVtKFe"
      },
      "source": [
        "Has cargado:\n",
        "- `word_to_index`: mapeo del diccionario desde las palabras a sus √≠ndices en el vocabulario (400.001 palabras, con los √≠ndices v√°lidos que van de 0 a 400.000)\n",
        "- `index_to_word`: diccionario que relaciona los √≠ndices con las palabras correspondientes en el vocabulario.\n",
        "- `word_to_vec_map`: diccionario que asigna las palabras a su representaci√≥n vectorial en GloVe.\n",
        "\n",
        "Ejecute la siguiente celda para comprobar si funciona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqffT3T7tKFe",
        "outputId": "1127d248-0785-4c6d-bea3-25ba1f736913"
      },
      "source": [
        "word = \"cucumber\"\n",
        "index = 289846\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the index of cucumber in the vocabulary is 113317\n",
            "the 289846th word in the vocabulary is potatos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibv9NmCNtKFf"
      },
      "source": [
        "Vamos a implementar `sentence_to_avg()`. Deber√° realizar dos pasos:\n",
        "1. Convertir cada frase a min√∫sculas, y luego dividir la frase en una lista de palabras. Puede ser √∫til utilizar `X.lower()` y `X.split()`. \n",
        "2. Para cada palabra de la frase, acceda a su representaci√≥n GloVe. A continuaci√≥n, promedia todos estos valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TpR1sqJUtKFf"
      },
      "source": [
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Convierte una frase (cadena) en una lista de palabras (cadenas). Extrae la representaci√≥n GloVe de cada palabra\n",
        "    y promedia su valor en un √∫nico vector que codifica el significado de la frase.\n",
        "    \n",
        "    Argumentos:\n",
        "    frase -- cadena, un ejemplo de entrenamiento de X\n",
        "    word_to_vec_map -- diccionario que mapea cada palabra de un vocabulario en su representaci√≥n vectorial de 50 dimensiones\n",
        "    \n",
        "    Devuelve:\n",
        "    avg -- vector medio que codifica la informaci√≥n sobre la frase, matriz numpy de forma (50,)\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Split sentence into list of lower case words (‚âà 1 line)\n",
        "    words = [i.lower() for i in sentence.split()]\n",
        "\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros((50,))\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    for w in words:\n",
        "        avg += word_to_vec_map[w]\n",
        "    avg = avg / len(words)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0VQQTy7tKFg",
        "outputId": "ca2213dd-8cd3-49f8-9f65-5a9be7ef3886"
      },
      "source": [
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "print(\"avg = \", avg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
            " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
            "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
            "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
            "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
            "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
            " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
            " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
            "  0.1445417   0.09808667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2mtMMxftKFg"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **avg= **\n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
        " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
        "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
        "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
        "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
        "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
        " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
        " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
        "  0.1445417   0.09808667]\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jtihxKr6tKFh"
      },
      "source": [
        "#### Modelo\n",
        "\n",
        "Ahora tenemos todas las piezas para terminar de implementar la funci√≥n `model()`. Despu√©s de usar `sentence_to_avg()` necesitas pasar el promedio a trav√©s de la propagaci√≥n hacia adelante, calcular el costo, y luego retropropagar para actualizar los par√°metros del softmax. \n",
        "\n",
        "Vamso ahora a implementar la funci√≥n `model()` descrita en la Figura (2). Asumiendo aqu√≠ que $Yoh$ (\"Y one hot\") es la codificaci√≥n one-hot de las etiquetas de salida, las ecuaciones que necesitas implementar en el forward pass y para calcular el coste de la entrop√≠a cruzada son\n",
        "\n",
        "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
        "$$ a^{(i)} = softmax(z^{(i)})$$\n",
        "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$\n",
        "\n",
        "Es posible llegar a una implementaci√≥n vectorizada m√°s eficiente. Pero ya que estamos utilizando un bucle for para convertir las sentencias de una en una en la representaci√≥n $avg^{(i)}$ de todos modos, no vamos a molestarnos esta vez. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMZl_8DitKFh"
      },
      "source": [
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
        "    \"\"\"\n",
        "    Modelo para entrenar representaciones de vectores de palabras en numpy.\n",
        "    \n",
        "    Argumentos:\n",
        "    X -- datos de entrada, array numpy de frases como cadenas, de forma (m, 1)\n",
        "    Y -- etiquetas, matriz numpy de enteros entre 0 y 7, matriz numpy de forma (m, 1)\n",
        "    word_to_vec_map -- diccionario que mapea cada palabra de un vocabulario en su representaci√≥n vectorial de 50 dimensiones\n",
        "    learning_rate -- tasa de aprendizaje para el algoritmo de descenso de gradiente estoc√°stico\n",
        "    num_iterations -- n√∫mero de iteraciones\n",
        "    \n",
        "    Devuelve:\n",
        "    pred -- vector de predicciones, numpy-array de forma (m, 1)\n",
        "    W -- matriz de pesos de la capa softmax, de forma (n_y, n_h)\n",
        "    b -- sesgo de la capa softmax, de forma (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                          # number of training examples\n",
        "    n_y = 5                                 # number of classes  \n",
        "    n_h = 50                                # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
        "        for i in range(m):                                # Loop over the training examples\n",
        "            \n",
        "  \n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer\n",
        "            z = np.dot(W, avg) + b\n",
        "            a = softmax(z)\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
        "\n",
        "            \n",
        "            # Compute gradients \n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
        "\n",
        "    return pred, W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDPXz4rbtKFj",
        "outputId": "739ad297-047d-49c8-f4d2-214bad3448ad"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(X_train[0])\n",
        "print(type(X_train))\n",
        "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
        "print(Y.shape)\n",
        "\n",
        "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
        " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
        " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
        " 'You totally deserve this prize', 'Let us go play football',\n",
        " 'Are you down for football this afternoon', 'Work hard play harder',\n",
        " 'It is suprising how people can be dumb sometimes',\n",
        " 'I am very disappointed','It is the best day in my life',\n",
        " 'I think I will end up alone','My life is so boring','Good job',\n",
        " 'Great so awesome'])\n",
        "\n",
        "print(X.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(type(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(132,)\n",
            "(132,)\n",
            "(132, 5)\n",
            "never talk to me again\n",
            "<class 'numpy.ndarray'>\n",
            "(20,)\n",
            "(20,)\n",
            "(132, 5)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcr9Qn7ltKFk"
      },
      "source": [
        "Ejecute la siguiente celda para entrenar su modelo y aprender los par√°metros softmax (W,b). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEja-X_jtKFl",
        "outputId": "6e4fcdef-cd77-45d6-b539-b44417aa7a69"
      },
      "source": [
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
        "#print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 --- cost = 1.9520498812810076\n",
            "Accuracy: 0.3484848484848485\n",
            "Epoch: 100 --- cost = 0.07971818726014807\n",
            "Accuracy: 0.9318181818181818\n",
            "Epoch: 200 --- cost = 0.04456369243681402\n",
            "Accuracy: 0.9545454545454546\n",
            "Epoch: 300 --- cost = 0.03432267378786059\n",
            "Accuracy: 0.9696969696969697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jip-JhRctKFl"
      },
      "source": [
        "**Salida esperada** (on a subset of iterations):\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 0**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 1.95204988128\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.348484848485\n",
        "        </td>\n",
        "    </tr>\n",
        "\n",
        "\n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 100**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0797181872601\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.931818181818\n",
        "        </td>\n",
        "    </tr>\n",
        "    \n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 200**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0445636924368\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.954545454545\n",
        "        </td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 300**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0343226737879\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.969696969697\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63fzDJn0tKFm"
      },
      "source": [
        "Genial. El modelo tiene una precisi√≥n bastante alta en el conjunto de entrenamiento. Veamos ahora c√≥mo lo hace en el conjunto de pruebas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "gaBjaMvitKFm"
      },
      "source": [
        "### 1.4 - Examinar el rendimiento del conjunto de pruebas \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAaKFbnytKFm",
        "outputId": "1c5222e3-fb56-45f3-f993-883b34a0cb5e"
      },
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print('Test set:')\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set:\n",
            "Accuracy: 0.9772727272727273\n",
            "Test set:\n",
            "Accuracy: 0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBOwQUortKFn"
      },
      "source": [
        "La adivinaci√≥n aleatoria habr√≠a tenido un 20% de precisi√≥n dado que hay 5 clases. Se trata de un rendimiento bastante bueno despu√©s de entrenar con s√≥lo 127 ejemplos. \n",
        "\n",
        "En el conjunto de entrenamiento, el algoritmo vio la frase \"*I love you*\" con la etiqueta ‚ù§Ô∏è. Sin embargo, puedes comprobar que la palabra \"adore\" no aparece en el conjunto de entrenamiento. No obstante, veamos qu√© ocurre si se escribe \"i adore you*\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ3KNPVLtKFo",
        "outputId": "edf41161-8ee1-4997-dd9e-0f68b1ef2eaa"
      },
      "source": [
        "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "print_predictions(X_my_sentences, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8333333333333334\n",
            "\n",
            "i adore you ‚ù§Ô∏è\n",
            "i love you ‚ù§Ô∏è\n",
            "funny lol üòÑ\n",
            "lets play with a ball ‚öæ\n",
            "food is ready üç¥\n",
            "not feeling happy üòÑ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG3TSDU7tKFo"
      },
      "source": [
        "Sorprendente. Como *adore* tiene una embedding similar a la de *love*, el algoritmo ha generalizado correctamente incluso con una palabra que nunca hab√≠a visto antes. Palabras como *heart*, *dear*, *beloved* or *adore* tienen vectores de embedding similares a *love*, por lo que tambi√©n podr√≠an funcionar; si√©ntase libre de modificar las entradas anteriores y probar una variedad de frases de entrada. ¬øQu√© tal funciona?\n",
        "\n",
        "Observa que no acierta con \"not feeling happy\". Este algoritmo ignora el orden de las palabras, por lo que no es bueno para entender frases como \"no happy\". \n",
        "\n",
        "Imprimir la matriz de confusi√≥n tambi√©n puede ayudar a entender qu√© clases son m√°s dif√≠ciles para tu modelo. Una matriz de confusi√≥n muestra la frecuencia con la que un ejemplo cuya etiqueta es de una clase (clase \"real\") es etiquetado err√≥neamente por el algoritmo con una clase diferente (clase \"predicha\"). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "EFPIq8_itKFo",
        "outputId": "0fda061b-ad52-475f-cf48-b57512f86f1e"
      },
      "source": [
        "print(Y_test.shape)\n",
        "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56,)\n",
            "           ‚ù§Ô∏è    ‚öæ    üòÑ    üòû   üç¥\n",
            "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
            "Actual                                 \n",
            "0            6    0    0    1    0    7\n",
            "1            0    8    0    0    0    8\n",
            "2            2    0   16    0    0   18\n",
            "3            1    1    2   12    0   16\n",
            "4            0    0    1    0    6    7\n",
            "All          9    9   19   13    6   56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ChmSA4dUtKFp"
      },
      "source": [
        "<font color='blue'>\n",
        "**Lo MAS IMPORTANTE de esta parte**:\n",
        "\n",
        "- Incluso con 127 ejemplos de entrenamiento, puedes obtener un modelo razonablemente bueno para la emojificaci√≥n. Esto se debe al poder de generalizaci√≥n que le dan los vectores de palabras. \n",
        "- Emojify-V1 tendr√° un mal rendimiento en frases como *\"Esta pel√≠cula no es buena y no es agradable \"* porque no entiende las combinaciones de palabras -simplemente promedia todos los vectores de embedding de las palabras, sin prestar atenci√≥n al orden de las palabras. En la siguiente parte construir√°s un algoritmo mejor. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHyD9auntKFp"
      },
      "source": [
        "## 2 - Emojifier-V2: Usando GRUs en Keras: \n",
        "\n",
        "Vamos a construir un modelo con GRUS que tome como entrada secuencias de palabras. Este modelo ser√° capaz de tener en cuenta el orden de las palabras. Emojifier-V2 continuar√° utilizando incrustaciones de palabras pre-entrenadas para representar las palabras, pero las alimentar√° en un GRU, cuyo trabajo es predecir el emoji m√°s apropiado. \n",
        "\n",
        "Ejecuta la siguiente celda para cargar los paquetes Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGokrRLatKFp"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, GRU, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQ30GvJtKFq"
      },
      "source": [
        "### 2.1 - Visi√≥n general del modelo\n",
        "\n",
        "Aqu√≠ est√° el Emojifier-v2 que implementar√°s:\n",
        "\n",
        "![Emojifier v2](https://i.imgur.com/zDC1Z2U.png)\n",
        "<caption><center> **Figura 3**: Emojifier-V2. Un clasificador de secuencias RGU de 2 capas. </center></caption>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaVwFmT9tKFq"
      },
      "source": [
        "### 2.2 Keras y el mini-batching \n",
        "\n",
        "En este ejercicio, queremos entrenar a Keras usando mini-lotes. Sin embargo, la mayor√≠a de los marcos de aprendizaje profundo requieren que todas las secuencias en el mismo mini-batch tengan la misma longitud. Esto es lo que permite que la vectorizaci√≥n funcione: Si tuvi√©ramos una frase de 3 palabras y otra de 4, los c√°lculos necesarios para ellas son diferentes (una requiere 3 pasos de un GRU, otra requiere 4 pasos), por lo que no es posible hacer ambas al mismo tiempo.\n",
        "\n",
        "La soluci√≥n com√∫n a esto es utilizar el relleno. En concreto, se establece una longitud m√°xima de secuencia y se rellenan todas las secuencias con la misma longitud. Por ejemplo, si la longitud m√°xima de la secuencia es 20, podr√≠amos rellenar cada frase con \"0\" para que cada frase de entrada tenga una longitud de 20. As√≠, una frase \"te quiero\" se representar√≠a como $(e_{i}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$. En este ejemplo, cualquier frase de m√°s de 20 palabras tendr√≠a que ser truncada. Una forma sencilla de elegir la longitud m√°xima de la secuencia es simplemente elegir la longitud de la frase m√°s larga del conjunto de entrenamiento. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSuTdszRtKFq"
      },
      "source": [
        "### 2.3 - La capa de embedding\n",
        "\n",
        "En Keras, la matriz de embedding se representa como una \"capa\", y mapea enteros positivos (√≠ndices correspondientes a las palabras) en vectores densos de tama√±o fijo (los vectores de embedding). Puede ser entrenada o inicializada con una embedding preentrenada. En esta parte, aprender√°s a crear una capa [Embedding()](https://keras.io/layers/embeddings/) en Keras, inicializ√°ndola con los vectores GloVe de 50 dimensiones cargados anteriormente en el cuaderno. Como nuestro conjunto de entrenamiento es bastante peque√±o, no actualizaremos las incrustaciones de palabras, sino que dejaremos sus valores fijos. Pero en el c√≥digo siguiente, mostraremos c√≥mo Keras permite entrenar o dejar fija esta capa.  \n",
        "\n",
        "La capa `Embedding()` toma como entrada una matriz entera de tama√±o (tama√±o del lote, longitud m√°xima de la entrada). Esta corresponde a sentencias convertidas en listas de √≠ndices (enteros), como se muestra en la siguiente figura.\n",
        "![Embedding1](https://i.imgur.com/g4xvysH.png)\n",
        "\n",
        "<caption><center> **Figura 4**: Capa de embedding. Este ejemplo muestra la propagaci√≥n de dos ejemplos a trav√©s de la capa de embedding. Ambos han sido rellenados con cero hasta una longitud de `max_len=5`. La dimensi√≥n final de la representaci√≥n es `(2,max_len,50)` porque las incrustaciones de palabras que estamos utilizando son de 50 dimensiones. </center></caption>\n",
        "\n",
        "El mayor n√∫mero entero (es decir, el √≠ndice de la palabra) en la entrada no debe ser mayor que el tama√±o del vocabulario. La capa da salida a un array de forma (tama√±o del lote, longitud m√°xima de la entrada, dimensi√≥n de los vectores de palabras).\n",
        "\n",
        "El primer paso es convertir todas las frases de entrenamiento en listas de √≠ndices, y luego poner a cero todas estas listas para que su longitud sea la de la frase m√°s larga. \n",
        "\n",
        "Implementamos la funci√≥n siguiente para convertir X (matriz de sentencias como cadenas) en una matriz de √≠ndices correspondientes a las palabras de las sentencias. La forma de salida debe ser tal que se pueda dar a `Embedding()` (descrito en la Figura 4). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0dyrKqEctKFr"
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Convierte una matriz de frases (cadenas) en una matriz de √≠ndices correspondientes a las palabras de las frases.\n",
        "    La forma de salida debe ser tal que se pueda dar a `Embedding()` (descrito en la Figura 4). \n",
        "    \n",
        "    Argumentos:\n",
        "    X -- matriz de frases (cadenas), de forma (m, 1)\n",
        "    word_to_index -- un diccionario que contiene cada palabra asignada a su √≠ndice\n",
        "    max_len -- n√∫mero m√°ximo de palabras en una frase. Se puede asumir que cada frase en X no es m√°s larga que esto. \n",
        "    \n",
        "    Devuelve:\n",
        "    X_indices -- array de √≠ndices correspondientes a las palabras de las sentencias de X, de forma (m, max_len)\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = X.shape[0]                                   \n",
        "\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (‚âà 1 line)\n",
        "    X_indices = np.zeros((m, max_len))\n",
        "\n",
        "    # loop over training examples\n",
        "    for i in range(m):                               \n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = [w.lower() for w in X[i].split()]\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            # Increment j to j + 1\n",
        "            j += 1\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kEWahKetKFr"
      },
      "source": [
        "Chequeamos `sentences_to_indices()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABX5Bh9BtKFr",
        "outputId": "82563781-61f0-418d-d934-e16ff2c2fcbf"
      },
      "source": [
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
            "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
            " [220930. 286375.  69714.      0.      0.]\n",
            " [151204. 192973. 302254. 151349. 394475.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qts9amsVtKFs"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **X1 =**\n",
        "        </td>\n",
        "        <td>\n",
        "           ['funny lol' 'lets play football' 'food is ready for you']\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **X1_indices =**\n",
        "        </td>\n",
        "        <td>\n",
        "           [[ 155345.  225122.       0.       0.       0.] <br>\n",
        "            [ 220930.  286375.  151266.       0.       0.] <br>\n",
        "            [ 151204.  192973.  302254.  151349.  394475.]]\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f35MPre8tKFs"
      },
      "source": [
        "Vamos a construir la capa `Embedding()` en Keras, utilizando vectores de palabras pre-entrenados. Una vez construida esta capa, le pasaremos la salida de `sentences_to_indices()` como entrada, y la capa `Embedding()` devolver√° las incrustaciones de palabras para una frase. \n",
        "\n",
        "Implementaremos la capa `pretrained_embedding()`. Deber√°s realizar los siguientes pasos:\n",
        "1. Inicializar la matriz de embedding como un array numpy de ceros con la forma correcta.\n",
        "2. Rellenar la matriz de embedding con todas las incrustaciones de palabras extra√≠das de `word_to_vec_map`.\n",
        "3. Definir la capa de embedding de Keras. Utilice [Embedding()](https://keras.io/layers/embeddings/). Aseg√∫rese de hacer que esta capa no sea entrenable, estableciendo `trainable = False` cuando llame a `Embedding()`. Si establece `trainable = True`, entonces permitir√° que el algoritmo de optimizaci√≥n modifique los valores de las incrustaciones de palabras. \n",
        "4. Establezca los pesos de embedding para que sean iguales a la matriz de embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "D-XY1AK2tKFs"
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Crea una capa de Keras Embedding() y carga en ella vectores preentrenados GloVe de 50 dimensiones.\n",
        "    \n",
        "    Argumentos:\n",
        "    word_to_vec_map -- diccionario que mapea las palabras a su representaci√≥n vectorial en GloVe.\n",
        "    word_to_index -- diccionario que mapea las palabras a sus √≠ndices en el vocabulario (400.001 palabras)\n",
        "\n",
        "    Devuelve:\n",
        "    embedding_layer -- instancia de Keras de la capa preentrenada\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFESHM4itKFt",
        "outputId": "df55913d-0519-44bf-f5ab-3dfaf2290324"
      },
      "source": [
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights[0][1][3] = -0.3403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucGmUvVctKFt"
      },
      "source": [
        "** Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **weights[0][1][3] =**\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.3403\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkSIvkqFtKFu"
      },
      "source": [
        "## 2.3 Construir el Emojifier-V2\n",
        "\n",
        "Construyamos ahora el modelo Emojifier-V2. Lo har√°s utilizando la capa de embedding que has construido, y alimentar√°s su salida a una red GRU. \n",
        "![Emojifier v2](https://i.imgur.com/zDC1Z2U.png)\n",
        "<caption><center> **Figura 3**: Emojifier-v2. Un clasificador de secuencias GRU de 2 capas. </center></caption>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtR7WWzTFDu7"
      },
      "source": [
        "Implementaremos `Emojify_V2()`, que construye un gr√°fico Keras de la arquitectura mostrada en la Figura 3. El modelo toma como entrada un array de frases de forma (`m`, `max_len`, ) definido por `input_shape`. Deber√≠a dar como salida un vector de probabilidad softmax de forma (`m`, `C = 5`). Puede necesitar `Input(shape = ..., dtype = '...')`, [GRU()](https://keras.io/layers/recurrent/#GRU), [Dropout()](https://keras.io/layers/core/#dropout), [Dense()](https://keras.io/layers/core/#dense), y [Activation()](https://keras.io/activations/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aSckngTXtKFu"
      },
      "source": [
        "# Emojify_V2\n",
        "\n",
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Funci√≥n que crea el gr√°fico del modelo Emojify-v2.\n",
        "    \n",
        "    Argumentos:\n",
        "    input_shape -- forma de la entrada, normalmente (max_len,)\n",
        "    word_to_vec_map -- diccionario que mapea cada palabra del vocabulario en su representaci√≥n vectorial de 50 dimensiones\n",
        "    word_to_index -- diccionario que asigna las palabras a sus √≠ndices en el vocabulario (400.001 palabras)\n",
        "\n",
        "    Devuelve:\n",
        "    model -- una instancia del modelo en Keras\n",
        "    \"\"\"\n",
        "\n",
        "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
        "    sentence_indices = Input(input_shape, dtype='int32')\n",
        "    \n",
        "    # Create the embedding layer pretrained with GloVe Vectors (‚âà1 line)\n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "    embeddings = embedding_layer(sentence_indices)   \n",
        "    \n",
        "    # Propagate the embeddings through an GRU layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a batch of sequences.\n",
        "    X = GRU(128, return_sequences=True)(embeddings)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X trough another GRU layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = GRU(128, return_sequences=False)(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
        "    X = Dense(5)(X)\n",
        "    # Add a softmax activation\n",
        "    X = Activation('softmax')(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(inputs=sentence_indices, outputs=X)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IiM3hK4tKFu"
      },
      "source": [
        "Ejecute la siguiente celda para crear su modelo y comprobar su resumen. Como todas las frases del conjunto de datos tienen menos de 10 palabras, elegimos `max_len = 10`.  \r\n",
        "Si revisamos la arquitectura, vemos que utiliza \"20.168.887\" par√°metros, de los cuales 20.000.050 (las incrustaciones de palabras) no son entrenables, y los 168.837 restantes s√≠. Como nuestro vocabulario tiene 400.001 palabras (con √≠ndices v√°lidos de 0 a 400.000) hay 400.001*50 = 20.000.050 par√°metros no entrenables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6aEIdUTtKFu",
        "outputId": "045394ef-56b6-4d9d-a0e6-d4fec7968b52"
      },
      "source": [
        "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 10)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 50)            20000050  \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 10, 128)           69120     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 20,168,887\n",
            "Trainable params: 168,837\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_r7LHSVtKFv"
      },
      "source": [
        "Como siempre, despu√©s de crear tu modelo en Keras, necesitas compilarlo y definir qu√© p√©rdida, optimizador y m√©trica quieres usar. Compila tu modelo usando la p√©rdida `categorical_crossentropy`, el optimizador `adam` y la m√©trica `['accuracy']`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PX_R2zuutKFv"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfcgaV6tKFw"
      },
      "source": [
        "Es hora de entrenar tu modelo. Tu `modelo` de Emojifier-V2 toma como entrada un array de formas (`m`, `max_len`) y da como salida vectores de probabilidad de formas (`m`, `n√∫mero de clases`). Por lo tanto, tenemos que convertir X_train (matriz de frases como cadenas) en X_train_indices (matriz de frases como lista de √≠ndices de palabras), e Y_train (etiquetas como √≠ndices) en Y_train_oh (etiquetas como vectores de un solo golpe)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iQt1rKGUtKFw"
      },
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIYQcsFtKFw"
      },
      "source": [
        "Ajusta el modelo Keras en `X_train_indices` y `Y_train_oh`. Utilizaremos `epochs = 50` y `batch_size = 32`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6rKZD20tKF0",
        "outputId": "7922e661-f610-47c8-a0ca-afb5d3f5c1e1"
      },
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 8.0368e-04 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 9.8403e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f319b3bf198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9CLRk89tKF1"
      },
      "source": [
        "El modelo debe tener una precisi√≥n cercana al **100%** en el conjunto de entrenamiento. La precisi√≥n exacta que obtenga puede ser un poco diferente. Ejecuta la siguiente celda para evaluar tu modelo en el conjunto de pruebas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KMawwTltKF1",
        "outputId": "3040199e-e2a5-4a67-8973-335f9afb054f"
      },
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4234 - accuracy: 0.8571\n",
            "\n",
            "Test accuracy =  0.8571428656578064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkeLlnsutKF2"
      },
      "source": [
        "Deber√≠a obtener una precisi√≥n de la prueba entre el 80% y el 95%. Ejecuta la celda de abajo para ver los ejemplos mal etiquetados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpYsP2yEtKF2",
        "outputId": "42b92047-1bbb-477b-8ca2-8234e8a0daf1"
      },
      "source": [
        "C = 5\n",
        "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
            "Expected emoji:‚ù§Ô∏è prediction: I love taking breaks\tüòû\n",
            "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
            "Expected emoji:üòû prediction: she is a bully\t‚ù§Ô∏è\n",
            "Expected emoji:üòû prediction: My life is so boring\t‚ù§Ô∏è\n",
            "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
            "Expected emoji:‚öæ prediction: he can pitch really well\tüòÑ\n",
            "Expected emoji:‚ù§Ô∏è prediction: family is all I have\tüòû\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1lYq921tKF2"
      },
      "source": [
        "Ahora puedes probarlo con tu propio ejemplo. Escribe tu propia frase a continuaci√≥n. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5UjT_OhtKF3",
        "outputId": "a8c45f50-c957-4c76-d7f8-eab915eba687"
      },
      "source": [
        "# Juega con diferentes frases para ver las predecciones. Asegurate que las palabras existen en Glove embeddings.  \n",
        "# Sentences= [\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "x_test = np.array(['congratulations for the job'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "congratulations for the job üòÑ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liIQQR7BtKF3"
      },
      "source": [
        "Prueba diferentes frases y observa los resultados. Ten en cuenta varias cosas: \n",
        "* las salidas de Keras son ligeramente aleatorias cada vez, as√≠ que puede que no haya obtenido el mismo resultado)\n",
        "* El modelo actual todav√≠a no es muy robusto a la hora de entender la negaci√≥n (como \"no ser feliz\") porque el conjunto de entrenamiento es peque√±o y, por tanto, no tiene muchos ejemplos de negaci√≥n. Pero si el conjunto de entrenamiento fuera m√°s grande, el modelo GRU ser√≠a mucho mejor que el modelo Emojify-V1 para entender esas frases complejas. \n",
        "\n",
        "¬øQue posibilidades de mejora crees que tiene? ¬øPodr√≠amos usar alguna arquitectura mejor?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF_kORZLtKF3"
      },
      "source": [
        "**Lo que debes recordar**:\n",
        "- Si tiene una tarea de PLN en la que el conjunto de entrenamiento es peque√±o, el uso de incrustaciones de palabras puede ayudar a su algoritmo de forma significativa. Las incrustaciones de palabras permiten que tu modelo trabaje con palabras en el conjunto de prueba que pueden no haber aparecido en tu conjunto de entrenamiento. \n",
        "- El entrenamiento de modelos de secuencia en Keras (y en la mayor√≠a de los otros marcos de aprendizaje profundo) requiere algunos detalles importantes:\n",
        "    - Para utilizar minilotes, las secuencias deben ser rellenadas para que todos los ejemplos de un minilote tengan la misma longitud. \n",
        "    - Una capa `Embedding()` puede ser inicializada con valores pre-entrenados. Estos valores pueden ser fijos o entrenados posteriormente en su conjunto de datos. Sin embargo, si su conjunto de datos etiquetados es peque√±o, normalmente no vale la pena intentar entrenar un gran conjunto preentrenado de incrustaciones.   \n",
        "    - `GRU()` tiene una bandera llamada `return_sequences` para decidir si quieres devolver todos los estados ocultos o s√≥lo el √∫ltimo. \n",
        "    - Puedes usar `Dropout()` justo despu√©s de `GRU()` para regularizar tu red. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tssylD8C_pMJ"
      },
      "source": [
        "## Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJE1_qv6-_XT"
      },
      "source": [
        "* Documento inspirado en los cuadernos del curso Deep Learning de Andrew Ng en Coursera: https://es.coursera.org/\r\n",
        "* Reconocimiento tambien a Alison Darcy y al equipo de Woebot por el asesoramiento en la creaci√≥n de esta tarea:  http://woebot.io\r\n",
        "* Doc oficial Pytorch https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\r\n",
        "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5icC_T58_rLU"
      },
      "source": [
        "##Fin del cuaderno"
      ]
    }
  ]
}