{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "xxuVc",
      "launcher_item_id": "X20PE"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "colab": {
      "name": "0-RNN_paso_paso.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UDYfKV10eXvJ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/Sesion%205/0_RNN_paso_paso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8BYubh3vOKG"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \r\n",
        "\r\n",
        "#**Máster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hmuBvi7wAi2"
      },
      "source": [
        "# <strong><center>Construyendo una Red Neuronal Recurrente - Paso a Paso - Ejercicio</strong>\r\n",
        "En este cuaderno, vamos a implementar una Red Neuronal Recurrente en numpy.\r\n",
        "\r\n",
        "Las Redes Neuronales Recurrentes (RNN) son muy efectivas para el Procesamiento del Lenguaje Natural y otras tareas de secuencia porque tienen \"memoria\". Pueden leer entradas $x^{langle t \\rangle}$ (como palabras) de una en una, y recordar alguna información/contexto a través de las activaciones de la capa oculta que se pasan de un paso de tiempo al siguiente. Esto permite a una RNN unidireccional tomar información del pasado para procesar entradas posteriores. Una RNN bidireccional puede tomar el contexto tanto del pasado como del futuro. \r\n",
        "\r\n",
        "**Nota**:\r\n",
        "- El superíndice $[l]$ indica un objeto asociado a la capa $l^{th}$. \r\n",
        "    - Ejemplo: $a^{[4]}$ es la activación de la capa $4^{th}$. $W^{[5]}$ y $b^{[5]}$ son los parámetros de la capa de $5^{th}$.\r\n",
        "\r\n",
        "- El superíndice $(i)$ indica un objeto asociado al ejemplo $i^{th}$. \r\n",
        "    - Ejemplo: $x^{(i)}$ es la entrada del ejemplo de entrenamiento $i^{th}$.\r\n",
        "\r\n",
        "- El superíndice $\\langle t \\rangle$ denota un objeto en el $t^{th}$ paso de tiempo. \r\n",
        "    - Ejemplo: $x^{\\langle t \\rangle}$ es la entrada x en el paso de tiempo $t^{th}$. $x^ {(i)\\langle t \\rangle}$ es la entrada en el paso de tiempo $t^{th}$ del ejemplo $i$.\r\n",
        "    \r\n",
        "- El guión bajo $i$ denota la entrada $i^{th}$ de un vector.\r\n",
        "    - Ejemplo: $a^{[l]}_i$ denota la entrada $i^{th}$ de las activaciones en la capa $l$.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBmEK61Fcyzb"
      },
      "source": [
        "Importación de paquetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4iITL9dQyu"
      },
      "source": [
        "*****\r\n",
        "**NOTA**: Es importante poner al alcance de Colab el script \"rnn_utils.py\" suministrado. Para ello debeis subirlo al directorio temporal \"/content/\" de vuestra sesion en ese momento\r\n",
        "*****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp8qzhKkcyzc"
      },
      "source": [
        "import numpy as np\n",
        "from rnn_utils import *"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LasHBbt6G7Me"
      },
      "source": [
        "##INCLUYO EN ESTA CELDA EL CONTENIDO DE rnn_utils. Es un archivo .py\n",
        "# que incluye algunas funciones necesarias para hacer los ejercicios.\n",
        "#\n",
        "# NO EJECUTAR ESTA CELDA CUANDO SE CARGUE EL ARCHIVO, tal y como se\n",
        "# explica en la nota de arriba\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "    ### START CODE HERE ### (approx. 4 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v, s\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQcVj6Q4cyzd"
      },
      "source": [
        "## 1 - Propagación hacia delante en una Red Neuronal Recurrente básica\n",
        "\n",
        "Una RNN básica que implementará tiene la estructura siguiente. En este ejemplo, $T_x = T_y$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cYt4Oqcyzd"
      },
      "source": [
        "![RNN_basica](https://i.imgur.com/e5aY9bf.png)\n",
        "\n",
        "<caption><center> **Figura 1**: Basic RNN model </center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyC5maFmx6jM"
      },
      "source": [
        "Así es como se puede implementar una RNN: \r\n",
        "\r\n",
        "**Pasos**:\r\n",
        "1. Implementar los cálculos necesarios para un paso de tiempo de la RNN.\r\n",
        "2. Implementar un bucle sobre $T_x$ pasos de tiempo para procesar todas las entradas, una a la vez. \r\n",
        "\r\n",
        "### 1.1 - Célula RNN\r\n",
        "\r\n",
        "Una red neuronal recurrente puede ser vista como la repetición de una sola célula. Primero vas a implementar los cálculos para un solo paso de tiempo. La siguiente figura describe las operaciones para un solo paso de tiempo de una célula RNN. \r\n",
        "\r\n",
        "![RNN_forward](https://i.imgur.com/gnr9GLl.png)\r\n",
        "<caption><center> **Figura 2**: Celda RNN básica. Toma como entrada $x^{\\langle t \\rangle}$ (entrada actual) y $a^{\\langle t - 1\\rangle}$ (estado oculto anterior que contiene información del pasado), y da como resultado $a^{\\langle t \\rangle}$ que se da a la siguiente célula RNN y también se utiliza para predecir $y^{\\langle t \\rangle}$ </center></caption>.\r\n",
        "\r\n",
        "**Ejercicio**: Implementar la célula RNN descrita en la figura (2).\r\n",
        "\r\n",
        "**Instrucciones**:\r\n",
        "1. Calcular el estado oculto con activación tanh: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\r\n",
        "2. Utilizando su nuevo estado oculto $a^{\\langle t \\rangle}$, calcular la predicción $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. Le proporcionamos una función: `softmax`.\r\n",
        "3. Almacenar $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parámetros)$ en la caché\r\n",
        "4. Devolver $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ y caché\r\n",
        "\r\n",
        "Vectorizaremos sobre $m$ ejemplos. Así, $x^{\\langle t \\rangle}$ tendrá dimensión $(n_x,m)$, y $a^{\\langle t \\rangle}$ tendrá dimensión $(n_a,m)$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VPsyOmLcyzf"
      },
      "source": [
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implementa un único paso adelante de la célula RNN como se describe en la figura (2)\n",
        "\n",
        "    Argumentos:\n",
        "    xt -- sus datos de entrada en el paso de tiempo \"t\", matriz numpy de forma (n_x, m).\n",
        "    a_prev -- Estado oculto en el paso de tiempo \"t-1\", matriz numpy de forma (n_a, m)\n",
        "    parameters -- diccionario python que contiene:\n",
        "                        Wax -- Matriz de pesos que multiplica la entrada, matriz numpy de forma (n_a, n_x)\n",
        "                        Waa -- Matriz de pesos multiplicando el estado oculto, matriz numpy de forma (n_a, n_a)\n",
        "                        Wya -- Matriz de pesos que relaciona el estado oculto con la salida, matriz numpy de forma (n_y, n_a)\n",
        "                        ba -- Bias, matriz numpy de forma (n_a, 1)\n",
        "                        by -- Sesgo que relaciona el estado oculto con la salida, matriz numpy de forma (n_y, 1)\n",
        "    Devuelve:\n",
        "    a_next -- siguiente estado oculto, de forma (n_a, m)\n",
        "    yt_pred -- predicción en el paso de tiempo \"t\", matriz numpy de forma (n_y, m)\n",
        "    cache -- tupla de valores necesarios para el pase hacia atrás, contiene (a_next, a_prev, xt, parameters)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Recupera parametros de \"parameters\"\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    ### INICIA TU CODIGO AQUÍ ###(≈2 lines)\n",
        "    \n",
        "    # Calcula el próximo estado de activación según la formula dada\n",
        "    a_next = np.tanh(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)\n",
        "    # Calcula la salida de la celda según la formula dada\n",
        "    yt_pred = softmax(np.dot(Wya,a_next)+by)\n",
        "    \n",
        "    ### ACABA TU CODIGO AQUÍ ###\n",
        "    \n",
        "    # guarda valores que necesitaremos para la backward propagation en cache\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "    \n",
        "    return a_next, yt_pred, cache"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBFQbvQNcyzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889e4751-6ba4-4d32-d2c9-f9638b0541d3"
      },
      "source": [
        "# Ejecuta esta celda para comprobar. \n",
        "# Los resultados deben coincidir con \"Salida esperada\"\n",
        "\n",
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "print(\"a_next[4] = \", a_next[4])\n",
        "print(\"a_next.shape = \", a_next.shape)\n",
        "print(\"yt_pred[1] =\", yt_pred[1])\n",
        "print(\"yt_pred.shape = \", yt_pred.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a_next[4] = ', array([ 0.59584544,  0.18141802,  0.61311866,  0.99808218,  0.85016201,\n",
            "        0.99980978, -0.18887155,  0.99815551,  0.6531151 ,  0.82872037]))\n",
            "('a_next.shape = ', (5, 10))\n",
            "('yt_pred[1] =', array([0.9888161 , 0.01682021, 0.21140899, 0.36817467, 0.98988387,\n",
            "       0.88945212, 0.36920224, 0.9966312 , 0.9982559 , 0.17746526]))\n",
            "('yt_pred.shape = ', (2, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OErNsy1qcyzi"
      },
      "source": [
        "**Salida esperada**: \n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **a_next[4]**:\n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
        " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **a_next.shape**:\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **yt[1]**:\n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
        "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **yt.shape**:\n",
        "        </td>\n",
        "        <td>\n",
        "           (2, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR-US2rRcyzi"
      },
      "source": [
        "### 1.2 - Paso adelante de la RNN \n",
        "\n",
        "Puedes ver una RNN como la repetición de la celda que acabas de construir. Si su secuencia de datos de entrada se lleva a cabo durante 10 pasos de tiempo, entonces usted va a copiar la célula RNN 10 veces. Cada célula toma como entrada el estado oculto de la célula anterior ($a^{\\langle t-1 \\rangle}$) y los datos de entrada del paso de tiempo actual ($x^{\\langle t \\rangle}$). Se emite un estado oculto ($a^{\\langle t \\rangle}$) y una predicción ($y^{\\langle t \\rangle}$) para este paso de tiempo.\n",
        "\n",
        "![RNN](https://i.imgur.com/X88Lhk2.png)\n",
        "\n",
        "<caption><center> **Figura 3**: RNN básica. La secuencia de entrada $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ se lleva a cabo durante $T_x$ pasos de tiempo. Las salidas de la red $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
        "\n",
        "\n",
        "\n",
        "**Ejercicio**: Codificar la propagación hacia delante de la RNN descrita en la figura (3).\n",
        "\n",
        "**Instrucciones**:\n",
        "1. Crear un vector de ceros ($a$) que almacenará todos los estados ocultos computados por la RNN.\n",
        "2. Inicializar el \"siguiente\" estado oculto como $a_0$ (estado oculto inicial).\n",
        "3. Comenzar un bucle sobre cada paso de tiempo, su índice incremental es $t$ :\n",
        "    - Actualizar el \"siguiente\" estado oculto y la caché ejecutando `rnn_cell_forward`.\n",
        "    - Almacenar el \"próximo\" estado oculto en $a$ ($t^{th}$ posición) \n",
        "    - Almacenar la predicción en y\n",
        "    - Añadir la caché a la lista de cachés\n",
        "4. Retornar $a$, $y$ y las cachés"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdTzNhipcyzj"
      },
      "source": [
        "def rnn_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implementa la propagación hacia delante de la red neuronal recurrente descrita en la figura (3).\n",
        "\n",
        "    Argumentos:\n",
        "    x -- Datos de entrada para cada paso de tiempo, de forma (n_x, m, T_x).\n",
        "    a0 -- Estado oculto inicial, de forma (n_a, m)\n",
        "    parámetros -- diccionario de python que contiene:\n",
        "                        Waa -- Matriz de pesos que multiplica el estado oculto, matriz numpy de forma (n_a, n_a)\n",
        "                        Wax -- Matriz de pesos multiplicando la entrada, array numpy de forma (n_a, n_x)\n",
        "                        Wya -- Matriz de pesos que relaciona el estado oculto con la salida, matriz numpy de forma (n_y, n_a)\n",
        "                        ba -- Matriz numpy de forma (n_a, 1)\n",
        "                        by -- Sesgo que relaciona el estado oculto con la salida, matriz numpy de forma (n_y, 1)\n",
        "\n",
        "    Devuelve:\n",
        "    a -- Estados ocultos para cada paso de tiempo, matriz numpy de forma (n_a, m, T_x)\n",
        "    y_pred -- Predicciones para cada paso de tiempo, matriz numpy de forma (n_y, m, T_x)\n",
        "    caches -- tupla de valores necesarios para el pase hacia atrás, contiene (lista de caches, x)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Inicializa \"caches\" que contendrá la lista de caches\n",
        "    caches = []\n",
        "    \n",
        "    # Recupera dimensiones de x y Wy\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\n",
        "    \n",
        "    ### INICIA TU CODIGO AQUÍ ###\n",
        "    \n",
        "    # Inicializa \"a\" y \"y\" con ceros (≈2 lines)\n",
        "    a = np.zeros(shape=(n_a, m, T_x))\n",
        "    y_pred = np.zeros(shape=(n_y, m, T_x))\n",
        "    \n",
        "    # Inicializa a_next (≈1 line)\n",
        "    a_next = a0\n",
        "    \n",
        "    # bucle sobre los time-steps\n",
        "    for t in range(T_x):\n",
        "        #  Actualiza próximo hidden state, calcula la prediccion, coge la cache (≈1 line)\n",
        "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a, parameters)\n",
        "        # Guarda el valor del \"nuevo\" estado oculto en a (≈1 line)\n",
        "        a[:,:,t] = a_next\n",
        "        # Guarda el calor de la prediccion en y (≈1 line)\n",
        "        y_pred[:,:,t] = yt_pred\n",
        "        # Añade \"cache\" a \"caches\" (≈1 line)\n",
        "        \n",
        "        \n",
        "    ### ACABA TU CODIGO AQUÍ ###\n",
        "    \n",
        "    # guarda valores que necesitaremos para la backward propagation en cache\n",
        "    caches = (caches, x)\n",
        "    \n",
        "    return a, y_pred, caches"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbqp-dFpcyzk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "cbebd22c-fa97-4eb3-dbc1-0fcf9ff699e8"
      },
      "source": [
        "# Ejecuta esta celda para comprobar. \n",
        "# Los resultados deben coincidir con \"Salida esperada\"\n",
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wax = np.random.randn(5,3)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
        "print(\"a[4][1] = \", a[4][1])\n",
        "print(\"a.shape = \", a.shape)\n",
        "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
        "print(\"y_pred.shape = \", y_pred.shape)\n",
        "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
        "print(\"len(caches) = \", len(caches))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e56f0e24cc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Waa\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wax\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wya\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ba\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"by\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a[4][1] = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a.shape = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-120828802e5c>\u001b[0m in \u001b[0;36mrnn_forward\u001b[0;34m(x, a0, parameters)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#  Actualiza próximo hidden state, calcula la prediccion, coge la cache (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0ma_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_cell_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Guarda el valor del \"nuevo\" estado oculto en a (≈1 line)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7f5865b850ab>\u001b[0m in \u001b[0;36mrnn_cell_forward\u001b[0;34m(xt, a_prev, parameters)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Calcula el próximo estado de activación según la formula dada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWaa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Calcula la salida de la celda según la formula dada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0myt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWya\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (5,5) and (5,10,4) not aligned: 5 (dim 1) != 10 (dim 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QJcwsXacyzl"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **a[4][1]**:\n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **a.shape**:\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10, 4)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **y[1][3]**:\n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **y.shape**:\n",
        "        </td>\n",
        "        <td>\n",
        "           (2, 10, 4)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **cache[1][1][3]**:\n",
        "        </td>\n",
        "        <td>\n",
        "           [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **len(cache)**:\n",
        "        </td>\n",
        "        <td>\n",
        "           2\n",
        "        </td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf52x1Lccyzm"
      },
      "source": [
        "Enhorabuena. Has construido con éxito la propagación hacia delante de una red neuronal recurrente desde cero. Esto funcionará lo suficientemente bien para algunas aplicaciones, pero sufre de problemas, uno de los principales es la fuga  de gradiente o *vanishing gradient*. Por lo tanto, funciona mejor cuando cada salida $y^{\\langle t \\rangle}$ se puede estimar utilizando principalmente el contexto \"local\" (es decir, la información de las entradas $x^{\\langle t' \\rangle}$ donde $t'$ no está demasiado lejos de $t$). \n",
        "\n",
        "A continuación vamos a estudiar su retropropagación para ir actualizando los pesos e ir mejorando los resultados. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sQYozBDcyzt"
      },
      "source": [
        "## 2 - Retropropagación en redes neuronales recurrentes \n",
        "\n",
        "En los entornos y librerías  modernos de aprendizaje profundo, sólo tienes que implementar el pase hacia adelante, y el entorno se encarga del pase hacia atrás, por lo que la mayoría de los ingenieros de aprendizaje profundo no necesitan molestarse con los detalles del pase hacia atrás. Sin embargo, si eres un experto en cálculo y quieres comprender bien los detalles del backprop en las RNN, debes trabajar en esta parte opcional del cuaderno. \n",
        "\n",
        "En una rede neuronal \"normal\" o *feedforward*  se emplea la retropropagación para calcular las derivadas con respecto al coste para actualizar los parámetros. Del mismo modo, en las redes neuronales recurrentes puedes calcular las derivadas con respecto al coste para actualizar los parámetros. Las ecuaciones de backprop son bastante complicadas y no las derivamos en la clase. Sin embargo, las presentaremos brevemente a continuación. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd7Elallcyzu"
      },
      "source": [
        "### 2.1 - Retropropagación en una RNN básica\n",
        "\n",
        "Comenzaremos calculando el pase hacia atrás para una celda RNN básica.\n",
        "![RNN_backward](https://i.imgur.com/8eTNKcM.png)\n",
        "\n",
        "<caption><center> **Figura 5**: El paso hacia atrás de la célula RNN. Al igual que en una red neuronal totalmente conectada, la derivada de la función de coste $J$ se propaga hacia atrás a través de la RNN siguiendo la regla de la cadena de cálculos. La regla de la cadena también se utiliza para calcular $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ para actualizar los parámetros $(W_{ax}, W_{aa}, b_a)$. </center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jfVM_Kfcyzu"
      },
      "source": [
        "### Derivación de las funciones de un paso hacia atrás: \n",
        "\n",
        "Para calcular la `rnn_cell_backward` hay que calcular las siguientes ecuaciones. Es un buen ejercicio derivarlas a mano. \n",
        "\n",
        "La derivada de $\\tanh$ es $1-\\tanh(x)^2$. Puedes encontrar la demostración completa [aquí](https://www.wyzant.com/resources/lessons/math/calculus/derivative_proofs/tanx). Observa que: $ \\sec(x)^2 = 1 - \\tanh(x)^2$\n",
        "\n",
        "De forma análoga para $\\frac{ \\partial a^{\\langle t \\rangle}} {\\partial W_{ax}}, \\frac{\\partial a^{\\langle t \\rangle}} {\\partial W_{aa}}, \\frac{\\partial a^{\\langle t \\rangle}} {\\partial b}$, la derivada de $\\tanh(u)$ es $(1-\\tanh(u)^2)du$. \n",
        "\n",
        "Las dos últimas ecuaciones también siguen la misma regla y se derivan utilizando la derivada de $\\tanh$. Obsérvese que el arreglo se hace de manera que coincidan las mismas dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow3vFxTXcyzu"
      },
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "    \"\"\"\n",
        "    Implementa el backward pass para la célula RNN (un solo paso de tiempo).\n",
        "\n",
        "    Argumentos:\n",
        "    da_next -- Gradiente de pérdida con respecto al siguiente estado oculto\n",
        "    cache -- diccionario python que contiene valores útiles (salida de rnn_cell_forward())\n",
        "\n",
        "    Devuelve:\n",
        "    gradientes -- diccionario de python que contiene:\n",
        "                        dx -- Gradientes de los datos de entrada, de forma (n_x, m)\n",
        "                        da_prev -- Gradientes del estado oculto anterior, de forma (n_a, m)\n",
        "                        dWax -- Gradientes de los pesos de entrada a la ocultación, de forma (n_a, n_x)\n",
        "                        dWaa -- Gradientes de los pesos ocultos, de forma (n_a, n_a)\n",
        "                        dba -- Gradientes del vector de sesgo, de forma (n_a, 1)\n",
        "    \"\"\"\n",
        "    ### INICIA  TU CODIGO AQUÍ ###\n",
        "\n",
        "    # Recupera valores de cache\n",
        "    (a_next, a_prev, xt, parameters) = cache\n",
        "    \n",
        "    # Recupera valores de parameters\n",
        "    Wax = parameters['Wax']\n",
        "    Waa = parameters['Waa']\n",
        "    Wya = parameters['Wya']\n",
        "    ba = parameters['ba']\n",
        "    by = parameters['by']\n",
        "\n",
        "    # Calcula el gradiente de tanh con respecto a a_next (≈1 line)\n",
        "    dtanh = (1 - a_next ** 2) * da_next\n",
        "\n",
        "    # Calcula el gradiente of the loss con respecto a Wax (≈2 lines)\n",
        "    dxt = \n",
        "    dWax = \n",
        "\n",
        "    # Calcula el gradiente con respecto a Waa (≈2 lines)\n",
        "    da_prev = \n",
        "    dWaa = \n",
        "\n",
        "    # Calcula el gradiente con respecto a b (≈1 line)\n",
        "    dba = \n",
        "    \n",
        "    # Guarda los gradientes en un diccionario\n",
        "    gradients = { }\n",
        "    \n",
        "    ### ACABA TU CODIGO AQUÍ ###\n",
        "\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3zwZM37wcyzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3cce9f-33aa-46e2-9e63-05b32f70d517"
      },
      "source": [
        "# Ejecuta esta celda para comprobar. \n",
        "# Los resultados deben coincidir con \"Salida esperada\"\n",
        "\n",
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "a_prev = np.random.randn(5,10)\n",
        "Wax = np.random.randn(5,3)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wya = np.random.randn(2,5)\n",
        "b = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "\n",
        "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
        "\n",
        "da_next = np.random.randn(5,10)\n",
        "gradients = rnn_cell_backward(da_next, cache)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('gradients[\"dxt\"][1][2] =', -0.4605641030588796)\n",
            "('gradients[\"dxt\"].shape =', (3, 10))\n",
            "('gradients[\"da_prev\"][2][3] =', 0.08429686538067724)\n",
            "('gradients[\"da_prev\"].shape =', (5, 10))\n",
            "('gradients[\"dWax\"][3][1] =', 0.39308187392193034)\n",
            "('gradients[\"dWax\"].shape =', (5, 3))\n",
            "('gradients[\"dWaa\"][1][2] =', -0.28483955786960663)\n",
            "('gradients[\"dWaa\"].shape =', (5, 5))\n",
            "('gradients[\"dba\"][4] =', array([0.80517166]))\n",
            "('gradients[\"dba\"].shape =', (5, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRV-fdndcyzv"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dxt\"][1][2]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.460564103059\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dxt\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (3, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da_prev\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           0.0842968653807\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da_prev\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"][3][1]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           0.393081873922\n",
        "        </td>\n",
        "    </tr>\n",
        "            <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 3)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           -0.28483955787\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 5)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.80517166]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69dsyX4tcyzw"
      },
      "source": [
        "### Paso hacia atrás a través de la RNN\n",
        "\n",
        "El cálculo de los gradientes del coste con respecto a $a^{\\langle t \\rangle}$ en cada paso de tiempo $t$ es útil porque es lo que ayuda a que el gradiente se retropropague a la célula RNN anterior. Para ello, hay que iterar por todos los pasos de tiempo empezando por el final, y en cada paso, se incrementa el conjunto $db_a$, $dW_{aa}$, $dW_{ax}$ y se almacena $dx$.\n",
        "\n",
        "**Instrucciones**:\n",
        "\n",
        "Implementar la función `rnn_backward`. Inicializar las variables de retorno con ceros en primer lugar y luego bucle a través de todos los pasos de tiempo, mientras que llamamos a la `rnn_cell_backward` en cada paso de tiempo y actualizamos las otras variables en consecuencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyl37wY5cyzw"
      },
      "source": [
        "def rnn_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Implementa el backward pass para una RNN sobre una secuencia completa de datos de entrada.\n",
        "\n",
        "    Argumentos:\n",
        "    da -- gradientes ascendentes de todos los estados ocultos, de forma (n_a, m, T_x)\n",
        "    caches -- tupla que contiene información del pase hacia adelante (rnn_forward)\n",
        "    \n",
        "    Devuelve\n",
        "    gradientes -- diccionario python que contiene:\n",
        "                        dx -- Gradiente con respecto a los datos de entrada, matriz numpy de forma (n_x, m, T_x)\n",
        "                        da0 -- Gradiente con respecto al estado oculto inicial, matriz numpy de forma (n_a, m)\n",
        "                        dWax -- Gradiente respecto a la matriz de pesos de entrada, matriz numpy de forma (n_a, n_x)\n",
        "                        dWaa -- Gradiente respecto a la matriz de pesos del estado oculto, matriz numpy de shape (n_a, n_a)\n",
        "                        dba -- Gradiente respecto al sesgo, de forma (n_a, 1)\n",
        "    \"\"\"\n",
        "    ### INICIA TU CODIGO AQUÍ ###\n",
        "\n",
        "    # Recupera valores de la primera cache (t=1) de caches (≈2 lines)\n",
        "    (caches, x) = \n",
        "    (a1, a0, x1, parameters) = \n",
        "    \n",
        "    # Recupera dimensions de da's y x1's  (≈2 lines)\n",
        "    n_a, m, T_x = \n",
        "    n_x, m = \n",
        "    \n",
        "    # Inicializa los gradientes con los tamaños correctos (≈6 lines)\n",
        "    dx = \n",
        "    dWax = \n",
        "    dWaa = \n",
        "    dba = \n",
        "    da0 = \n",
        "    da_prevt = \n",
        "        \n",
        "    \n",
        "    # Bucle a través de todos los time steps\n",
        "    for t in reversed(range(T_x)):\n",
        "        # Calcula gradiente en el time step t. \n",
        "        # Escoge bien el \"da_next\" y el \"cache\" a usar en el paso de backward propagation. (≈1 line)\n",
        "        gradients = \n",
        "        # Recupera derivadas de los gradientes (≈ 1 line)\n",
        "        dxt, da_prevt, dWaxt, dWaat, dbat = \n",
        "        # Incrementa las derivadas globales w.r.t parameters sumandoles su derivada en el time-step t (≈4 lines)\n",
        "        dx[:, :, t] = \n",
        "        dWax += \n",
        "        dWaa += \n",
        "        dba += \n",
        "        \n",
        "    # Pon en da0 el gradiente del que ha sido retropropagado a través de todos los time-steps (≈1 line) \n",
        "    da0 = \n",
        "\n",
        "    # Guarda los gradientes en un diccionario\n",
        "    gradients = {}\n",
        "    \n",
        "    ### ACABA TU CODIGO AQUÍ ###\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia4AeI0Qcyzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d156993f-e86b-4bb1-accf-4cfdd6392042"
      },
      "source": [
        "# Ejecuta esta celda para comprobar. \n",
        "# Los resultados deben coincidir con los indicados\n",
        "\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "a0 = np.random.randn(5,10)\n",
        "Wax = np.random.randn(5,3)\n",
        "Waa = np.random.randn(5,5)\n",
        "Wya = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
        "a, y, caches = rnn_forward(x, a0, parameters)\n",
        "da = np.random.randn(5, 10, 4)\n",
        "gradients = rnn_backward(da, caches)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('gradients[\"dx\"][1][2] =', array([-2.07101689, -0.59255627,  0.02466855,  0.01483317]))\n",
            "('gradients[\"dx\"].shape =', (3, 10, 4))\n",
            "('gradients[\"da0\"][2][3] =', -0.31494237512664996)\n",
            "('gradients[\"da0\"].shape =', (5, 10))\n",
            "('gradients[\"dWax\"][3][1] =', 11.264104496527777)\n",
            "('gradients[\"dWax\"].shape =', (5, 3))\n",
            "('gradients[\"dWaa\"][1][2] =', 2.303333126579893)\n",
            "('gradients[\"dWaa\"].shape =', (5, 5))\n",
            "('gradients[\"dba\"][4] =', array([-0.74747722]))\n",
            "('gradients[\"dba\"].shape =', (5, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFDo_r3Hcyzx"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dx\"][1][2]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dx\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (3, 10, 4)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da0\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.314942375127\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da0\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "         <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"][3][1]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           11.2641044965\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 3)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           2.30333312658\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 5)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.74747722]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBOnLDgKcyz1"
      },
      "source": [
        "### Enhorabuena \n",
        "\n",
        "Enhorabuena por haber completado esta tarea. Ahora entiendes bien cómo funcionan las redes neuronales recurrentes. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDYfKV10eXvJ"
      },
      "source": [
        "##Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joD3ePK2eZzI"
      },
      "source": [
        "*   Este ejercicio está tomado del excelente curso de Andrew Ng  \"Deep Learning\", accesible a través de Coursera: https://es.coursera.org/\r\n",
        "* Doc oficial Pytorch https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\r\n",
        "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w381MQ6x4e2S"
      },
      "source": [
        "## Fin del Notebook"
      ]
    }
  ]
}