{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "3-GRU_AEP_PT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5cdc195cdde4260ae15ac317dea1e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec7ab4a82dc9488c83f0a7505b7e3005",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_122a4bafae214df98f5983f9ab0120c7",
              "IPY_MODEL_c90286d1b6734699854c91d8e98399bc"
            ]
          }
        },
        "ec7ab4a82dc9488c83f0a7505b7e3005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "122a4bafae214df98f5983f9ab0120c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa118b79bb654fad963d02eed0bf03be",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_559b5a7b86f54aaaac19e2b9c539dd1e"
          }
        },
        "c90286d1b6734699854c91d8e98399bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a840b98ab064e81a64a5428ed76e5ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [11:18&lt;00:00, 52.17s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cd140fa2cd54d51a9afb95b51da4bd7"
          }
        },
        "aa118b79bb654fad963d02eed0bf03be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "559b5a7b86f54aaaac19e2b9c539dd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a840b98ab064e81a64a5428ed76e5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cd140fa2cd54d51a9afb95b51da4bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/Sesion%205/B3_GRU_AEP_PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPSdI5f9E5iY"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \r\n",
        "\r\n",
        "#**Máster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7he17T2FK0C"
      },
      "source": [
        "#<strong><center>Predicción de consumo energético: GRU vs LSTM</center></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF75btraYOaR"
      },
      "source": [
        "En este cuaderno, utilizaremos un modelo GRU para una tarea de predicción de series temporales y compararemos el rendimiento del modelo GRU contra un modelo LSTM. Los modelos LSTM los vais a estudiar en detalle en próximas sesiones, pero aquí va un pequeño adelanto comparativo. \n",
        "\n",
        "El conjunto de datos que utilizaremos es el conjunto de datos de consumo de energía por hora que se puede encontrar en [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). El conjunto de datos contiene datos de consumo de energía en diferentes regiones de los Estados Unidos registrados cada hora.\n",
        "\n",
        "El objetivo de esta implementación es crear un modelo que pueda predecir con exactitud el uso de energía en la siguiente hora dados los datos históricos de uso. Utilizaremos tanto el modelo GRU como el LSTM para entrenar en un conjunto de datos históricos y evaluaremos ambos modelos en un conjunto de pruebas no visto. Para ello, comenzaremos con la selección de características, el preprocesamiento de datos, seguido de la definición, el entrenamiento y, finalmente, la evaluación de los modelos.\n",
        "\n",
        "Utilizaremos la librería PyTorch para implementar ambos tipos de modelos junto con otras librerías de Python habituales en el análisis de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH5PDlOmYOab"
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DvNeXIgdYOae",
        "outputId": "9c16de20-8a0b-4c7b-fa71-d7c95b74d26b"
      },
      "source": [
        "data_dir = '/content/AEP_data/'\r\n",
        "pd.read_csv(data_dir + 'AEP_hourly.csv').head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>AEP_MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-12-31 01:00:00</td>\n",
              "      <td>13478.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-12-31 02:00:00</td>\n",
              "      <td>12865.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-12-31 03:00:00</td>\n",
              "      <td>12577.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-12-31 04:00:00</td>\n",
              "      <td>12517.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-12-31 05:00:00</td>\n",
              "      <td>12670.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Datetime   AEP_MW\n",
              "0  2004-12-31 01:00:00  13478.0\n",
              "1  2004-12-31 02:00:00  12865.0\n",
              "2  2004-12-31 03:00:00  12577.0\n",
              "3  2004-12-31 04:00:00  12517.0\n",
              "4  2004-12-31 05:00:00  12670.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdu_5rs2YOae"
      },
      "source": [
        "Tenemos un total de **12** archivos *.csv* que contienen datos de tendencias energéticas horarias (*'est_hourly.paruqet'* y *'pjm_hourly_est.csv'* no se utilizan). En nuestro siguiente paso, leeremos estos archivos y preprocesaremos estos datos en este orden:\n",
        "- Obteniendo los datos de tiempo de cada paso de tiempo individual y generalizándolos\n",
        "    - Hora del día *es decir, 0-23*.\n",
        "    - Día de la semana *es decir, del 1 al 7*.\n",
        "    - Mes *es 1-12*.\n",
        "    - Día del año *es decir, 1-365*.\n",
        "    \n",
        "    \n",
        "- Escala los datos a valores entre 0 y 1\n",
        "    - Los algoritmos tienden a funcionar mejor o a converger más rápidamente cuando las características están en una escala relativamente similar y/o se acercan a una distribución normal\n",
        "    - La escala preserva la forma de la distribución original y no reduce la importancia de los valores atípicos.\n",
        "    \n",
        "    \n",
        "- Agrupar los datos en secuencias que se utilizarán como entradas del modelo y almacenar sus correspondientes etiquetas\n",
        "    - La **longitud de la secuencia** o **período de espera** es el número de puntos de datos de la historia que el modelo utilizará para hacer la predicción\n",
        "    - La etiqueta será el siguiente punto de datos en el tiempo después del último de la secuencia de entrada\n",
        "    \n",
        "\n",
        "- Las entradas y las etiquetas se dividirán en conjuntos de entrenamiento y de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fXc5GTZYOaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341,
          "referenced_widgets": [
            "c5cdc195cdde4260ae15ac317dea1e83",
            "ec7ab4a82dc9488c83f0a7505b7e3005",
            "122a4bafae214df98f5983f9ab0120c7",
            "c90286d1b6734699854c91d8e98399bc",
            "aa118b79bb654fad963d02eed0bf03be",
            "559b5a7b86f54aaaac19e2b9c539dd1e",
            "6a840b98ab064e81a64a5428ed76e5ca",
            "5cd140fa2cd54d51a9afb95b51da4bd7"
          ]
        },
        "outputId": "c93791a1-66ac-41e1-9294-788c2fd4b19b"
      },
      "source": [
        "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
        "label_scalers = {}\n",
        "\n",
        "train_x = []\n",
        "test_x = {}\n",
        "test_y = {}\n",
        "\n",
        "for file in tqdm_notebook(os.listdir(data_dir)): \n",
        "    # Skipping the files we're not using\n",
        "    if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n",
        "        continue\n",
        "        \n",
        "    print(file)\n",
        "    # Store csv file in a Pandas DataFrame\n",
        "    df = pd.read_csv(data_dir + file, parse_dates=[0])\n",
        "    # Processing the time data into suitable input formats\n",
        "    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)\n",
        "    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n",
        "    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)\n",
        "    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n",
        "    df = df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1)\n",
        "    \n",
        "    # Scaling the input data\n",
        "    sc = MinMaxScaler()\n",
        "    label_sc = MinMaxScaler()\n",
        "    data = sc.fit_transform(df.values)\n",
        "    # Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
        "    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))\n",
        "    label_scalers[file] = label_sc\n",
        "    \n",
        "    # Define lookback period and split inputs/labels\n",
        "    lookback = 90\n",
        "    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
        "    labels = np.zeros(len(data)-lookback)\n",
        "    \n",
        "    for i in range(lookback, len(data)):\n",
        "        inputs[i-lookback] = data[i-lookback:i]\n",
        "        labels[i-lookback] = data[i,0]\n",
        "    inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
        "    labels = labels.reshape(-1,1)\n",
        "    \n",
        "    # Split data into train/test portions and combining all data from different files into a single array\n",
        "    test_portion = int(0.1*len(inputs))\n",
        "    if len(train_x) == 0:\n",
        "        train_x = inputs[:-test_portion]\n",
        "        train_y = labels[:-test_portion]\n",
        "    else:\n",
        "        train_x = np.concatenate((train_x,inputs[:-test_portion]))\n",
        "        train_y = np.concatenate((train_y,labels[:-test_portion]))\n",
        "    test_x[file] = (inputs[-test_portion:])\n",
        "    test_y[file] = (labels[-test_portion:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5cdc195cdde4260ae15ac317dea1e83",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "PJME_hourly.csv\n",
            "DAYTON_hourly.csv\n",
            "DOM_hourly.csv\n",
            "DUQ_hourly.csv\n",
            "PJMW_hourly.csv\n",
            "EKPC_hourly.csv\n",
            "COMED_hourly.csv\n",
            "AEP_hourly.csv\n",
            "FE_hourly.csv\n",
            "DEOK_hourly.csv\n",
            "NI_hourly.csv\n",
            "PJM_Load_hourly.csv\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf1gPAwBYOag",
        "outputId": "8b18cbf7-49d8-4c30-ca57-8b3077278908"
      },
      "source": [
        "print(train_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(980185, 90, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kIZTcZwjFn6",
        "outputId": "b0cfd0f2-e6af-49ae-95d8-84c803490df9"
      },
      "source": [
        "train_x[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.31014432, 0.08695652, 0.16666667, 0.        , 0.        ],\n",
              "        [0.29101443, 0.13043478, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28136522, 0.17391304, 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.4217634 , 0.73913043, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.48806489, 0.7826087 , 0.66666667, 0.        , 0.00821918],\n",
              "        [0.49971558, 0.82608696, 0.66666667, 0.        , 0.00821918]],\n",
              "\n",
              "       [[0.29101443, 0.13043478, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28136522, 0.17391304, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28469399, 0.2173913 , 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.48806489, 0.7826087 , 0.66666667, 0.        , 0.00821918],\n",
              "        [0.49971558, 0.82608696, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.48446224, 0.86956522, 0.66666667, 0.        , 0.00821918]],\n",
              "\n",
              "       [[0.28136522, 0.17391304, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28469399, 0.2173913 , 0.16666667, 0.        , 0.        ],\n",
              "        [0.29727167, 0.26086957, 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.49971558, 0.82608696, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.48446224, 0.86956522, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.46809228, 0.91304348, 0.66666667, 0.        , 0.00821918]],\n",
              "\n",
              "       [[0.28469399, 0.2173913 , 0.16666667, 0.        , 0.        ],\n",
              "        [0.29727167, 0.26086957, 0.16666667, 0.        , 0.        ],\n",
              "        [0.31105025, 0.30434783, 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.48446224, 0.86956522, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.46809228, 0.91304348, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.43872327, 0.95652174, 0.66666667, 0.        , 0.00821918]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_TJazVJYOah"
      },
      "source": [
        "Tenemos un total de 980.185 secuencias de datos de entrenamiento\n",
        "\n",
        "Para mejorar la velocidad de nuestro entrenamiento, podemos procesar los datos en lotes para que el modelo no necesite actualizar sus pesos con tanta frecuencia. Las clases *Dataset* y *DataLoader* de Torch son útiles para dividir nuestros datos en lotes y mezclarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4K0eR3NYOah"
      },
      "source": [
        "batch_size = 1024\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEmJEO0YOai"
      },
      "source": [
        "También podemos comprobar si tenemos alguna GPU para acelerar nuestro tiempo de entrenamiento. Si utilizas GPU para ejecutar este código, el tiempo de entrenamiento se reducirá considerablemente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbMyeCrNYOak"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKv0fIxpd9S2",
        "outputId": "1f8fd06b-20e3-4dc7-c37c-2761dafa2aeb"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTXKWbOSYOam"
      },
      "source": [
        "A continuación, definiremos la estructura de los modelos GRU y LSTM. Ambos modelos tienen la misma estructura, con la única diferencia de la **capa recurrente** (GRU/LSTM) y la inicialización del estado oculto. El estado oculto para el LSTM es una tupla que contiene tanto el **estado de las celdas** como el **estado oculto**, mientras que el GRU sólo tiene un único estado oculto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meFHlBdyYOam"
      },
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_wVKeHcYOao"
      },
      "source": [
        "El proceso de entrenamiento lo vamos a definir en una función a continuación para que podamos reproducirlo para ambos modelos, especificandolo como parámetro. \n",
        "\n",
        "Ambos modelos tendrán el mismo número de **dimensiones** en el *estado oculto* y en las *capas*, se entrenarán con el mismo número de **epochs** y la misma **tasa de aprendizaje**, y se entrenarán y probarán con el mismo conjunto de datos.\n",
        "\n",
        "Con el fin de comparar el rendimiento de ambos modelos, también haremos un seguimiento del tiempo que tarda el modelo en entrenarse y, finalmente, compararemos la precisión final de ambos modelos en el conjunto de pruebas. Para medir la precisión, utilizaremos el *Porcentaje Medio Absoluto de Error Simétrico (sMAPE)* para evaluar los modelos. El *sMAPE* es la suma de la **diferencia absoluta** entre los valores predichos y los reales dividida por la media de los valores predichos y los reales, lo que da un porcentaje que mide la cantidad de error. \n",
        "\n",
        "Esta es la fórmula de *sMAPE*:\n",
        "\n",
        "$sMAPE = \\frac{100%}{n}\\sum_{t=1}^n \\frac{|F_t - A_t|}{(|F_t + A_t|)/2}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOrfDsV1YOap"
      },
      "source": [
        "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
        "    \n",
        "    # Setting common hyperparameters\n",
        "    input_dim = next(iter(train_loader))[0].shape[2]\n",
        "    output_dim = 1\n",
        "    n_layers = 2\n",
        "    # Instantiating the models\n",
        "    if model_type == \"GRU\":\n",
        "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    else:\n",
        "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    \n",
        "    model.train()\n",
        "    print(\"Starting Training of {} model\".format(model_type))\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.clock()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            if model_type == \"GRU\":\n",
        "                h = h.data\n",
        "            else:\n",
        "                h = tuple([e.data for e in h])\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out, h = model(x.to(device).float(), h)\n",
        "            loss = criterion(out, label.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            if counter%200 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.clock()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model\n",
        "\n",
        "def evaluate(model, test_x, test_y, label_scalers):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    start_time = time.clock()\n",
        "    for i in test_x.keys():\n",
        "        inp = torch.from_numpy(np.array(test_x[i]))\n",
        "        labs = torch.from_numpy(np.array(test_y[i]))\n",
        "        h = model.init_hidden(inp.shape[0])\n",
        "        out, h = model(inp.to(device).float(), h)\n",
        "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
        "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
        "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
        "    sMAPE = 0\n",
        "    for i in range(len(outputs)):\n",
        "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
        "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
        "    return outputs, targets, sMAPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHDNYd7BYOaq",
        "outputId": "4e2ad27c-e9c1-47c9-8122-db6c1ceadf0e"
      },
      "source": [
        "lr = 0.001\n",
        "gru_model = train(train_loader, lr, model_type=\"GRU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training of GRU model\n",
            "Epoch 1......Step: 200/957....... Average Loss for Epoch: 0.005936735754075926\n",
            "Epoch 1......Step: 400/957....... Average Loss for Epoch: 0.003330760081371409\n",
            "Epoch 1......Step: 600/957....... Average Loss for Epoch: 0.0023780939747909237\n",
            "Epoch 1......Step: 800/957....... Average Loss for Epoch: 0.0018692187203487266\n",
            "Epoch 1/5 Done, Total Loss: 0.0016085070442723873\n",
            "Time Elapsed for Epoch: 125.908102 seconds\n",
            "Epoch 2......Step: 200/957....... Average Loss for Epoch: 0.0002401996850676369\n",
            "Epoch 2......Step: 400/957....... Average Loss for Epoch: 0.00023332162822043757\n",
            "Epoch 2......Step: 600/957....... Average Loss for Epoch: 0.00022223237501748372\n",
            "Epoch 2......Step: 800/957....... Average Loss for Epoch: 0.0002121916153919301\n",
            "Epoch 2/5 Done, Total Loss: 0.00020969844344894371\n",
            "Time Elapsed for Epoch: 134.599167 seconds\n",
            "Epoch 3......Step: 200/957....... Average Loss for Epoch: 0.0001650260577298468\n",
            "Epoch 3......Step: 400/957....... Average Loss for Epoch: 0.00016182479052076815\n",
            "Epoch 3......Step: 600/957....... Average Loss for Epoch: 0.0001610328426977503\n",
            "Epoch 3......Step: 800/957....... Average Loss for Epoch: 0.00015765687919156335\n",
            "Epoch 3/5 Done, Total Loss: 0.00015477790360421948\n",
            "Time Elapsed for Epoch: 134.17163600000003 seconds\n",
            "Epoch 4......Step: 200/957....... Average Loss for Epoch: 0.0001315745440297178\n",
            "Epoch 4......Step: 400/957....... Average Loss for Epoch: 0.00013123489952704404\n",
            "Epoch 4......Step: 600/957....... Average Loss for Epoch: 0.00013162058825400892\n",
            "Epoch 4......Step: 800/957....... Average Loss for Epoch: 0.00012842919709328272\n",
            "Epoch 4/5 Done, Total Loss: 0.00012767016858165615\n",
            "Time Elapsed for Epoch: 134.77856199999997 seconds\n",
            "Epoch 5......Step: 200/957....... Average Loss for Epoch: 0.00011881694084877382\n",
            "Epoch 5......Step: 400/957....... Average Loss for Epoch: 0.00011674721381496055\n",
            "Epoch 5......Step: 600/957....... Average Loss for Epoch: 0.00011567124754947145\n",
            "Epoch 5......Step: 800/957....... Average Loss for Epoch: 0.00011486476458230754\n",
            "Epoch 5/5 Done, Total Loss: 0.00011478620628466735\n",
            "Time Elapsed for Epoch: 134.83363399999996 seconds\n",
            "Total Training Time: 664.2911009999999 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftJnMoD1YOaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e928c6-7f3e-4d9b-edf4-abccab646ae1"
      },
      "source": [
        "lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training of LSTM model\n",
            "Epoch 1......Step: 200/957....... Average Loss for Epoch: 0.009550642013200559\n",
            "Epoch 1......Step: 400/957....... Average Loss for Epoch: 0.005338132382748881\n",
            "Epoch 1......Step: 600/957....... Average Loss for Epoch: 0.003776723096477023\n",
            "Epoch 1......Step: 800/957....... Average Loss for Epoch: 0.002958885270782048\n",
            "Epoch 1/5 Done, Total Loss: 0.002542278045545602\n",
            "Time Elapsed for Epoch: 182.40060799999992 seconds\n",
            "Epoch 2......Step: 200/957....... Average Loss for Epoch: 0.0003535627499513794\n",
            "Epoch 2......Step: 400/957....... Average Loss for Epoch: 0.0003139567621474271\n",
            "Epoch 2......Step: 600/957....... Average Loss for Epoch: 0.00029095206363611697\n",
            "Epoch 2......Step: 800/957....... Average Loss for Epoch: 0.000271598013532639\n",
            "Epoch 2/5 Done, Total Loss: 0.0002602678715574235\n",
            "Time Elapsed for Epoch: 183.7056389999999 seconds\n",
            "Epoch 3......Step: 200/957....... Average Loss for Epoch: 0.0001842213617055677\n",
            "Epoch 3......Step: 400/957....... Average Loss for Epoch: 0.00018130114436644364\n",
            "Epoch 3......Step: 600/957....... Average Loss for Epoch: 0.00017962168390416386\n",
            "Epoch 3......Step: 800/957....... Average Loss for Epoch: 0.00017475959728471935\n",
            "Epoch 3/5 Done, Total Loss: 0.00017164779446611343\n",
            "Time Elapsed for Epoch: 183.96399899999983 seconds\n",
            "Epoch 4......Step: 200/957....... Average Loss for Epoch: 0.0001632523755688453\n",
            "Epoch 4......Step: 400/957....... Average Loss for Epoch: 0.00015031557572001475\n",
            "Epoch 4......Step: 600/957....... Average Loss for Epoch: 0.0001489069922172348\n",
            "Epoch 4......Step: 800/957....... Average Loss for Epoch: 0.00014423536116737524\n",
            "Epoch 4/5 Done, Total Loss: 0.00014179454527137454\n",
            "Time Elapsed for Epoch: 184.18955500000015 seconds\n",
            "Epoch 5......Step: 200/957....... Average Loss for Epoch: 0.00012522745375463274\n",
            "Epoch 5......Step: 400/957....... Average Loss for Epoch: 0.0001252502666284272\n",
            "Epoch 5......Step: 600/957....... Average Loss for Epoch: 0.0001225301593391729\n",
            "Epoch 5......Step: 800/957....... Average Loss for Epoch: 0.00012174524720649061\n",
            "Epoch 5/5 Done, Total Loss: 0.00012098384584441817\n",
            "Time Elapsed for Epoch: 184.49883099999988 seconds\n",
            "Total Training Time: 918.7586319999997 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCIWNzk4YOar"
      },
      "source": [
        "Como podemos ver en el tiempo de entrenamiento de ambos modelos, el modelo GRU es el claro ganador en términos de **velocidad**, como hemos mencionado anteriormente. Esto es algo lógico ya que la arquitectura GTU requiere **menos parametros de entrenamiento**.\n",
        "\n",
        "Pasando a medir la precisión de ambos modelos, ahora utilizaremos nuestra función evaluate() y el conjunto de datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZubwWRBYOar",
        "outputId": "5e2a15f7-a32c-410f-c5b1-3ea018d3bdfb"
      },
      "source": [
        "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation Time: 4.140718000000106\n",
            "sMAPE: 0.33043812440727127%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv_HZSYQhdCC",
        "outputId": "e184edd9-c258-449a-b6f8-f13d229dace3"
      },
      "source": [
        "gru_outputs[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([2090.7698, 2077.991 , 2105.4268, ..., 2410.7932, 2263.2756,\n",
              "        2073.1133], dtype=float32),\n",
              " array([ 9827.664,  9934.596, 10016.663, ..., 13441.883, 12503.295,\n",
              "        11408.929], dtype=float32),\n",
              " array([1262.1996, 1308.6973, 1348.8604, ..., 1896.7057, 1800.5834,\n",
              "        1663.644 ], dtype=float32),\n",
              " array([6637.2   , 7046.5054, 7352.679 , ..., 6330.481 , 5971.9004,\n",
              "        5453.276 ], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKVsY9V6hlS6",
        "outputId": "31957912-463d-401a-9ee2-24cc4810f3b7"
      },
      "source": [
        "targets[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([2063., 2061., 2119., ..., 2405., 2250., 2042.]),\n",
              " array([ 9618.,  9803.,  9870., ..., 13312., 12390., 11385.]),\n",
              " array([1247., 1298., 1352., ..., 1901., 1789., 1656.]),\n",
              " array([6700., 7248., 7319., ..., 6325., 5892., 5489.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZLCyes-ZiM9o",
        "outputId": "c636216f-0380-4089-c8f2-1b9a792b5adb"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PJM_Load_MW</th>\n",
              "      <th>hour</th>\n",
              "      <th>dayofweek</th>\n",
              "      <th>month</th>\n",
              "      <th>dayofyear</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24157</th>\n",
              "      <td>36392.0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24158</th>\n",
              "      <td>35082.0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24159</th>\n",
              "      <td>33890.0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24160</th>\n",
              "      <td>32590.0</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24161</th>\n",
              "      <td>31569.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PJM_Load_MW  hour  dayofweek  month  dayofyear\n",
              "24157      36392.0    20          0     12        365\n",
              "24158      35082.0    21          0     12        365\n",
              "24159      33890.0    22          0     12        365\n",
              "24160      32590.0    23          0     12        365\n",
              "24161      31569.0     0          1      1          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ho6vhaMYOas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d9c375-395b-4d90-a5c4-34d493308e15"
      },
      "source": [
        "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation Time: 5.306215000000066\n",
            "sMAPE: 0.26701289796413774%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoZvgSFTYOas"
      },
      "source": [
        "Aunque el modelo LSTM puede haber cometido menos errores y haber superado ligeramente al modelo GRU en términos de precisión del rendimiento, la diferencia es insignificante y, por tanto, no concluyente. \r\n",
        "\r\n",
        "Se han realizado numerosas  pruebas en las que se comparan estos dos modelos, pero en general no ha habido un ganador claro en cuanto a cuál es la mejor arquitectura en general. La realidad indica que la mejor solución a cada problema suele ser muy específica y dependiente de la naturaleza del problea, tipo de datos, etc. sin que haya una regla aplicable para todos los casos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5alV4HaLW4Q"
      },
      "source": [
        "## Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AYeyZjKLaJC"
      },
      "source": [
        "* *texto en cursiva* Documento inspirado en los ejemplos disponibles en FloidHub: https://github.com/floydhub/examples\r\n",
        "* Doc oficial Pytorch https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\r\n",
        "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA76f4TuLaqd"
      },
      "source": [
        "## Fin del cuaderno"
      ]
    }
  ]
}