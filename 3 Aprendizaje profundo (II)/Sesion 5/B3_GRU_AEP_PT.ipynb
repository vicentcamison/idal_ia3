{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "3-GRU_AEP_PT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5cdc195cdde4260ae15ac317dea1e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec7ab4a82dc9488c83f0a7505b7e3005",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_122a4bafae214df98f5983f9ab0120c7",
              "IPY_MODEL_c90286d1b6734699854c91d8e98399bc"
            ]
          }
        },
        "ec7ab4a82dc9488c83f0a7505b7e3005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "122a4bafae214df98f5983f9ab0120c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa118b79bb654fad963d02eed0bf03be",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_559b5a7b86f54aaaac19e2b9c539dd1e"
          }
        },
        "c90286d1b6734699854c91d8e98399bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a840b98ab064e81a64a5428ed76e5ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [11:18&lt;00:00, 52.17s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5cd140fa2cd54d51a9afb95b51da4bd7"
          }
        },
        "aa118b79bb654fad963d02eed0bf03be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "559b5a7b86f54aaaac19e2b9c539dd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a840b98ab064e81a64a5428ed76e5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5cd140fa2cd54d51a9afb95b51da4bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/Sesion%205/B3_GRU_AEP_PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPSdI5f9E5iY"
      },
      "source": [
        "![IDAL](https://i.imgur.com/tIKXIG1.jpg)  \r\n",
        "\r\n",
        "#**MÃ¡ster en Inteligencia Artificial Avanzada y Aplicada:  IA^3**\r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7he17T2FK0C"
      },
      "source": [
        "#<strong><center>PredicciÃ³n de consumo energÃ©tico: GRU vs LSTM</center></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF75btraYOaR"
      },
      "source": [
        "En este cuaderno, utilizaremos un modelo GRU para una tarea de predicciÃ³n de series temporales y compararemos el rendimiento del modelo GRU contra un modelo LSTM. Los modelos LSTM los vais a estudiar en detalle en prÃ³ximas sesiones, pero aquÃ­ va un pequeÃ±o adelanto comparativo. \n",
        "\n",
        "El conjunto de datos que utilizaremos es el conjunto de datos de consumo de energÃ­a por hora que se puede encontrar en [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption). El conjunto de datos contiene datos de consumo de energÃ­a en diferentes regiones de los Estados Unidos registrados cada hora.\n",
        "\n",
        "El objetivo de esta implementaciÃ³n es crear un modelo que pueda predecir con exactitud el uso de energÃ­a en la siguiente hora dados los datos histÃ³ricos de uso. Utilizaremos tanto el modelo GRU como el LSTM para entrenar en un conjunto de datos histÃ³ricos y evaluaremos ambos modelos en un conjunto de pruebas no visto. Para ello, comenzaremos con la selecciÃ³n de caracterÃ­sticas, el preprocesamiento de datos, seguido de la definiciÃ³n, el entrenamiento y, finalmente, la evaluaciÃ³n de los modelos.\n",
        "\n",
        "Utilizaremos la librerÃ­a PyTorch para implementar ambos tipos de modelos junto con otras librerÃ­as de Python habituales en el anÃ¡lisis de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH5PDlOmYOab"
      },
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DvNeXIgdYOae",
        "outputId": "9c16de20-8a0b-4c7b-fa71-d7c95b74d26b"
      },
      "source": [
        "data_dir = '/content/AEP_data/'\r\n",
        "pd.read_csv(data_dir + 'AEP_hourly.csv').head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>AEP_MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-12-31 01:00:00</td>\n",
              "      <td>13478.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-12-31 02:00:00</td>\n",
              "      <td>12865.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-12-31 03:00:00</td>\n",
              "      <td>12577.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-12-31 04:00:00</td>\n",
              "      <td>12517.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-12-31 05:00:00</td>\n",
              "      <td>12670.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Datetime   AEP_MW\n",
              "0  2004-12-31 01:00:00  13478.0\n",
              "1  2004-12-31 02:00:00  12865.0\n",
              "2  2004-12-31 03:00:00  12577.0\n",
              "3  2004-12-31 04:00:00  12517.0\n",
              "4  2004-12-31 05:00:00  12670.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdu_5rs2YOae"
      },
      "source": [
        "Tenemos un total de **12** archivos *.csv* que contienen datos de tendencias energÃ©ticas horarias (*'est_hourly.paruqet'* y *'pjm_hourly_est.csv'* no se utilizan). En nuestro siguiente paso, leeremos estos archivos y preprocesaremos estos datos en este orden:\n",
        "- Obteniendo los datos de tiempo de cada paso de tiempo individual y generalizÃ¡ndolos\n",
        "    - Hora del dÃ­a *es decir, 0-23*.\n",
        "    - DÃ­a de la semana *es decir, del 1 al 7*.\n",
        "    - Mes *es 1-12*.\n",
        "    - DÃ­a del aÃ±o *es decir, 1-365*.\n",
        "    \n",
        "    \n",
        "- Escala los datos a valores entre 0 y 1\n",
        "    - Los algoritmos tienden a funcionar mejor o a converger mÃ¡s rÃ¡pidamente cuando las caracterÃ­sticas estÃ¡n en una escala relativamente similar y/o se acercan a una distribuciÃ³n normal\n",
        "    - La escala preserva la forma de la distribuciÃ³n original y no reduce la importancia de los valores atÃ­picos.\n",
        "    \n",
        "    \n",
        "- Agrupar los datos en secuencias que se utilizarÃ¡n como entradas del modelo y almacenar sus correspondientes etiquetas\n",
        "    - La **longitud de la secuencia** o **perÃ­odo de espera** es el nÃºmero de puntos de datos de la historia que el modelo utilizarÃ¡ para hacer la predicciÃ³n\n",
        "    - La etiqueta serÃ¡ el siguiente punto de datos en el tiempo despuÃ©s del Ãºltimo de la secuencia de entrada\n",
        "    \n",
        "\n",
        "- Las entradas y las etiquetas se dividirÃ¡n en conjuntos de entrenamiento y de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fXc5GTZYOaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341,
          "referenced_widgets": [
            "c5cdc195cdde4260ae15ac317dea1e83",
            "ec7ab4a82dc9488c83f0a7505b7e3005",
            "122a4bafae214df98f5983f9ab0120c7",
            "c90286d1b6734699854c91d8e98399bc",
            "aa118b79bb654fad963d02eed0bf03be",
            "559b5a7b86f54aaaac19e2b9c539dd1e",
            "6a840b98ab064e81a64a5428ed76e5ca",
            "5cd140fa2cd54d51a9afb95b51da4bd7"
          ]
        },
        "outputId": "c93791a1-66ac-41e1-9294-788c2fd4b19b"
      },
      "source": [
        "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
        "label_scalers = {}\n",
        "\n",
        "train_x = []\n",
        "test_x = {}\n",
        "test_y = {}\n",
        "\n",
        "for file in tqdm_notebook(os.listdir(data_dir)): \n",
        "    # Skipping the files we're not using\n",
        "    if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n",
        "        continue\n",
        "        \n",
        "    print(file)\n",
        "    # Store csv file in a Pandas DataFrame\n",
        "    df = pd.read_csv(data_dir + file, parse_dates=[0])\n",
        "    # Processing the time data into suitable input formats\n",
        "    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)\n",
        "    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n",
        "    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)\n",
        "    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n",
        "    df = df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1)\n",
        "    \n",
        "    # Scaling the input data\n",
        "    sc = MinMaxScaler()\n",
        "    label_sc = MinMaxScaler()\n",
        "    data = sc.fit_transform(df.values)\n",
        "    # Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
        "    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))\n",
        "    label_scalers[file] = label_sc\n",
        "    \n",
        "    # Define lookback period and split inputs/labels\n",
        "    lookback = 90\n",
        "    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
        "    labels = np.zeros(len(data)-lookback)\n",
        "    \n",
        "    for i in range(lookback, len(data)):\n",
        "        inputs[i-lookback] = data[i-lookback:i]\n",
        "        labels[i-lookback] = data[i,0]\n",
        "    inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
        "    labels = labels.reshape(-1,1)\n",
        "    \n",
        "    # Split data into train/test portions and combining all data from different files into a single array\n",
        "    test_portion = int(0.1*len(inputs))\n",
        "    if len(train_x) == 0:\n",
        "        train_x = inputs[:-test_portion]\n",
        "        train_y = labels[:-test_portion]\n",
        "    else:\n",
        "        train_x = np.concatenate((train_x,inputs[:-test_portion]))\n",
        "        train_y = np.concatenate((train_y,labels[:-test_portion]))\n",
        "    test_x[file] = (inputs[-test_portion:])\n",
        "    test_y[file] = (labels[-test_portion:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5cdc195cdde4260ae15ac317dea1e83",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "PJME_hourly.csv\n",
            "DAYTON_hourly.csv\n",
            "DOM_hourly.csv\n",
            "DUQ_hourly.csv\n",
            "PJMW_hourly.csv\n",
            "EKPC_hourly.csv\n",
            "COMED_hourly.csv\n",
            "AEP_hourly.csv\n",
            "FE_hourly.csv\n",
            "DEOK_hourly.csv\n",
            "NI_hourly.csv\n",
            "PJM_Load_hourly.csv\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf1gPAwBYOag",
        "outputId": "8b18cbf7-49d8-4c30-ca57-8b3077278908"
      },
      "source": [
        "print(train_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(980185, 90, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kIZTcZwjFn6",
        "outputId": "b0cfd0f2-e6af-49ae-95d8-84c803490df9"
      },
      "source": [
        "train_x[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.31014432, 0.08695652, 0.16666667, 0.        , 0.        ],\n",
              "        [0.29101443, 0.13043478, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28136522, 0.17391304, 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.4217634 , 0.73913043, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.48806489, 0.7826087 , 0.66666667, 0.        , 0.00821918],\n",
              "        [0.49971558, 0.82608696, 0.66666667, 0.        , 0.00821918]],\n",
              "\n",
              "       [[0.29101443, 0.13043478, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28136522, 0.17391304, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28469399, 0.2173913 , 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.48806489, 0.7826087 , 0.66666667, 0.        , 0.00821918],\n",
              "        [0.49971558, 0.82608696, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.48446224, 0.86956522, 0.66666667, 0.        , 0.00821918]],\n",
              "\n",
              "       [[0.28136522, 0.17391304, 0.16666667, 0.        , 0.        ],\n",
              "        [0.28469399, 0.2173913 , 0.16666667, 0.        , 0.        ],\n",
              "        [0.29727167, 0.26086957, 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.49971558, 0.82608696, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.48446224, 0.86956522, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.46809228, 0.91304348, 0.66666667, 0.        , 0.00821918]],\n",
              "\n",
              "       [[0.28469399, 0.2173913 , 0.16666667, 0.        , 0.        ],\n",
              "        [0.29727167, 0.26086957, 0.16666667, 0.        , 0.        ],\n",
              "        [0.31105025, 0.30434783, 0.16666667, 0.        , 0.        ],\n",
              "        ...,\n",
              "        [0.48446224, 0.86956522, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.46809228, 0.91304348, 0.66666667, 0.        , 0.00821918],\n",
              "        [0.43872327, 0.95652174, 0.66666667, 0.        , 0.00821918]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_TJazVJYOah"
      },
      "source": [
        "Tenemos un total de 980.185 secuencias de datos de entrenamiento\n",
        "\n",
        "Para mejorar la velocidad de nuestro entrenamiento, podemos procesar los datos en lotes para que el modelo no necesite actualizar sus pesos con tanta frecuencia. Las clases *Dataset* y *DataLoader* de Torch son Ãºtiles para dividir nuestros datos en lotes y mezclarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4K0eR3NYOah"
      },
      "source": [
        "batch_size = 1024\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLEmJEO0YOai"
      },
      "source": [
        "TambiÃ©n podemos comprobar si tenemos alguna GPU para acelerar nuestro tiempo de entrenamiento. Si utilizas GPU para ejecutar este cÃ³digo, el tiempo de entrenamiento se reducirÃ¡ considerablemente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbMyeCrNYOak"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKv0fIxpd9S2",
        "outputId": "1f8fd06b-20e3-4dc7-c37c-2761dafa2aeb"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTXKWbOSYOam"
      },
      "source": [
        "A continuaciÃ³n, definiremos la estructura de los modelos GRU y LSTM. Ambos modelos tienen la misma estructura, con la Ãºnica diferencia de la **capa recurrente** (GRU/LSTM) y la inicializaciÃ³n del estado oculto. El estado oculto para el LSTM es una tupla que contiene tanto el **estado de las celdas** como el **estado oculto**, mientras que el GRU sÃ³lo tiene un Ãºnico estado oculto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meFHlBdyYOam"
      },
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_wVKeHcYOao"
      },
      "source": [
        "El proceso de entrenamiento lo vamos a definir en una funciÃ³n a continuaciÃ³n para que podamos reproducirlo para ambos modelos, especificandolo como parÃ¡metro. \n",
        "\n",
        "Ambos modelos tendrÃ¡n el mismo nÃºmero de **dimensiones** en el *estado oculto* y en las *capas*, se entrenarÃ¡n con el mismo nÃºmero de **epochs** y la misma **tasa de aprendizaje**, y se entrenarÃ¡n y probarÃ¡n con el mismo conjunto de datos.\n",
        "\n",
        "Con el fin de comparar el rendimiento de ambos modelos, tambiÃ©n haremos un seguimiento del tiempo que tarda el modelo en entrenarse y, finalmente, compararemos la precisiÃ³n final de ambos modelos en el conjunto de pruebas. Para medir la precisiÃ³n, utilizaremos el *Porcentaje Medio Absoluto de Error SimÃ©trico (sMAPE)* para evaluar los modelos. El *sMAPE* es la suma de la **diferencia absoluta** entre los valores predichos y los reales dividida por la media de los valores predichos y los reales, lo que da un porcentaje que mide la cantidad de error. \n",
        "\n",
        "Esta es la fÃ³rmula de *sMAPE*:\n",
        "\n",
        "$sMAPE = \\frac{100%}{n}\\sum_{t=1}^n \\frac{|F_t - A_t|}{(|F_t + A_t|)/2}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOrfDsV1YOap"
      },
      "source": [
        "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5, model_type=\"GRU\"):\n",
        "    \n",
        "    # Setting common hyperparameters\n",
        "    input_dim = next(iter(train_loader))[0].shape[2]\n",
        "    output_dim = 1\n",
        "    n_layers = 2\n",
        "    # Instantiating the models\n",
        "    if model_type == \"GRU\":\n",
        "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    else:\n",
        "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Defining loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "    \n",
        "    model.train()\n",
        "    print(\"Starting Training of {} model\".format(model_type))\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.clock()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            if model_type == \"GRU\":\n",
        "                h = h.data\n",
        "            else:\n",
        "                h = tuple([e.data for e in h])\n",
        "            model.zero_grad()\n",
        "            \n",
        "            out, h = model(x.to(device).float(), h)\n",
        "            loss = criterion(out, label.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            if counter%200 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.clock()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        print(\"Time Elapsed for Epoch: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model\n",
        "\n",
        "def evaluate(model, test_x, test_y, label_scalers):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    start_time = time.clock()\n",
        "    for i in test_x.keys():\n",
        "        inp = torch.from_numpy(np.array(test_x[i]))\n",
        "        labs = torch.from_numpy(np.array(test_y[i]))\n",
        "        h = model.init_hidden(inp.shape[0])\n",
        "        out, h = model(inp.to(device).float(), h)\n",
        "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
        "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
        "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
        "    sMAPE = 0\n",
        "    for i in range(len(outputs)):\n",
        "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
        "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
        "    return outputs, targets, sMAPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHDNYd7BYOaq",
        "outputId": "4e2ad27c-e9c1-47c9-8122-db6c1ceadf0e"
      },
      "source": [
        "lr = 0.001\n",
        "gru_model = train(train_loader, lr, model_type=\"GRU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training of GRU model\n",
            "Epoch 1......Step: 200/957....... Average Loss for Epoch: 0.005936735754075926\n",
            "Epoch 1......Step: 400/957....... Average Loss for Epoch: 0.003330760081371409\n",
            "Epoch 1......Step: 600/957....... Average Loss for Epoch: 0.0023780939747909237\n",
            "Epoch 1......Step: 800/957....... Average Loss for Epoch: 0.0018692187203487266\n",
            "Epoch 1/5 Done, Total Loss: 0.0016085070442723873\n",
            "Time Elapsed for Epoch: 125.908102 seconds\n",
            "Epoch 2......Step: 200/957....... Average Loss for Epoch: 0.0002401996850676369\n",
            "Epoch 2......Step: 400/957....... Average Loss for Epoch: 0.00023332162822043757\n",
            "Epoch 2......Step: 600/957....... Average Loss for Epoch: 0.00022223237501748372\n",
            "Epoch 2......Step: 800/957....... Average Loss for Epoch: 0.0002121916153919301\n",
            "Epoch 2/5 Done, Total Loss: 0.00020969844344894371\n",
            "Time Elapsed for Epoch: 134.599167 seconds\n",
            "Epoch 3......Step: 200/957....... Average Loss for Epoch: 0.0001650260577298468\n",
            "Epoch 3......Step: 400/957....... Average Loss for Epoch: 0.00016182479052076815\n",
            "Epoch 3......Step: 600/957....... Average Loss for Epoch: 0.0001610328426977503\n",
            "Epoch 3......Step: 800/957....... Average Loss for Epoch: 0.00015765687919156335\n",
            "Epoch 3/5 Done, Total Loss: 0.00015477790360421948\n",
            "Time Elapsed for Epoch: 134.17163600000003 seconds\n",
            "Epoch 4......Step: 200/957....... Average Loss for Epoch: 0.0001315745440297178\n",
            "Epoch 4......Step: 400/957....... Average Loss for Epoch: 0.00013123489952704404\n",
            "Epoch 4......Step: 600/957....... Average Loss for Epoch: 0.00013162058825400892\n",
            "Epoch 4......Step: 800/957....... Average Loss for Epoch: 0.00012842919709328272\n",
            "Epoch 4/5 Done, Total Loss: 0.00012767016858165615\n",
            "Time Elapsed for Epoch: 134.77856199999997 seconds\n",
            "Epoch 5......Step: 200/957....... Average Loss for Epoch: 0.00011881694084877382\n",
            "Epoch 5......Step: 400/957....... Average Loss for Epoch: 0.00011674721381496055\n",
            "Epoch 5......Step: 600/957....... Average Loss for Epoch: 0.00011567124754947145\n",
            "Epoch 5......Step: 800/957....... Average Loss for Epoch: 0.00011486476458230754\n",
            "Epoch 5/5 Done, Total Loss: 0.00011478620628466735\n",
            "Time Elapsed for Epoch: 134.83363399999996 seconds\n",
            "Total Training Time: 664.2911009999999 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftJnMoD1YOaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e928c6-7f3e-4d9b-edf4-abccab646ae1"
      },
      "source": [
        "lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training of LSTM model\n",
            "Epoch 1......Step: 200/957....... Average Loss for Epoch: 0.009550642013200559\n",
            "Epoch 1......Step: 400/957....... Average Loss for Epoch: 0.005338132382748881\n",
            "Epoch 1......Step: 600/957....... Average Loss for Epoch: 0.003776723096477023\n",
            "Epoch 1......Step: 800/957....... Average Loss for Epoch: 0.002958885270782048\n",
            "Epoch 1/5 Done, Total Loss: 0.002542278045545602\n",
            "Time Elapsed for Epoch: 182.40060799999992 seconds\n",
            "Epoch 2......Step: 200/957....... Average Loss for Epoch: 0.0003535627499513794\n",
            "Epoch 2......Step: 400/957....... Average Loss for Epoch: 0.0003139567621474271\n",
            "Epoch 2......Step: 600/957....... Average Loss for Epoch: 0.00029095206363611697\n",
            "Epoch 2......Step: 800/957....... Average Loss for Epoch: 0.000271598013532639\n",
            "Epoch 2/5 Done, Total Loss: 0.0002602678715574235\n",
            "Time Elapsed for Epoch: 183.7056389999999 seconds\n",
            "Epoch 3......Step: 200/957....... Average Loss for Epoch: 0.0001842213617055677\n",
            "Epoch 3......Step: 400/957....... Average Loss for Epoch: 0.00018130114436644364\n",
            "Epoch 3......Step: 600/957....... Average Loss for Epoch: 0.00017962168390416386\n",
            "Epoch 3......Step: 800/957....... Average Loss for Epoch: 0.00017475959728471935\n",
            "Epoch 3/5 Done, Total Loss: 0.00017164779446611343\n",
            "Time Elapsed for Epoch: 183.96399899999983 seconds\n",
            "Epoch 4......Step: 200/957....... Average Loss for Epoch: 0.0001632523755688453\n",
            "Epoch 4......Step: 400/957....... Average Loss for Epoch: 0.00015031557572001475\n",
            "Epoch 4......Step: 600/957....... Average Loss for Epoch: 0.0001489069922172348\n",
            "Epoch 4......Step: 800/957....... Average Loss for Epoch: 0.00014423536116737524\n",
            "Epoch 4/5 Done, Total Loss: 0.00014179454527137454\n",
            "Time Elapsed for Epoch: 184.18955500000015 seconds\n",
            "Epoch 5......Step: 200/957....... Average Loss for Epoch: 0.00012522745375463274\n",
            "Epoch 5......Step: 400/957....... Average Loss for Epoch: 0.0001252502666284272\n",
            "Epoch 5......Step: 600/957....... Average Loss for Epoch: 0.0001225301593391729\n",
            "Epoch 5......Step: 800/957....... Average Loss for Epoch: 0.00012174524720649061\n",
            "Epoch 5/5 Done, Total Loss: 0.00012098384584441817\n",
            "Time Elapsed for Epoch: 184.49883099999988 seconds\n",
            "Total Training Time: 918.7586319999997 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCIWNzk4YOar"
      },
      "source": [
        "Como podemos ver en el tiempo de entrenamiento de ambos modelos, el modelo GRU es el claro ganador en tÃ©rminos de **velocidad**, como hemos mencionado anteriormente. Esto es algo lÃ³gico ya que la arquitectura GTU requiere **menos parametros de entrenamiento**.\n",
        "\n",
        "Pasando a medir la precisiÃ³n de ambos modelos, ahora utilizaremos nuestra funciÃ³n evaluate() y el conjunto de datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZubwWRBYOar",
        "outputId": "5e2a15f7-a32c-410f-c5b1-3ea018d3bdfb"
      },
      "source": [
        "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation Time: 4.140718000000106\n",
            "sMAPE: 0.33043812440727127%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv_HZSYQhdCC",
        "outputId": "e184edd9-c258-449a-b6f8-f13d229dace3"
      },
      "source": [
        "gru_outputs[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([2090.7698, 2077.991 , 2105.4268, ..., 2410.7932, 2263.2756,\n",
              "        2073.1133], dtype=float32),\n",
              " array([ 9827.664,  9934.596, 10016.663, ..., 13441.883, 12503.295,\n",
              "        11408.929], dtype=float32),\n",
              " array([1262.1996, 1308.6973, 1348.8604, ..., 1896.7057, 1800.5834,\n",
              "        1663.644 ], dtype=float32),\n",
              " array([6637.2   , 7046.5054, 7352.679 , ..., 6330.481 , 5971.9004,\n",
              "        5453.276 ], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKVsY9V6hlS6",
        "outputId": "31957912-463d-401a-9ee2-24cc4810f3b7"
      },
      "source": [
        "targets[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([2063., 2061., 2119., ..., 2405., 2250., 2042.]),\n",
              " array([ 9618.,  9803.,  9870., ..., 13312., 12390., 11385.]),\n",
              " array([1247., 1298., 1352., ..., 1901., 1789., 1656.]),\n",
              " array([6700., 7248., 7319., ..., 6325., 5892., 5489.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZLCyes-ZiM9o",
        "outputId": "c636216f-0380-4089-c8f2-1b9a792b5adb"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PJM_Load_MW</th>\n",
              "      <th>hour</th>\n",
              "      <th>dayofweek</th>\n",
              "      <th>month</th>\n",
              "      <th>dayofyear</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24157</th>\n",
              "      <td>36392.0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24158</th>\n",
              "      <td>35082.0</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24159</th>\n",
              "      <td>33890.0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24160</th>\n",
              "      <td>32590.0</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24161</th>\n",
              "      <td>31569.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PJM_Load_MW  hour  dayofweek  month  dayofyear\n",
              "24157      36392.0    20          0     12        365\n",
              "24158      35082.0    21          0     12        365\n",
              "24159      33890.0    22          0     12        365\n",
              "24160      32590.0    23          0     12        365\n",
              "24161      31569.0     0          1      1          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ho6vhaMYOas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d9c375-395b-4d90-a5c4-34d493308e15"
      },
      "source": [
        "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation Time: 5.306215000000066\n",
            "sMAPE: 0.26701289796413774%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoZvgSFTYOas"
      },
      "source": [
        "Aunque el modelo LSTM puede haber cometido menos errores y haber superado ligeramente al modelo GRU en tÃ©rminos de precisiÃ³n del rendimiento, la diferencia es insignificante y, por tanto, no concluyente. \r\n",
        "\r\n",
        "Se han realizado numerosas  pruebas en las que se comparan estos dos modelos, pero en general no ha habido un ganador claro en cuanto a cuÃ¡l es la mejor arquitectura en general. La realidad indica que la mejor soluciÃ³n a cada problema suele ser muy especÃ­fica y dependiente de la naturaleza del problea, tipo de datos, etc. sin que haya una regla aplicable para todos los casos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5alV4HaLW4Q"
      },
      "source": [
        "## Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AYeyZjKLaJC"
      },
      "source": [
        "* *texto en cursiva* Documento inspirado en los ejemplos disponibles en FloidHub: https://github.com/floydhub/examples\r\n",
        "* Doc oficial Pytorch https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\r\n",
        "* https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA76f4TuLaqd"
      },
      "source": [
        "## Fin del cuaderno"
      ]
    }
  ]
}