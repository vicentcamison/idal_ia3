{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/InfoGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrPUEoXu4Kwc"
      },
      "source": [
        "#import os\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_zyeJVjPxzH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import glob\n",
        "\n",
        "from itertools import chain"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A7cX_TKXiNg"
      },
      "source": [
        "# CONTROL VARIABLES\n",
        "\n",
        "# GAN style\n",
        "# 0: InfoGAN\n",
        "# 1: AC-GAN (Auxiliary Conditional GAN)\n",
        "GAN_STYLE = 1\n",
        "\n",
        "\n",
        "# lesser importance variables\n",
        "LRELU_ALPHA = 0.05 #negative slope of leaky relu activation\n",
        "LR_G = 0.002 #generator learning rate\n",
        "LR_D = 0.001 #discriminator learning rate\n",
        "EPSILON = 0.05 #noise factor in images to discriminator"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kiXnudPmyF"
      },
      "source": [
        "We are going to create a GAN to generate new fonts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6DzjWi9Py-Y",
        "outputId": "9991ba98-55c6-4f31-ce41-fe46cf965789"
      },
      "source": [
        "# Access to the BOB ROSS images folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XE9urSNQRO2"
      },
      "source": [
        "#directory = \"gdrive/MyDrive/UNIVERSIDAD/Master Propio IA UV/3 Aprendizaje profundo (II)/Datasets/Bob Ross/train\"\n",
        "directory = \"gdrive/MyDrive/UNIVERSIDAD/Master Propio IA UV/3 Aprendizaje profundo (II)/Trabajo final/fonts\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2-COIKsvtCk"
      },
      "source": [
        "# get data file names (this code takes a while to retrieve all data and prepare all dataframes)\n",
        "filenames = glob.glob(directory + \"/*.csv\")\n",
        "dfs = [pd.read_csv(filename) for filename in filenames]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WHqiLelw80u",
        "outputId": "cd2ff88c-3a31-449f-ba4d-9b45496d67ae"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHz6XvH8Vd8c"
      },
      "source": [
        "# Characters have an index associated to them (m_label). The index is related to the character in the following dictionary\n",
        "index_to_char = {65:'A', 66:'B', 67:'C', 68:'D', 69:'E', 70:'F', 71:'G', 72:'H', 73:'I', 74:'J',\n",
        "                 75:'K', 76:'L', 77:'M', 78:'N', 79:'O', 80:'P', 81:'Q', 82:'R', 83:'S', 84:'T',\n",
        "                 85:'U', 86:'V', 87:'W', 88:'X', 89:'Y', 90:'Z', 97:'a', 98:'b', 99:'c', 100:'d',\n",
        "                 101:'e', 102:'f', 103:'g', 104:'h', 105:'i', 106:'j', 107:'k', 108:'l', 109:'m',\n",
        "                 110:'n', 111:'o', 112:'p', 113:'q', 114:'r', 115:'s', 116:'t', 117:'u', 118:'v',\n",
        "                 119:'w', 120:'x', 121:'y', 122:'z', 48:'0', 49:'1', 50:'2', 51:'3', 52:'4', 53:'5',\n",
        "                 54:'6', 55:'7', 56:'8', 57:'9'}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USLE1ztGoADb"
      },
      "source": [
        "number_min = 48\n",
        "number_max = 57\n",
        "upper_min = 65\n",
        "upper_max = 90\n",
        "lower_min = 97\n",
        "lower_max = 122"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAVfnqrzfmxJ",
        "outputId": "d29b64ed-f226-4062-8ede-cc2d499a1d74"
      },
      "source": [
        "# We create the datasets. We are going to divide them in NUMBERS, UPPERCASE LETTERS and LOWERCASE LETTERS\n",
        "X_numbers = np.array([], ndmin=3)\n",
        "X_upper = np.array([], ndmin=3)\n",
        "X_lower = np.array([], ndmin=3)\n",
        "Y_numbers = np.array([], ndmin=2)\n",
        "Y_upper = np.array([], ndmin=2)\n",
        "Y_lower = np.array([], ndmin=2)\n",
        "\n",
        "i = 0\n",
        "for df in dfs:\n",
        "  #df = df[np.logical_and(df.loc[:,'strength'] < 0.5, df.loc[:,'italic'] == 0]\n",
        "  df_straight = df[np.logical_and(df.loc[:,'strength'] < 0.5, df.loc[:,'italic'] == 0)] #remove the columns that are in bold or cursive\n",
        "  #BY SLIGHTLY MODIFYING THIS LINE OF CODE AND ADDING THE CORRESPONDING LABELS TO Y, CURSIVE AND BOLD CAN BE ADDED TO THE MODEL\n",
        "\n",
        "  # We get the elements of the dataframe that reference the images of characters (upper and lower letters, and numbers)\n",
        "  X_numbers_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= number_min, df_straight.loc[:, 'm_label'] <= number_max)].iloc[:, 12:]\n",
        "  X_upper_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= upper_min, df_straight.loc[:, 'm_label'] <= upper_max)].iloc[:, 12:]\n",
        "  X_lower_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= lower_min, df_straight.loc[:, 'm_label'] <= lower_max)].iloc[:, 12:]\n",
        "\n",
        "  Y_numbers_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= number_min, df_straight.loc[:, 'm_label'] <= number_max)].loc[:, ['m_label', 'font']]\n",
        "  Y_upper_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= upper_min, df_straight.loc[:, 'm_label'] <= upper_max)].loc[:, ['m_label', 'font']]\n",
        "  Y_lower_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= lower_min, df_straight.loc[:, 'm_label'] <= lower_max)].loc[:, ['m_label', 'font']]\n",
        "\n",
        "  # We transform each element from a 400x1 array into a 20x20 array\n",
        "  #X_numbers_new = np.reshape(X_numbers_new.values, (20,20,-1))\n",
        "  #X_upper_new = np.reshape(X_upper_new.values, (20,20,-1))\n",
        "  #X_lower_new = np.reshape(X_lower_new.values, (20,20,-1))\n",
        "\n",
        "  # We rearrange the indexes so that the first index refers to image index\n",
        "  #X_numbers_new = np.transpose(X_numbers_new, axes=[2, 0, 1])\n",
        "  #X_upper_new = np.transpose(X_upper_new, axes=[2, 0, 1])\n",
        "  #X_lower_new = np.transpose(X_lower_new, axes=[2, 0, 1])\n",
        "\n",
        "  X_numbers_new = np.array(np.reshape(X_numbers_new.values, (-1,20,20)))\n",
        "  X_upper_new = np.array(np.reshape(X_upper_new.values, (-1,20,20)))\n",
        "  X_lower_new = np.array(np.reshape(X_lower_new.values, (-1,20,20)))\n",
        "\n",
        "  X_numbers_new = X_numbers_new.astype('float64')\n",
        "  X_upper_new = X_upper_new.astype('float64')\n",
        "  X_lower_new = X_lower_new.astype('float64')\n",
        "\n",
        "  Y_numbers_new.iloc[:, 0] = Y_numbers_new.iloc[:, 0].astype('int')\n",
        "  Y_upper_new.iloc[:, 0] = Y_upper_new.iloc[:, 0].astype('int')\n",
        "  Y_lower_new.iloc[:, 0] = Y_lower_new.iloc[:, 0].astype('int')\n",
        "\n",
        "  # We append the _new vectors to the standard vectors\n",
        "  if i == 0: # IS THERE A WAY TO PROGRAM THIS BETTER?\n",
        "    X_numbers = X_numbers_new\n",
        "    X_upper = X_upper_new\n",
        "    X_lower = X_lower_new\n",
        "\n",
        "    Y_numbers = Y_numbers_new\n",
        "    Y_upper = Y_upper_new\n",
        "    Y_lower = Y_lower_new\n",
        "\n",
        "  else:\n",
        "    X_numbers = np.concatenate((X_numbers, X_numbers_new), axis=0)\n",
        "    X_upper = np.concatenate((X_upper, X_upper_new), axis=0)\n",
        "    X_lower = np.concatenate((X_lower, X_lower_new), axis=0)\n",
        "\n",
        "    Y_numbers = pd.concat([Y_numbers, Y_numbers_new], axis=0)\n",
        "    Y_upper = pd.concat([Y_upper, Y_upper_new], axis=0)\n",
        "    Y_lower = pd.concat([Y_lower, Y_lower_new], axis=0)\n",
        "\n",
        "    #Y_numbers = np.concatenate((Y_numbers, np.array(Y_numbers_new)), axis=0)\n",
        "    #Y_upper = np.concatenate((Y_upper, np.array(Y_upper_new)), axis=0)\n",
        "    #Y_lower = np.concatenate((Y_lower, np.array(Y_lower_new)), axis=0)\n",
        "\n",
        "    \n",
        "  #X_numbers.append(X_numbers_new)\n",
        "  #X_upper.append(X_upper_new)\n",
        "  #X_lower.append(X_lower)\n",
        "\n",
        "  #Y_numbers.append(Y_numbers_new)\n",
        "  #Y_upper.append(Y_upper_new)\n",
        "  #Y_lower.append(Y_lower_new)\n",
        "  \n",
        "  i += 1\n",
        "  if i % 10 == 0 or i==len(dfs):\n",
        "    print(str(i)+'/'+str(len(dfs)))\n",
        "\n",
        "  #for row in df[np.logical_and(df.loc[:,'strength'] < 0.5, df.loc[:,'italic'] == 0].iterrows():\n",
        "   # if row.strength < 0.5 and row.italic == 0:\n",
        "\n",
        "#Y_numbers.reset_index(inplace=True)\n",
        "#Y_lower.reset_index(inplace=True)\n",
        "#Y_upper.reset_index(inplace=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/153\n",
            "20/153\n",
            "30/153\n",
            "40/153\n",
            "50/153\n",
            "60/153\n",
            "70/153\n",
            "80/153\n",
            "90/153\n",
            "100/153\n",
            "110/153\n",
            "120/153\n",
            "130/153\n",
            "140/153\n",
            "150/153\n",
            "153/153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyHxV2Dv6brP"
      },
      "source": [
        "#THIS CODE IS NOW RENDERED UNNECESSARY\n",
        "\n",
        "#print('finish0')\n",
        "#X_numbers = np.concatenate(X_numbers, axis=0)\n",
        "##print('finish1')\n",
        "#X_upper = np.concatenate(X_upper, axis=0)\n",
        "#print('finish2')\n",
        "#X_lower = np.concatenate(X_lower, axis=0)\n",
        "#print('finish3')\n",
        "\n",
        "#Y_numbers = np.concatenate(Y_numbers, axis=0)\n",
        "#print('finish4')\n",
        "#Y_upper = np.concatenate(Y_upper, axis=0)\n",
        "#print('finish5')\n",
        "#Y_lower = np.concatenate(Y_lower, axis=0)\n",
        "#print('finish6')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQDDjU6Pc5bn"
      },
      "source": [
        "# We create a custom transform that can escale the images to [-1, 1] the range that's typically used\n",
        "# in GANs. It's the image of the function tanh(), used sometimes as the last activation function\n",
        "# in generators\n",
        "\n",
        "class ToTanh(object):\n",
        "    \"\"\"Convert single-channel images from range [0, 256] to range [-1, 1].\"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        return torch.tensor(pic * 2 - 1, dtype=torch.float)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LxQhTpj0zh9"
      },
      "source": [
        "# Generation of the transform for the DataLoader\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(0.5, 0.5)\n",
        "  #ToTanh()\n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDfRsj_w06rs"
      },
      "source": [
        "#Append the Y columns at the end of the X columns\n",
        "\n",
        "#X_numbers = np.concatenate([X_numbers, Y_numbers], axis=1)\n",
        "#X_upper = np.concatenate([X_upper, Y_upper], axis=1)\n",
        "#X_lower = np.concatenate([])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux0fsk9-7uDw"
      },
      "source": [
        "#We modify the dataset class to fit our needs: deliver an image and a label per example\n",
        "class FontsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_images, Y_labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X_images (np.array): np.array containing all the font images\n",
        "            Y_labels (np.array): np.array containing all the font labels\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.X_images = X_images\n",
        "        #self.X_images.type(torch.FloatTensor)\n",
        "        #for now, we only want to keep the column that indicates the character\n",
        "        self.Y_labels = Y_labels.iloc[:,0]\n",
        "        #we also apply one-hot encoding to the labels.\n",
        "        #USE EMBEDDING FOR BIGGER COLLECTIONS OF CHARACTERS\n",
        "        #self.Y_labels = np.array(pd.get_dummies(Y_labels.astype('str')))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        #img_name = os.path.join(self.root_dir,\n",
        "        #                        self.landmarks_frame.iloc[idx, 0])\n",
        "        #image = io.imread(img_name)\n",
        "        #landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        #landmarks = np.array([landmarks])\n",
        "        #landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        #sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        image = self.X_images[idx]\n",
        "        label = self.Y_labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            image.type(torch.FloatTensor)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23jRriua689n"
      },
      "source": [
        "#plt.imshow(numbers_dataset.X_images[0,:,:])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L5eTsTFIEre"
      },
      "source": [
        "#Y_numbers.reset_index(drop=True).head(50)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICcYez71HEF8"
      },
      "source": [
        "#numbers_dataset.Y_labels[0]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgLNDtv0LTOx"
      },
      "source": [
        "#max(numbers_dataset.Y_labels)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gn0g-mgDwMr"
      },
      "source": [
        "#type(numbers_dataset.Y_labels[0])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTCDrE6y2DmZ"
      },
      "source": [
        "# We want the label to go from 0 to label_max. In order to do that,\n",
        "# the following operation needs to be performed:\n",
        "\n",
        "Y_numbers.iloc[:, 0] = Y_numbers.iloc[:, 0] - number_min\n",
        "Y_upper.iloc[:, 0] = Y_upper.iloc[:, 0] - upper_min\n",
        "Y_lower.iloc[:, 0] = Y_lower.iloc[:, 0] - lower_min\n",
        "\n",
        "Y_numbers.reset_index(drop=True, inplace=True)\n",
        "Y_upper.reset_index(drop=True, inplace=True)\n",
        "Y_lower.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbu6Nata_UTw"
      },
      "source": [
        "numbers_dataset = FontsDataset(X_numbers, Y_numbers, transform)\n",
        "upper_dataset = FontsDataset(X_upper, Y_upper, transform)\n",
        "lower_dataset = FontsDataset(X_lower, Y_lower, transform)\n",
        "\n",
        "batch_size = 32 #THIS NUMBER MAY BE INTERESTING TO PUT IT AT THE BEGINNING OF THE CODE\n",
        "\n",
        "numbers_loader = DataLoader(numbers_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "upper_loader = DataLoader(upper_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "lower_loader = DataLoader(lower_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og_7jmTAAZl5",
        "outputId": "e66dd996-3863-4472-da1b-5118f036c4b1"
      },
      "source": [
        "numbers_dataset.Y_labels.unique()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 8, 1, 2, 5, 9, 7, 0, 6, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-QcXSF2Rsqh"
      },
      "source": [
        "#FUNCIÓN PARA PROBAR COSAS\n",
        "# SE IRÁ DEL CÓDIGO EVENTUALMENTE\n",
        "\n",
        "#index = 21\n",
        "\n",
        "#plt.figure()\n",
        "#for i in np.arange(index, index+10):\n",
        "#  plt.imshow(X_numbers[i])\n",
        "#  plt.show()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqIjRdQSpQJT"
      },
      "source": [
        "#FUNCIÓN PARA PROBAR COSAS\n",
        "# SE IRÁ DEL CÓDIGO EVENTUALMENTE\n",
        "\n",
        "#Esta función en concreto busca todos los caracteres de un tipo en concreto\n",
        "# y los dibuja\n",
        "\n",
        "#char_one = dfs[0].loc[:, 'm_label'] == 57\n",
        "#chars = dfs[0][char_one]\n",
        "#for i in range(chars.shape[0]):\n",
        "#  chars2 = np.array(chars.iloc[i, 12:])\n",
        "#  chars3 = np.reshape(chars2, newshape=(20,20))\n",
        "#  plt.figure()\n",
        "#  plt.imshow(chars3.astype(float))\n",
        "#  plt.show()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaVXHesWll3X"
      },
      "source": [
        "### Building the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJ5m2_juw0P"
      },
      "source": [
        "#this is the tensor used as 'error', or 'output not expected', as\n",
        "# using None is not possible because it raises some errors\n",
        "error_tensor = -1 * torch.ones(1)\n",
        "error_tensor = error_tensor.to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn78mMb5lpmN"
      },
      "source": [
        "##Right now, the generator only generates one image per execution,\n",
        "## maybe it should be readapted to generate a BATCH of images\n",
        "## EDIT: readapted!!\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, noise_dim=50, categorical_dim=10, categorical_noise_dim=30, image_size=20):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "    self.categorical_dim = categorical_dim\n",
        "    self.categorical_noise_dim = categorical_noise_dim\n",
        "\n",
        "    self.image_size = image_size #size of one of the sides of the image (we are working with square images)\n",
        "    self.hidden_channels = 128 #channels of the first convolutional layer\n",
        "    self.output_channels = 1 #the images we work with are black & white\n",
        "\n",
        "    self.hidden_layer = self.hidden_channels * (self.image_size // 4) ** 2\n",
        "\n",
        "    self.linear = nn.Linear(self.noise_dim + self.categorical_dim + self.categorical_noise_dim, self.hidden_layer)\n",
        "    self.bn = nn.BatchNorm1d(self.hidden_layer)\n",
        "    self.bn2 = nn.BatchNorm2d(self.hidden_channels//2)\n",
        "    #self.upscale = F.upsample_bilinear(2)\n",
        "    #self.upscale = nn.PixelShuffle(upscale_factor=2) # Better than F.upsample_bilinear: training GANs means avoiding sparse gradients\n",
        "    self.conv1 = nn.Conv2d(self.hidden_channels, self.hidden_channels//2, (3,3), stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(self.hidden_channels//2, self.output_channels, (3,3), stride=1, padding=1)\n",
        "\n",
        "\n",
        "  def forward(self, z, cat, z_cat):\n",
        "    if not z_cat == error_tensor:\n",
        "      x = self.bn(F.leaky_relu(self.linear(torch.cat((z, cat, z_cat), axis=1)), negative_slope=LRELU_ALPHA))\n",
        "    else:\n",
        "      x = self.bn(F.leaky_relu(self.linear(torch.cat((z, cat), axis=1)), negative_slope=LRELU_ALPHA))\n",
        "    x = x.view(-1, self.hidden_channels, self.image_size // 4, self.image_size // 4)\n",
        "    x = F.interpolate(x, scale_factor=2)\n",
        "    #x = self.bn2(F.leaky_relu(self.conv1(self.upscale(x)), , negative_slope=LRELU_ALPHA))\n",
        "    x = self.bn2(F.leaky_relu(self.conv1(x), negative_slope=LRELU_ALPHA))\n",
        "    x = F.interpolate(x, scale_factor=2)\n",
        "    x = torch.tanh(self.conv2(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqGdW9FrjX9B"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, noise_dim=50, categorical_dim=10, categorical_noise_dim=30, image_size=20):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "    self.categorical_dim = categorical_dim\n",
        "    self.categorical_noise_dim = categorical_noise_dim\n",
        "\n",
        "    self.image_size = image_size #size of one of the sides of the image (we are working with square images)\n",
        "    self.hidden_channels = 128 #channels of the last convolutional layer\n",
        "    self.input_channels = 1 #the images we work with are black & white\n",
        "\n",
        "    self.hidden_layer = self.hidden_channels * (self.image_size // 4) ** 2\n",
        "\n",
        "    self.conv1 = nn.Conv2d(self.input_channels, self.hidden_channels//2, (3,3), stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(self.hidden_channels//2, self.hidden_channels, (3,3), stride=1, padding=1)\n",
        "\n",
        "    self.bn = nn.BatchNorm2d(self.hidden_channels//2)\n",
        "    self.bn2 = nn.BatchNorm2d(self.hidden_channels)\n",
        "    self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.true_dense = nn.Linear(self.hidden_layer, 1)\n",
        "    self.cat_dense = nn.Linear(self.hidden_layer, self.categorical_dim)\n",
        "    if categorical_noise_dim > 0:\n",
        "      self.noise_dense = nn.Linear(self.hidden_layer, self.categorical_noise_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn(self.avgpool(F.leaky_relu(self.conv1(x), negative_slope=LRELU_ALPHA)))\n",
        "    x = self.bn2(self.avgpool(F.leaky_relu(self.conv2(x), negative_slope=LRELU_ALPHA)))\n",
        "    x = self.flatten(x)\n",
        "    true = F.sigmoid(self.true_dense(x))\n",
        "    cat = F.softmax(self.cat_dense(x))\n",
        "    if self.categorical_noise_dim > 0:\n",
        "      cat_noise = self.noise_dense(x)\n",
        "    else:\n",
        "      cat_noise = error_tensor\n",
        "\n",
        "    return true, cat, cat_noise\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw-GQzfVFULs"
      },
      "source": [
        "#Generation of the different datasets that we're going to send to the generator\n",
        "def generate_generator_data(batch_size, noise_dim=50, cat_dim=10, noise_cat_dim=30):\n",
        "\n",
        "  noise = torch.normal(mean=0, std=1, size=(batch_size, noise_dim))\n",
        "\n",
        "  #The categorical variable has to be one-hot encoded. This process does exactly that\n",
        "  cat = torch.zeros(size=(batch_size, cat_dim))\n",
        "  cat_vector = torch.randint(low=0, high=cat_dim, size=(batch_size,))\n",
        "  for i in range(batch_size):\n",
        "    cat[i, cat_vector[i]] = 1\n",
        "\n",
        "  if noise_cat_dim > 0:\n",
        "    noise_cat = torch.normal(mean=0, std=1, size=(batch_size, noise_cat_dim))\n",
        "  else:\n",
        "    noise_cat = error_tensor\n",
        "\n",
        "  return noise, cat, noise_cat"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_dOcxBKx2yR"
      },
      "source": [
        "# Dummification in torch\n",
        "def torch_dummies(variable, dim):\n",
        "  cat = torch.zeros(size=(len(variable), dim))\n",
        "  for i in range(len(variable)):\n",
        "    cat[i, variable[i]] = 1\n",
        "  return cat"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgGk2WhCLwWl"
      },
      "source": [
        "# MODEL SETUP (Gen, Discr, Loss, Optimizer)\n",
        "\n",
        "# After defining the generator and discriminator classes, we define the rest of elements\n",
        "# that are necessary for the training: loss functions and optimizers\n",
        "\n",
        "mse = nn.MSELoss()\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "mse, cross_entropy = mse.to(device), cross_entropy.to(device)\n",
        "\n",
        "# Functions for the calculation of the loss function\n",
        "def rand_ones_like(tensor):\n",
        "  return torch.rand_like(tensor)*0.3 + 0.9\n",
        "\n",
        "def rand_zeros_like(tensor):\n",
        "  return torch.rand_like(tensor)*0.3\n",
        "\n",
        "### Info-GAN ###\n",
        "if GAN_STYLE == 0:\n",
        "\n",
        "  lambda_cat = 1\n",
        "  lambda_con = 0.1\n",
        "\n",
        "  noise_dim = 50\n",
        "  cat_dim = 10\n",
        "  noise_cat_dim = 30\n",
        "\n",
        "  generator = Generator(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "  discriminator = Discriminator(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "\n",
        "  generator = generator.to(device)\n",
        "  discriminator = discriminator.to(device)\n",
        "\n",
        "\n",
        "  # -- Definition for each of the losses\n",
        "  # Generator loss: MSE Loss\n",
        "  def gen_loss(pred):\n",
        "    #print(max(pred))\n",
        "    return mse(pred, rand_ones_like(pred))\n",
        "\n",
        "  # Discriminator loss: two batches of MSE loss: one for each batch (true images and )\n",
        "  def discr_loss(real_pred, fake_pred):\n",
        "    #print(max(real_pred), max(fake_pred))\n",
        "    real_loss = mse(real_pred, rand_ones_like(real_pred))\n",
        "    fake_loss = mse(fake_pred, rand_zeros_like(fake_pred))\n",
        "    return (real_loss + fake_loss) / 2\n",
        "\n",
        "  # Features loss: batch of noise and categories predicted by the discr.\n",
        "  #  noise is continuous (MSE loss) and categories are not (cross entropy loss)\n",
        "  def features_loss(cat_input, cat_pred, noise_input, noise_pred):\n",
        "    return lambda_cat * cross_entropy(cat_input, torch.argmax(cat_pred, axis=1)) + lambda_con * mse(noise_input, noise_pred)\n",
        "\n",
        "  # Optimizer definition\n",
        "  gen_optim = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "  discr_optim = torch.optim.SGD(discriminator.parameters(), lr=0.001)\n",
        "  features_optim = torch.optim.Adam(chain(generator.parameters(), discriminator.parameters()))\n",
        "\n",
        "### AC-GAN ###\n",
        "elif GAN_STYLE == 1:\n",
        "\n",
        "  noise_dim = 50\n",
        "  cat_dim = 10\n",
        "  noise_cat_dim = 0 \n",
        "\n",
        "  generator = Generator(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "  discriminator = Discriminator(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "\n",
        "  generator = generator.to(device)\n",
        "  discriminator = discriminator.to(device)\n",
        "\n",
        "  # -- Definition for each of the losses\n",
        "  # Generator loss: MSE Loss\n",
        "  def gen_loss(pred, cat_input, cat_pred):\n",
        "    return 0.5 * (mse(pred, rand_ones_like(pred)) + cross_entropy(cat_input, torch.argmax(cat_pred, axis=1))/2.5)\n",
        "\n",
        "  # Discriminator loss: two batches of MSE loss: one for each batch (true images and )\n",
        "  def discr_loss(real_pred, fake_pred, real_cat_input, real_cat_labels, fake_cat_input, fake_cat_labels):\n",
        "    #print(real_cat_input.shape, real_cat_labels.shape)\n",
        "    #print(real_cat_input[0,:], real_cat_labels[0,:])\n",
        "    real_loss = 0.5 * (mse(real_pred, rand_ones_like(real_pred)) + cross_entropy(real_cat_input, torch.argmax(real_cat_labels, axis=1))/2.5)\n",
        "    fake_loss = 0.5 * (mse(fake_pred, rand_zeros_like(fake_pred)) + cross_entropy(fake_cat_input, torch.argmax(fake_cat_labels, axis=1))/2.5)\n",
        "\n",
        "    return (real_loss + fake_loss) / 2\n",
        "\n",
        "  # Optimizer definition\n",
        "  gen_optim = torch.optim.Adam(generator.parameters(), lr=LR_G)\n",
        "  #discr_optim = torch.optim.SGD(discriminator.parameters(), lr=LR_D)\n",
        "  discr_optim = torch.optim.Adam(discriminator.parameters(), lr=LR_D)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rifj4u5v3ack"
      },
      "source": [
        "# Noise in images\r\n",
        "def noisy_image(image_batch, epsilon):\r\n",
        "  #In this problem, images are normalised in the [-1, 1] range\r\n",
        "  noise = torch.rand_like(image_batch) * epsilon\r\n",
        "  noisy_image_batch = torch.clamp(image_batch + noise, min=-1, max=1)\r\n",
        "  return noisy_image_batch"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJR7zG0TZCV"
      },
      "source": [
        "# Noise strength decay\r\n",
        "def epsilon_decay(epoch, epsilon_decay_factor):\r\n",
        "  return epsilon_decay_factor / (epoch + 1)\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmww8RUaQKim",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59b345ce-1689-4ec6-960c-7131e0d6949c"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# --------\n",
        "# TRAINING\n",
        "# --------\n",
        "\n",
        "n_epochs = 5000\n",
        "dataloader = numbers_loader\n",
        "\n",
        "#images that we're going to use to see the performance of the Generator\n",
        "# at different epochs\n",
        "data_show = 6\n",
        "check_noise, check_cat, check_noise_cat = generate_generator_data(data_show, noise_cat_dim=0)\n",
        "check_noise, check_cat, check_noise_cat = check_noise.to(device), check_cat.to(device), check_noise_cat.to(device)\n",
        "\n",
        "# We set up these variables that are designed for plotting\n",
        "gen_loss_acum = []\n",
        "discr_loss_acum = []\n",
        "features_loss_acum = []\n",
        "\n",
        "# This training is only performed on the numbers dataset\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  gen_loss_epoch = 0\n",
        "  discr_loss_epoch = 0\n",
        "  features_loss_epoch = 0\n",
        "\n",
        "  for batch_idx, (real_img_batch, real_img_label) in enumerate(dataloader):\n",
        "\n",
        "    #print(real_img_label.shape, real_img_label)\n",
        "    real_img_batch = real_img_batch.to(device)\n",
        "    real_img_label = torch_dummies(real_img_label, cat_dim).to(device)\n",
        "    \n",
        "    #real_img_real_img_batch.type(torch.cuda.FloatTensor)\n",
        "\n",
        "    ## ------------------\n",
        "    ## GENERATOR TRAINING\n",
        "    ## ------------------\n",
        "\n",
        "    gen_optim.zero_grad()\n",
        "    #if GAN_STYLE == 0: #InfoGAN\n",
        "    noise, cat, noise_cat = generate_generator_data(batch_size, noise_dim, cat_dim, noise_cat_dim)\n",
        "    noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "    #elif GAN_STYLE == 1: #AC-GAN\n",
        "    img_batch = generator(noise, cat, noise_cat)\n",
        "    img_batch = img_batch.to(device)\n",
        "    img_batch = noisy_image(img_batch, epsilon_decay(epoch, EPSILON))\n",
        "    veracity, cat_pred, _ = discriminator(img_batch)\n",
        "    veracity, cat_pred = veracity.to(device), cat_pred.to(device)\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      generator_loss = gen_loss(veracity)\n",
        "    elif GAN_STYLE == 1: #AC-GAN\n",
        "      generator_loss = gen_loss(veracity, cat, cat_pred)\n",
        "    generator_loss.backward()\n",
        "    gen_optim.step()\n",
        "\n",
        "\n",
        "    ## ----------------------\n",
        "    ## DISCRIMINATOR TRAINING\n",
        "    ## ----------------------\n",
        "\n",
        "    discr_optim.zero_grad()\n",
        "\n",
        "    # First, we send a batch that has been created from the generator\n",
        "    noise, cat, noise_cat = generate_generator_data(batch_size, noise_dim, cat_dim, noise_cat_dim)\n",
        "    noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "    img_batch = generator(noise, cat, noise_cat).detach()\n",
        "    img_batch = img_batch.to(device)\n",
        "    img_batch = noisy_image(img_batch, epsilon_decay(epoch, EPSILON))\n",
        "    veracity_gen, cat_gen, _ = discriminator(img_batch)\n",
        "\n",
        "    # Then, we send a batch from the real images\n",
        "    real_img_batch = real_img_batch.type(torch.cuda.FloatTensor)\n",
        "    real_img_batch = noisy_image(real_img_batch, epsilon_decay(epoch, EPSILON))\n",
        "    veracity_real, cat_real_pred, _ = discriminator(real_img_batch.type(torch.cuda.FloatTensor))\n",
        "\n",
        "    # We calculate the loss and apply backpropagation\n",
        "    #print(noise_cat.shape, cat_gen.shape)\n",
        "    discriminator_loss = discr_loss(veracity_real, veracity_gen, real_img_label, cat_real_pred, cat, cat_gen)\n",
        "    discriminator_loss.backward()\n",
        "    discr_optim.step()\n",
        "\n",
        "\n",
        "    ## ----------------\n",
        "    ## FEATURE TRAINING\n",
        "    ## ----------------\n",
        "\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      features_optim.zero_grad()\n",
        "\n",
        "      noise, cat, noise_cat = generate_generator_data(batch_size, noise_dim, cat_dim, noise_cat_dim)\n",
        "      noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "      img_batch = generator(noise, cat, noise_cat)\n",
        "      img_batch = img_batch.to(device)\n",
        "      _, cat_pred, noise_cat_pred = discriminator(img_batch)\n",
        "      loss = features_loss(cat, cat_pred, noise_cat, noise_cat_pred)\n",
        "      loss.backward()\n",
        "      features_optim.step()\n",
        "\n",
        "    #print(generator_loss, generator_loss.item(), gen_loss_epoch + generator_loss.item())\n",
        "\n",
        "    gen_loss_epoch = gen_loss_epoch + generator_loss.item()\n",
        "    discr_loss_epoch = discr_loss_epoch + discriminator_loss.item()\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      features_loss_epoch = features_loss_epoch + loss.item()\n",
        "\n",
        "\n",
        "    #END OF BATCH TEXT\n",
        "    if batch_idx % 300 == 0:\n",
        "      print(\n",
        "          f'Train Epoch: {epoch+1}/{n_epochs} [{batch_idx*len(real_img_batch)}/{len(dataloader.dataset)} ({round(100. * batch_idx / len(dataloader), 1)}%)]')\n",
        "      #print(generator_loss, discriminator_loss, gen_loss_epoch, discr_loss_epoch)\n",
        "\n",
        "  # LOSS PLOT\n",
        "  with torch.no_grad():\n",
        "    gen_loss_epoch = gen_loss_epoch / (batch_idx+1)\n",
        "    discr_loss_epoch = discr_loss_epoch / (batch_idx+1)\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      features_loss_epoch = features_loss_epoch / (batch_idx+1)\n",
        "    #gen_loss_acum.append(gen_loss_epoch.detach())\n",
        "    #discr_loss_acum.append(discr_loss_epoch.detach())\n",
        "    gen_loss_acum.append(gen_loss_epoch)\n",
        "    discr_loss_acum.append(discr_loss_epoch)\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      #features_loss_acum.append(features_loss_epoch.detach())\n",
        "      features_loss_acum.append(features_loss_epoch)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  #print(gen_loss_epoch, discr_loss_epoch)\n",
        "  if GAN_STYLE == 0: #InfoGAN\n",
        "    plt.figure(figsize=(12,12))\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    plt.plot(gen_loss_acum, 'r')\n",
        "    plt.plot(discr_loss_acum, 'g')\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(features_loss_acum, 'b')\n",
        "    plt.show()\n",
        "  elif GAN_STYLE == 1: #AC_GAN\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(gen_loss_acum, 'r')\n",
        "    plt.plot(discr_loss_acum, 'g')\n",
        "    plt.show()\n",
        "\n",
        "  # SHOW THE GENERATOR'S INNER WORKINGS\n",
        "  #data_show = 6\n",
        "  #noise, cat, noise_cat = generate_generator_data(data_show, noise_cat_dim=0)\n",
        "  #noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "  img_batch = generator(check_noise, check_cat, check_noise_cat)\n",
        "  plt.figure(figsize=(4,4))\n",
        "  for i in range(data_show):\n",
        "    plt.subplot(data_show // 3, 3, i+1)\n",
        "    plt.imshow(img_batch[i,0,:,:].cpu().detach().numpy())\n",
        "  \n",
        "\n",
        "  ##GENERAR EL MISMO RUIDO AL PRINCIPIO DEL ENTRENAMIENTO PARA LOS PLOTS DEL GENERADOR\n",
        "  # ASÍ SE VE LA EVOLUCIÓN DE LAS MIMSAS MUESTRAS DE RUIDO"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 3/5000 [0/210229 (0.0%)]\n",
            "Train Epoch: 3/5000 [9600/210229 (4.6%)]\n",
            "Train Epoch: 3/5000 [19200/210229 (9.1%)]\n",
            "Train Epoch: 3/5000 [28800/210229 (13.7%)]\n",
            "Train Epoch: 3/5000 [38400/210229 (18.3%)]\n",
            "Train Epoch: 3/5000 [48000/210229 (22.8%)]\n",
            "Train Epoch: 3/5000 [57600/210229 (27.4%)]\n",
            "Train Epoch: 3/5000 [67200/210229 (32.0%)]\n",
            "Train Epoch: 3/5000 [76800/210229 (36.5%)]\n",
            "Train Epoch: 3/5000 [86400/210229 (41.1%)]\n",
            "Train Epoch: 3/5000 [96000/210229 (45.7%)]\n",
            "Train Epoch: 3/5000 [105600/210229 (50.2%)]\n",
            "Train Epoch: 3/5000 [115200/210229 (54.8%)]\n",
            "Train Epoch: 3/5000 [124800/210229 (59.4%)]\n",
            "Train Epoch: 3/5000 [134400/210229 (63.9%)]\n",
            "Train Epoch: 3/5000 [144000/210229 (68.5%)]\n",
            "Train Epoch: 3/5000 [153600/210229 (73.1%)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-bb5716ae4d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mGAN_STYLE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#AC-GAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mgenerator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mveracity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mgen_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQNJcmFEMUO"
      },
      "source": [
        "noise, cat, noise_cat = generate_generator_data(20)\n",
        "noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "img_batch = generator(noise, cat, noise_cat)\n",
        "plt.imshow(img_batch[14,0,:,:].cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}