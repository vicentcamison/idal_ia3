{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_GAN_GAN_example_GIT_tf1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/Sesion%203/4_Keras_GAN_GAN_example_GIT_tf1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj9oh6HGnGoa"
      },
      "source": [
        "# Keras-GAN\n",
        "https://github.com/eriklindernoren/Keras-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmITZlpwn3HV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c4fd9083-6095-482d-fe96-4e069721a372"
      },
      "source": [
        "## ESTO USA TENSORFLOW 1 EN VEZ DEL TENSORFLOW ACTUAL\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2hI9lfdpnmP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "235d65be-eb06-44aa-aeea-082bced595f6"
      },
      "source": [
        "!git clone https://github.com/eriklindernoren/Keras-GAN\n",
        "%cd Keras-GAN/\n",
        "!sudo pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Keras-GAN'...\n",
            "remote: Enumerating objects: 3480, done.\u001b[K\n",
            "remote: Total 3480 (delta 0), reused 0 (delta 0), pack-reused 3480\u001b[K\n",
            "Receiving objects: 100% (3480/3480), 88.40 MiB | 31.31 MiB/s, done.\n",
            "Resolving deltas: 100% (562/562), done.\n",
            "/content/Keras-GAN\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git (from -r requirements.txt (line 2))\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-7ue36l8_\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-7ue36l8_\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.3.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 9)) (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 9)) (4.4.2)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=4045b2f64cd40298d8f43726dffd3ff5849e654561a5f7080d1273ae32d7c806\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_0p006qq/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGKMsA2dpnqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "44c80909-ab2f-4c22-9f96-041ebdd782ab"
      },
      "source": [
        "%cd 'gan'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Keras-GAN/gan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq09PeVpW6_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b676e4f4-2459-42c7-e9bc-eac29e40f20f"
      },
      "source": [
        "from gan import GAN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgFxB79Wq4kV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3b4e839-1c23-4a07-c3a8-0eca63d0ce7d"
      },
      "source": [
        "model = GAN()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRcBAbkJq8nj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d63abb04-e804-417e-b588-f54dc1d326c4"
      },
      "source": [
        "model.train(epochs=200, batch_size=32, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.662868, acc.: 65.62%] [G loss: 0.958124]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 0.340069, acc.: 96.88%] [G loss: 0.949020]\n",
            "2 [D loss: 0.301054, acc.: 98.44%] [G loss: 0.899357]\n",
            "3 [D loss: 0.290559, acc.: 92.19%] [G loss: 0.983950]\n",
            "4 [D loss: 0.253338, acc.: 98.44%] [G loss: 1.168158]\n",
            "5 [D loss: 0.243877, acc.: 98.44%] [G loss: 1.290248]\n",
            "6 [D loss: 0.200403, acc.: 100.00%] [G loss: 1.433765]\n",
            "7 [D loss: 0.182042, acc.: 100.00%] [G loss: 1.548208]\n",
            "8 [D loss: 0.143257, acc.: 100.00%] [G loss: 1.817457]\n",
            "9 [D loss: 0.115411, acc.: 100.00%] [G loss: 2.023963]\n",
            "10 [D loss: 0.097767, acc.: 100.00%] [G loss: 2.080907]\n",
            "11 [D loss: 0.079104, acc.: 100.00%] [G loss: 2.258976]\n",
            "12 [D loss: 0.078423, acc.: 100.00%] [G loss: 2.340013]\n",
            "13 [D loss: 0.064062, acc.: 100.00%] [G loss: 2.507384]\n",
            "14 [D loss: 0.061022, acc.: 100.00%] [G loss: 2.621747]\n",
            "15 [D loss: 0.052388, acc.: 100.00%] [G loss: 2.728010]\n",
            "16 [D loss: 0.058631, acc.: 100.00%] [G loss: 2.730093]\n",
            "17 [D loss: 0.035763, acc.: 100.00%] [G loss: 2.784117]\n",
            "18 [D loss: 0.048604, acc.: 100.00%] [G loss: 2.896918]\n",
            "19 [D loss: 0.035161, acc.: 100.00%] [G loss: 2.963164]\n",
            "20 [D loss: 0.038159, acc.: 100.00%] [G loss: 3.071298]\n",
            "21 [D loss: 0.034235, acc.: 100.00%] [G loss: 3.108855]\n",
            "22 [D loss: 0.034074, acc.: 100.00%] [G loss: 3.202353]\n",
            "23 [D loss: 0.037671, acc.: 100.00%] [G loss: 3.151042]\n",
            "24 [D loss: 0.027463, acc.: 100.00%] [G loss: 3.383335]\n",
            "25 [D loss: 0.023325, acc.: 100.00%] [G loss: 3.486017]\n",
            "26 [D loss: 0.027743, acc.: 100.00%] [G loss: 3.472665]\n",
            "27 [D loss: 0.021581, acc.: 100.00%] [G loss: 3.587607]\n",
            "28 [D loss: 0.027337, acc.: 100.00%] [G loss: 3.567510]\n",
            "29 [D loss: 0.025487, acc.: 100.00%] [G loss: 3.599394]\n",
            "30 [D loss: 0.017884, acc.: 100.00%] [G loss: 3.675567]\n",
            "31 [D loss: 0.021279, acc.: 100.00%] [G loss: 3.630782]\n",
            "32 [D loss: 0.019684, acc.: 100.00%] [G loss: 3.778450]\n",
            "33 [D loss: 0.021564, acc.: 100.00%] [G loss: 3.682271]\n",
            "34 [D loss: 0.016636, acc.: 100.00%] [G loss: 3.951317]\n",
            "35 [D loss: 0.016196, acc.: 100.00%] [G loss: 3.872902]\n",
            "36 [D loss: 0.015769, acc.: 100.00%] [G loss: 3.972453]\n",
            "37 [D loss: 0.017696, acc.: 100.00%] [G loss: 3.815853]\n",
            "38 [D loss: 0.019960, acc.: 100.00%] [G loss: 4.007559]\n",
            "39 [D loss: 0.015253, acc.: 100.00%] [G loss: 3.919352]\n",
            "40 [D loss: 0.017295, acc.: 100.00%] [G loss: 4.059563]\n",
            "41 [D loss: 0.012301, acc.: 100.00%] [G loss: 4.036730]\n",
            "42 [D loss: 0.015210, acc.: 100.00%] [G loss: 3.982812]\n",
            "43 [D loss: 0.017266, acc.: 100.00%] [G loss: 3.993567]\n",
            "44 [D loss: 0.012196, acc.: 100.00%] [G loss: 4.223479]\n",
            "45 [D loss: 0.015427, acc.: 100.00%] [G loss: 4.169639]\n",
            "46 [D loss: 0.012175, acc.: 100.00%] [G loss: 4.112687]\n",
            "47 [D loss: 0.013565, acc.: 100.00%] [G loss: 4.171237]\n",
            "48 [D loss: 0.014273, acc.: 100.00%] [G loss: 4.248107]\n",
            "49 [D loss: 0.012912, acc.: 100.00%] [G loss: 4.309942]\n",
            "50 [D loss: 0.012674, acc.: 100.00%] [G loss: 4.286811]\n",
            "51 [D loss: 0.011621, acc.: 100.00%] [G loss: 4.378309]\n",
            "52 [D loss: 0.011202, acc.: 100.00%] [G loss: 4.365617]\n",
            "53 [D loss: 0.015654, acc.: 100.00%] [G loss: 4.373488]\n",
            "54 [D loss: 0.010696, acc.: 100.00%] [G loss: 4.494223]\n",
            "55 [D loss: 0.009884, acc.: 100.00%] [G loss: 4.265813]\n",
            "56 [D loss: 0.008301, acc.: 100.00%] [G loss: 4.459306]\n",
            "57 [D loss: 0.011441, acc.: 100.00%] [G loss: 4.432256]\n",
            "58 [D loss: 0.009841, acc.: 100.00%] [G loss: 4.485007]\n",
            "59 [D loss: 0.008734, acc.: 100.00%] [G loss: 4.447143]\n",
            "60 [D loss: 0.012763, acc.: 100.00%] [G loss: 4.545273]\n",
            "61 [D loss: 0.010959, acc.: 100.00%] [G loss: 4.364186]\n",
            "62 [D loss: 0.011126, acc.: 100.00%] [G loss: 4.384234]\n",
            "63 [D loss: 0.010714, acc.: 100.00%] [G loss: 4.638172]\n",
            "64 [D loss: 0.009090, acc.: 100.00%] [G loss: 4.583395]\n",
            "65 [D loss: 0.012238, acc.: 100.00%] [G loss: 4.346736]\n",
            "66 [D loss: 0.011423, acc.: 100.00%] [G loss: 4.641381]\n",
            "67 [D loss: 0.011311, acc.: 100.00%] [G loss: 4.603583]\n",
            "68 [D loss: 0.009959, acc.: 100.00%] [G loss: 4.538496]\n",
            "69 [D loss: 0.007731, acc.: 100.00%] [G loss: 4.675811]\n",
            "70 [D loss: 0.011824, acc.: 100.00%] [G loss: 4.535632]\n",
            "71 [D loss: 0.011091, acc.: 100.00%] [G loss: 4.627150]\n",
            "72 [D loss: 0.012280, acc.: 100.00%] [G loss: 4.600061]\n",
            "73 [D loss: 0.009387, acc.: 100.00%] [G loss: 4.542606]\n",
            "74 [D loss: 0.022113, acc.: 100.00%] [G loss: 4.682120]\n",
            "75 [D loss: 0.007399, acc.: 100.00%] [G loss: 4.942986]\n",
            "76 [D loss: 0.006992, acc.: 100.00%] [G loss: 4.709047]\n",
            "77 [D loss: 0.009778, acc.: 100.00%] [G loss: 4.748648]\n",
            "78 [D loss: 0.007546, acc.: 100.00%] [G loss: 4.788130]\n",
            "79 [D loss: 0.010651, acc.: 100.00%] [G loss: 4.617116]\n",
            "80 [D loss: 0.008576, acc.: 100.00%] [G loss: 4.867959]\n",
            "81 [D loss: 0.011372, acc.: 100.00%] [G loss: 4.819108]\n",
            "82 [D loss: 0.010298, acc.: 100.00%] [G loss: 4.880155]\n",
            "83 [D loss: 0.009785, acc.: 100.00%] [G loss: 4.729070]\n",
            "84 [D loss: 0.009757, acc.: 100.00%] [G loss: 4.949347]\n",
            "85 [D loss: 0.008129, acc.: 100.00%] [G loss: 4.910283]\n",
            "86 [D loss: 0.010684, acc.: 100.00%] [G loss: 4.898911]\n",
            "87 [D loss: 0.006309, acc.: 100.00%] [G loss: 4.797199]\n",
            "88 [D loss: 0.007913, acc.: 100.00%] [G loss: 4.829542]\n",
            "89 [D loss: 0.014999, acc.: 100.00%] [G loss: 4.802633]\n",
            "90 [D loss: 0.010318, acc.: 100.00%] [G loss: 5.109510]\n",
            "91 [D loss: 0.008415, acc.: 100.00%] [G loss: 4.922379]\n",
            "92 [D loss: 0.008302, acc.: 100.00%] [G loss: 4.905221]\n",
            "93 [D loss: 0.008038, acc.: 100.00%] [G loss: 4.974388]\n",
            "94 [D loss: 0.010498, acc.: 100.00%] [G loss: 4.912677]\n",
            "95 [D loss: 0.007419, acc.: 100.00%] [G loss: 5.101902]\n",
            "96 [D loss: 0.012860, acc.: 100.00%] [G loss: 5.142115]\n",
            "97 [D loss: 0.009131, acc.: 100.00%] [G loss: 5.081017]\n",
            "98 [D loss: 0.016054, acc.: 100.00%] [G loss: 5.250407]\n",
            "99 [D loss: 0.008490, acc.: 100.00%] [G loss: 5.242712]\n",
            "100 [D loss: 0.011493, acc.: 100.00%] [G loss: 5.253188]\n",
            "101 [D loss: 0.012299, acc.: 100.00%] [G loss: 5.024633]\n",
            "102 [D loss: 0.008731, acc.: 100.00%] [G loss: 5.047956]\n",
            "103 [D loss: 0.008990, acc.: 100.00%] [G loss: 5.071674]\n",
            "104 [D loss: 0.009973, acc.: 100.00%] [G loss: 5.317414]\n",
            "105 [D loss: 0.017774, acc.: 100.00%] [G loss: 5.365364]\n",
            "106 [D loss: 0.010781, acc.: 100.00%] [G loss: 5.363913]\n",
            "107 [D loss: 0.012147, acc.: 100.00%] [G loss: 5.136544]\n",
            "108 [D loss: 0.007214, acc.: 100.00%] [G loss: 5.360482]\n",
            "109 [D loss: 0.012690, acc.: 100.00%] [G loss: 5.324060]\n",
            "110 [D loss: 0.009837, acc.: 100.00%] [G loss: 5.332520]\n",
            "111 [D loss: 0.015234, acc.: 100.00%] [G loss: 5.266263]\n",
            "112 [D loss: 0.025089, acc.: 100.00%] [G loss: 5.653928]\n",
            "113 [D loss: 0.023622, acc.: 100.00%] [G loss: 5.142747]\n",
            "114 [D loss: 0.011212, acc.: 100.00%] [G loss: 5.216481]\n",
            "115 [D loss: 0.008343, acc.: 100.00%] [G loss: 5.091325]\n",
            "116 [D loss: 0.010375, acc.: 100.00%] [G loss: 5.442129]\n",
            "117 [D loss: 0.009671, acc.: 100.00%] [G loss: 5.221106]\n",
            "118 [D loss: 0.026602, acc.: 100.00%] [G loss: 5.417857]\n",
            "119 [D loss: 0.011911, acc.: 100.00%] [G loss: 5.301939]\n",
            "120 [D loss: 0.017135, acc.: 100.00%] [G loss: 5.078466]\n",
            "121 [D loss: 0.054860, acc.: 98.44%] [G loss: 5.648555]\n",
            "122 [D loss: 0.023784, acc.: 100.00%] [G loss: 4.874899]\n",
            "123 [D loss: 0.030861, acc.: 100.00%] [G loss: 5.760927]\n",
            "124 [D loss: 0.038015, acc.: 100.00%] [G loss: 5.711718]\n",
            "125 [D loss: 0.177232, acc.: 90.62%] [G loss: 5.520954]\n",
            "126 [D loss: 0.009303, acc.: 100.00%] [G loss: 6.089518]\n",
            "127 [D loss: 0.178885, acc.: 92.19%] [G loss: 4.780715]\n",
            "128 [D loss: 0.034426, acc.: 100.00%] [G loss: 5.636000]\n",
            "129 [D loss: 0.058734, acc.: 96.88%] [G loss: 6.318302]\n",
            "130 [D loss: 0.407213, acc.: 79.69%] [G loss: 4.807440]\n",
            "131 [D loss: 0.108329, acc.: 96.88%] [G loss: 6.136909]\n",
            "132 [D loss: 0.037561, acc.: 100.00%] [G loss: 5.670109]\n",
            "133 [D loss: 0.042606, acc.: 98.44%] [G loss: 5.737779]\n",
            "134 [D loss: 0.058106, acc.: 100.00%] [G loss: 4.467759]\n",
            "135 [D loss: 0.081495, acc.: 96.88%] [G loss: 4.838928]\n",
            "136 [D loss: 0.059670, acc.: 98.44%] [G loss: 5.803828]\n",
            "137 [D loss: 0.241712, acc.: 93.75%] [G loss: 3.703064]\n",
            "138 [D loss: 0.141940, acc.: 90.62%] [G loss: 4.932433]\n",
            "139 [D loss: 0.034499, acc.: 100.00%] [G loss: 5.566973]\n",
            "140 [D loss: 1.189956, acc.: 51.56%] [G loss: 3.095711]\n",
            "141 [D loss: 0.749911, acc.: 79.69%] [G loss: 2.756690]\n",
            "142 [D loss: 0.291480, acc.: 84.38%] [G loss: 4.198547]\n",
            "143 [D loss: 0.105272, acc.: 95.31%] [G loss: 5.599941]\n",
            "144 [D loss: 0.022412, acc.: 100.00%] [G loss: 5.795262]\n",
            "145 [D loss: 0.188406, acc.: 90.62%] [G loss: 4.463133]\n",
            "146 [D loss: 0.114770, acc.: 92.19%] [G loss: 4.671201]\n",
            "147 [D loss: 0.072656, acc.: 98.44%] [G loss: 4.973778]\n",
            "148 [D loss: 0.125652, acc.: 98.44%] [G loss: 3.996858]\n",
            "149 [D loss: 0.072455, acc.: 98.44%] [G loss: 4.195649]\n",
            "150 [D loss: 0.059359, acc.: 98.44%] [G loss: 4.220171]\n",
            "151 [D loss: 0.151718, acc.: 96.88%] [G loss: 3.944000]\n",
            "152 [D loss: 0.095905, acc.: 96.88%] [G loss: 4.380958]\n",
            "153 [D loss: 0.327591, acc.: 85.94%] [G loss: 4.072651]\n",
            "154 [D loss: 0.064142, acc.: 98.44%] [G loss: 4.662031]\n",
            "155 [D loss: 0.131093, acc.: 95.31%] [G loss: 3.517155]\n",
            "156 [D loss: 0.120875, acc.: 95.31%] [G loss: 4.034896]\n",
            "157 [D loss: 0.133061, acc.: 95.31%] [G loss: 4.106264]\n",
            "158 [D loss: 0.444137, acc.: 78.12%] [G loss: 3.675268]\n",
            "159 [D loss: 0.186125, acc.: 93.75%] [G loss: 3.908170]\n",
            "160 [D loss: 0.330216, acc.: 84.38%] [G loss: 3.655179]\n",
            "161 [D loss: 0.079353, acc.: 96.88%] [G loss: 4.269999]\n",
            "162 [D loss: 0.216232, acc.: 92.19%] [G loss: 3.867659]\n",
            "163 [D loss: 0.204315, acc.: 89.06%] [G loss: 4.800176]\n",
            "164 [D loss: 0.470513, acc.: 76.56%] [G loss: 3.706913]\n",
            "165 [D loss: 0.052993, acc.: 100.00%] [G loss: 4.337223]\n",
            "166 [D loss: 0.193471, acc.: 95.31%] [G loss: 3.670783]\n",
            "167 [D loss: 0.101824, acc.: 100.00%] [G loss: 4.666823]\n",
            "168 [D loss: 0.159742, acc.: 96.88%] [G loss: 2.729759]\n",
            "169 [D loss: 0.183987, acc.: 92.19%] [G loss: 3.705058]\n",
            "170 [D loss: 0.108872, acc.: 98.44%] [G loss: 4.730097]\n",
            "171 [D loss: 0.556336, acc.: 75.00%] [G loss: 2.997852]\n",
            "172 [D loss: 0.232089, acc.: 85.94%] [G loss: 4.690232]\n",
            "173 [D loss: 0.139590, acc.: 98.44%] [G loss: 3.999717]\n",
            "174 [D loss: 0.145504, acc.: 92.19%] [G loss: 4.183695]\n",
            "175 [D loss: 0.162584, acc.: 95.31%] [G loss: 4.407309]\n",
            "176 [D loss: 0.386408, acc.: 81.25%] [G loss: 3.434371]\n",
            "177 [D loss: 0.166619, acc.: 92.19%] [G loss: 4.003377]\n",
            "178 [D loss: 0.215266, acc.: 90.62%] [G loss: 4.332253]\n",
            "179 [D loss: 0.348253, acc.: 85.94%] [G loss: 3.030642]\n",
            "180 [D loss: 0.177558, acc.: 90.62%] [G loss: 4.501366]\n",
            "181 [D loss: 0.248528, acc.: 87.50%] [G loss: 3.130957]\n",
            "182 [D loss: 0.140652, acc.: 95.31%] [G loss: 3.748848]\n",
            "183 [D loss: 0.167914, acc.: 96.88%] [G loss: 3.588095]\n",
            "184 [D loss: 0.185871, acc.: 93.75%] [G loss: 3.434976]\n",
            "185 [D loss: 0.178327, acc.: 95.31%] [G loss: 4.322890]\n",
            "186 [D loss: 1.498520, acc.: 40.62%] [G loss: 1.490364]\n",
            "187 [D loss: 0.543486, acc.: 71.88%] [G loss: 1.711444]\n",
            "188 [D loss: 0.139957, acc.: 96.88%] [G loss: 3.543586]\n",
            "189 [D loss: 0.097776, acc.: 98.44%] [G loss: 4.341696]\n",
            "190 [D loss: 0.208373, acc.: 93.75%] [G loss: 2.634249]\n",
            "191 [D loss: 0.114983, acc.: 98.44%] [G loss: 3.218293]\n",
            "192 [D loss: 0.038508, acc.: 100.00%] [G loss: 3.614743]\n",
            "193 [D loss: 0.114100, acc.: 98.44%] [G loss: 3.446099]\n",
            "194 [D loss: 0.108326, acc.: 96.88%] [G loss: 3.439859]\n",
            "195 [D loss: 0.123779, acc.: 100.00%] [G loss: 3.377267]\n",
            "196 [D loss: 0.108567, acc.: 98.44%] [G loss: 3.854926]\n",
            "197 [D loss: 0.238175, acc.: 93.75%] [G loss: 3.822510]\n",
            "198 [D loss: 0.459112, acc.: 79.69%] [G loss: 2.529204]\n",
            "199 [D loss: 0.153074, acc.: 93.75%] [G loss: 3.577702]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po309LHJnlFx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}