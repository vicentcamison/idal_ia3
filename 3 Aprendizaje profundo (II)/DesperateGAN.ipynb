{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/3%20Aprendizaje%20profundo%20(II)/DesperateGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrPUEoXu4Kwc"
      },
      "source": [
        "#import os\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_zyeJVjPxzH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import glob\n",
        "\n",
        "from itertools import chain"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A7cX_TKXiNg"
      },
      "source": [
        "# CONTROL VARIABLES\n",
        "\n",
        "# GAN style\n",
        "# 0: InfoGAN\n",
        "# 1: AC-GAN (Auxiliary Conditional GAN)\n",
        "GAN_STYLE = 1\n",
        "\n",
        "\n",
        "# lesser importance variables\n",
        "BATCH_SIZE = 256\n",
        "LRELU_ALPHA = 0.2 #negative slope of leaky relu activation\n",
        "LR_G = 0.0004 #generator learning rate\n",
        "LR_D = 0.0002 #discriminator learning rate\n",
        "NOISE = True\n",
        "EPSILON = 0.15 #noise factor in images to discriminator\n",
        "EPSILON_EXPONENT = 0.5 #damping to the noise factor in images along different epochs\n",
        "GEN_PER_DISCR = 3 #times the generator is trained each time the discriminator is trained\n",
        "FLIP_LABELS = True\n",
        "DISCR_LABEL_INVERSION_FACTOR = 2 # label inversion for discriminator loss has a chance of 1 over 1/(2+epoch*DISCR_LABEL_INV_FACTOR) ** DISCR_lABEL_INVERSION_PWR\n",
        "DISCR_LABEL_INVERSION_PWR = 0.5"
      ],
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kiXnudPmyF"
      },
      "source": [
        "We are going to create a GAN to generate new fonts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6DzjWi9Py-Y",
        "outputId": "194b5ed3-fb4e-43eb-ad3c-e49c79dac9a1"
      },
      "source": [
        "# Access to the BOB ROSS images folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XE9urSNQRO2"
      },
      "source": [
        "#directory = \"gdrive/MyDrive/UNIVERSIDAD/Master Propio IA UV/3 Aprendizaje profundo (II)/Datasets/Bob Ross/train\"\n",
        "directory = \"gdrive/MyDrive/UNIVERSIDAD/Master Propio IA UV/3 Aprendizaje profundo (II)/Trabajo final/fonts\""
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2-COIKsvtCk"
      },
      "source": [
        "# get data file names (this code takes a while to retrieve all data and prepare all dataframes)\n",
        "filenames = glob.glob(directory + \"/*.csv\")\n",
        "dfs = [pd.read_csv(filename) for filename in filenames]"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WHqiLelw80u",
        "outputId": "7a771a22-b56c-4d1e-db1f-33b00baab4fb"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHz6XvH8Vd8c"
      },
      "source": [
        "# Characters have an index associated to them (m_label). The index is related to the character in the following dictionary\n",
        "index_to_char = {65:'A', 66:'B', 67:'C', 68:'D', 69:'E', 70:'F', 71:'G', 72:'H', 73:'I', 74:'J',\n",
        "                 75:'K', 76:'L', 77:'M', 78:'N', 79:'O', 80:'P', 81:'Q', 82:'R', 83:'S', 84:'T',\n",
        "                 85:'U', 86:'V', 87:'W', 88:'X', 89:'Y', 90:'Z', 97:'a', 98:'b', 99:'c', 100:'d',\n",
        "                 101:'e', 102:'f', 103:'g', 104:'h', 105:'i', 106:'j', 107:'k', 108:'l', 109:'m',\n",
        "                 110:'n', 111:'o', 112:'p', 113:'q', 114:'r', 115:'s', 116:'t', 117:'u', 118:'v',\n",
        "                 119:'w', 120:'x', 121:'y', 122:'z', 48:'0', 49:'1', 50:'2', 51:'3', 52:'4', 53:'5',\n",
        "                 54:'6', 55:'7', 56:'8', 57:'9'}"
      ],
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USLE1ztGoADb"
      },
      "source": [
        "#number_min = 48\n",
        "number_min = 50\n",
        "number_max = 50\n",
        "#number_max = 49\n",
        "#number_max = 57\n",
        "upper_min = 65\n",
        "upper_max = 90\n",
        "lower_min = 97\n",
        "lower_max = 122"
      ],
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAVfnqrzfmxJ",
        "outputId": "79c84133-8c91-4bde-90a9-98a83031ae55"
      },
      "source": [
        "# We create the datasets. We are going to divide them in NUMBERS, UPPERCASE LETTERS and LOWERCASE LETTERS\n",
        "X_numbers = np.array([], ndmin=3)\n",
        "X_upper = np.array([], ndmin=3)\n",
        "X_lower = np.array([], ndmin=3)\n",
        "Y_numbers = np.array([], ndmin=2)\n",
        "Y_upper = np.array([], ndmin=2)\n",
        "Y_lower = np.array([], ndmin=2)\n",
        "\n",
        "i = 0\n",
        "for df in dfs:\n",
        "  #df = df[np.logical_and(df.loc[:,'strength'] < 0.5, df.loc[:,'italic'] == 0]\n",
        "  df_straight = df[np.logical_and(df.loc[:,'strength'] < 0.5, df.loc[:,'italic'] == 0)] #remove the columns that are in bold or cursive\n",
        "  #BY SLIGHTLY MODIFYING THIS LINE OF CODE AND ADDING THE CORRESPONDING LABELS TO Y, CURSIVE AND BOLD CAN BE ADDED TO THE MODEL\n",
        "\n",
        "  # We get the elements of the dataframe that reference the images of characters (upper and lower letters, and numbers)\n",
        "  X_numbers_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= number_min, df_straight.loc[:, 'm_label'] <= number_max)].iloc[:, 12:]\n",
        "  X_upper_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= upper_min, df_straight.loc[:, 'm_label'] <= upper_max)].iloc[:, 12:]\n",
        "  X_lower_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= lower_min, df_straight.loc[:, 'm_label'] <= lower_max)].iloc[:, 12:]\n",
        "\n",
        "  Y_numbers_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= number_min, df_straight.loc[:, 'm_label'] <= number_max)].loc[:, ['m_label', 'font']]\n",
        "  Y_upper_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= upper_min, df_straight.loc[:, 'm_label'] <= upper_max)].loc[:, ['m_label', 'font']]\n",
        "  Y_lower_new = df_straight[np.logical_and(df_straight.loc[:,'m_label'] >= lower_min, df_straight.loc[:, 'm_label'] <= lower_max)].loc[:, ['m_label', 'font']]\n",
        "\n",
        "  # We transform each element from a 400x1 array into a 20x20 array\n",
        "  #X_numbers_new = np.reshape(X_numbers_new.values, (20,20,-1))\n",
        "  #X_upper_new = np.reshape(X_upper_new.values, (20,20,-1))\n",
        "  #X_lower_new = np.reshape(X_lower_new.values, (20,20,-1))\n",
        "\n",
        "  # We rearrange the indexes so that the first index refers to image index\n",
        "  #X_numbers_new = np.transpose(X_numbers_new, axes=[2, 0, 1])\n",
        "  #X_upper_new = np.transpose(X_upper_new, axes=[2, 0, 1])\n",
        "  #X_lower_new = np.transpose(X_lower_new, axes=[2, 0, 1])\n",
        "\n",
        "  X_numbers_new = np.array(np.reshape(X_numbers_new.values, (-1,20,20)))\n",
        "  X_upper_new = np.array(np.reshape(X_upper_new.values, (-1,20,20)))\n",
        "  X_lower_new = np.array(np.reshape(X_lower_new.values, (-1,20,20)))\n",
        "\n",
        "  X_numbers_new = X_numbers_new.astype('float64')\n",
        "  X_upper_new = X_upper_new.astype('float64')\n",
        "  X_lower_new = X_lower_new.astype('float64')\n",
        "\n",
        "  Y_numbers_new.iloc[:, 0] = Y_numbers_new.iloc[:, 0].astype('int')\n",
        "  Y_upper_new.iloc[:, 0] = Y_upper_new.iloc[:, 0].astype('int')\n",
        "  Y_lower_new.iloc[:, 0] = Y_lower_new.iloc[:, 0].astype('int')\n",
        "\n",
        "  # We append the _new vectors to the standard vectors\n",
        "  if i == 0: # IS THERE A WAY TO PROGRAM THIS BETTER?\n",
        "    X_numbers = X_numbers_new\n",
        "    X_upper = X_upper_new\n",
        "    X_lower = X_lower_new\n",
        "\n",
        "    Y_numbers = Y_numbers_new\n",
        "    Y_upper = Y_upper_new\n",
        "    Y_lower = Y_lower_new\n",
        "\n",
        "  else:\n",
        "    X_numbers = np.concatenate((X_numbers, X_numbers_new), axis=0)\n",
        "    X_upper = np.concatenate((X_upper, X_upper_new), axis=0)\n",
        "    X_lower = np.concatenate((X_lower, X_lower_new), axis=0)\n",
        "\n",
        "    Y_numbers = pd.concat([Y_numbers, Y_numbers_new], axis=0)\n",
        "    Y_upper = pd.concat([Y_upper, Y_upper_new], axis=0)\n",
        "    Y_lower = pd.concat([Y_lower, Y_lower_new], axis=0)\n",
        "\n",
        "    #Y_numbers = np.concatenate((Y_numbers, np.array(Y_numbers_new)), axis=0)\n",
        "    #Y_upper = np.concatenate((Y_upper, np.array(Y_upper_new)), axis=0)\n",
        "    #Y_lower = np.concatenate((Y_lower, np.array(Y_lower_new)), axis=0)\n",
        "\n",
        "    \n",
        "  #X_numbers.append(X_numbers_new)\n",
        "  #X_upper.append(X_upper_new)\n",
        "  #X_lower.append(X_lower)\n",
        "\n",
        "  #Y_numbers.append(Y_numbers_new)\n",
        "  #Y_upper.append(Y_upper_new)\n",
        "  #Y_lower.append(Y_lower_new)\n",
        "  \n",
        "  i += 1\n",
        "  if i % 10 == 0 or i==len(dfs):\n",
        "    print(str(i)+'/'+str(len(dfs)))\n",
        "\n",
        "  #for row in df[np.logical_and(df.loc[:,'strength'] < 0.5, df.loc[:,'italic'] == 0].iterrows():\n",
        "   # if row.strength < 0.5 and row.italic == 0:\n",
        "\n",
        "#Y_numbers.reset_index(inplace=True)\n",
        "#Y_lower.reset_index(inplace=True)\n",
        "#Y_upper.reset_index(inplace=True)"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/153\n",
            "20/153\n",
            "30/153\n",
            "40/153\n",
            "50/153\n",
            "60/153\n",
            "70/153\n",
            "80/153\n",
            "90/153\n",
            "100/153\n",
            "110/153\n",
            "120/153\n",
            "130/153\n",
            "140/153\n",
            "150/153\n",
            "153/153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyHxV2Dv6brP"
      },
      "source": [
        "#THIS CODE IS NOW RENDERED UNNECESSARY\n",
        "\n",
        "#print('finish0')\n",
        "#X_numbers = np.concatenate(X_numbers, axis=0)\n",
        "##print('finish1')\n",
        "#X_upper = np.concatenate(X_upper, axis=0)\n",
        "#print('finish2')\n",
        "#X_lower = np.concatenate(X_lower, axis=0)\n",
        "#print('finish3')\n",
        "\n",
        "#Y_numbers = np.concatenate(Y_numbers, axis=0)\n",
        "#print('finish4')\n",
        "#Y_upper = np.concatenate(Y_upper, axis=0)\n",
        "#print('finish5')\n",
        "#Y_lower = np.concatenate(Y_lower, axis=0)\n",
        "#print('finish6')"
      ],
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQDDjU6Pc5bn"
      },
      "source": [
        "# We create a custom transform that can escale the images to [-1, 1] the range that's typically used\n",
        "# in GANs. It's the image of the function tanh(), used sometimes as the last activation function\n",
        "# in generators\n",
        "\n",
        "class ToTanh(object):\n",
        "    \"\"\"Convert single-channel images from range [0, 256] to range [-1, 1].\"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        return torch.tensor(pic * 2 - 1, dtype=torch.float)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '()'"
      ],
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LxQhTpj0zh9"
      },
      "source": [
        "# Generation of the transform for the DataLoader\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(0.5, 0.5)\n",
        "  #ToTanh()\n",
        "])"
      ],
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDfRsj_w06rs"
      },
      "source": [
        "#Append the Y columns at the end of the X columns\n",
        "\n",
        "#X_numbers = np.concatenate([X_numbers, Y_numbers], axis=1)\n",
        "#X_upper = np.concatenate([X_upper, Y_upper], axis=1)\n",
        "#X_lower = np.concatenate([])"
      ],
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux0fsk9-7uDw"
      },
      "source": [
        "#We modify the dataset class to fit our needs: deliver an image and a label per example\n",
        "class FontsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_images, Y_labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X_images (np.array): np.array containing all the font images\n",
        "            Y_labels (np.array): np.array containing all the font labels\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.X_images = X_images\n",
        "        #self.X_images.type(torch.FloatTensor)\n",
        "        #for now, we only want to keep the column that indicates the character\n",
        "        self.Y_labels = Y_labels.iloc[:,0]\n",
        "        #we also apply one-hot encoding to the labels.\n",
        "        #USE EMBEDDING FOR BIGGER COLLECTIONS OF CHARACTERS\n",
        "        #self.Y_labels = np.array(pd.get_dummies(Y_labels.astype('str')))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        #img_name = os.path.join(self.root_dir,\n",
        "        #                        self.landmarks_frame.iloc[idx, 0])\n",
        "        #image = io.imread(img_name)\n",
        "        #landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
        "        #landmarks = np.array([landmarks])\n",
        "        #landmarks = landmarks.astype('float').reshape(-1, 2)\n",
        "        #sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        image = self.X_images[idx]\n",
        "        label = self.Y_labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            image.type(torch.FloatTensor)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23jRriua689n"
      },
      "source": [
        "#plt.imshow(numbers_dataset.X_images[0,:,:])"
      ],
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L5eTsTFIEre"
      },
      "source": [
        "#Y_numbers.reset_index(drop=True).head(50)"
      ],
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICcYez71HEF8"
      },
      "source": [
        "#numbers_dataset.Y_labels[0]"
      ],
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgLNDtv0LTOx"
      },
      "source": [
        "#max(numbers_dataset.Y_labels)"
      ],
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gn0g-mgDwMr"
      },
      "source": [
        "#type(numbers_dataset.Y_labels[0])"
      ],
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTCDrE6y2DmZ"
      },
      "source": [
        "# We want the label to go from 0 to label_max. In order to do that,\n",
        "# the following operation needs to be performed:\n",
        "\n",
        "Y_numbers.iloc[:, 0] = Y_numbers.iloc[:, 0] - number_min\n",
        "Y_upper.iloc[:, 0] = Y_upper.iloc[:, 0] - upper_min\n",
        "Y_lower.iloc[:, 0] = Y_lower.iloc[:, 0] - lower_min\n",
        "\n",
        "Y_numbers.reset_index(drop=True, inplace=True)\n",
        "Y_upper.reset_index(drop=True, inplace=True)\n",
        "Y_lower.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbu6Nata_UTw"
      },
      "source": [
        "numbers_dataset = FontsDataset(X_numbers, Y_numbers, transform)\n",
        "upper_dataset = FontsDataset(X_upper, Y_upper, transform)\n",
        "lower_dataset = FontsDataset(X_lower, Y_lower, transform)\n",
        "\n",
        "numbers_loader = DataLoader(numbers_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "upper_loader = DataLoader(upper_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "lower_loader = DataLoader(lower_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
      ],
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og_7jmTAAZl5",
        "outputId": "95c9efd4-33a6-4609-fd21-c04b1cf274ef"
      },
      "source": [
        "numbers_dataset.Y_labels.unique()"
      ],
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 344
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "k2bW-vGkI2FS",
        "outputId": "dbd9b408-f927-4054-af6c-47ba91392c11"
      },
      "source": [
        "plt.imshow(numbers_dataset.X_images[969])"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2acf7d87d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 345
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-QcXSF2Rsqh"
      },
      "source": [
        "#FUNCIÓN PARA PROBAR COSAS\n",
        "# SE IRÁ DEL CÓDIGO EVENTUALMENTE\n",
        "\n",
        "#index = 21\n",
        "\n",
        "#plt.figure()\n",
        "#for i in np.arange(index, index+10):\n",
        "#  plt.imshow(X_numbers[i])\n",
        "#  plt.show()"
      ],
      "execution_count": 346,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqIjRdQSpQJT"
      },
      "source": [
        "#FUNCIÓN PARA PROBAR COSAS\n",
        "# SE IRÁ DEL CÓDIGO EVENTUALMENTE\n",
        "\n",
        "#Esta función en concreto busca todos los caracteres de un tipo en concreto\n",
        "# y los dibuja\n",
        "\n",
        "#char_one = dfs[0].loc[:, 'm_label'] == 57\n",
        "#chars = dfs[0][char_one]\n",
        "#for i in range(chars.shape[0]):\n",
        "#  chars2 = np.array(chars.iloc[i, 12:])\n",
        "#  chars3 = np.reshape(chars2, newshape=(20,20))\n",
        "#  plt.figure()\n",
        "#  plt.imshow(chars3.astype(float))\n",
        "#  plt.show()"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaVXHesWll3X"
      },
      "source": [
        "### Building the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJ5m2_juw0P"
      },
      "source": [
        "#this is the tensor used as 'error', or 'output not expected', as\n",
        "# using None is not possible because it raises some errors\n",
        "error_tensor = -1 * torch.ones(1)\n",
        "error_tensor = error_tensor.to(device)"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn78mMb5lpmN"
      },
      "source": [
        "##Right now, the generator only generates one image per execution,\n",
        "## maybe it should be readapted to generate a BATCH of images\n",
        "## EDIT: readapted!!\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, noise_dim=50, categorical_dim=10, categorical_noise_dim=30, image_size=20):\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "    self.categorical_dim = categorical_dim\n",
        "    self.categorical_noise_dim = categorical_noise_dim\n",
        "\n",
        "    self.image_size = image_size #size of one of the sides of the image (we are working with square images)\n",
        "    self.hidden_channels = 128 #channels of the first convolutional layer\n",
        "    self.output_channels = 1 #the images we work with are black & white\n",
        "\n",
        "    self.hidden_layer = self.hidden_channels * (self.image_size // 4) ** 2\n",
        "\n",
        "    self.linear = nn.Linear(self.noise_dim + self.categorical_dim + self.categorical_noise_dim, self.hidden_layer)\n",
        "    self.bn = nn.BatchNorm1d(self.hidden_layer)\n",
        "    self.bn2 = nn.BatchNorm2d(self.hidden_channels//2)\n",
        "    #self.upscale = F.upsample_bilinear(2)\n",
        "    #self.upscale = nn.PixelShuffle(upscale_factor=2) # Better than F.upsample_bilinear: training GANs means avoiding sparse gradients\n",
        "    self.conv1 = nn.Conv2d(self.hidden_channels, self.hidden_channels//2, (3,3), stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(self.hidden_channels//2, self.output_channels, (3,3), stride=1, padding=1)\n",
        "\n",
        "    #self.conv3 = nn.Conv2d(self.hidden_channels//4, self.hidden_channels//8, (3,3), stride=1, padding=1)\n",
        "\n",
        "\n",
        "  def forward(self, z, cat, z_cat):\n",
        "    if not z_cat == error_tensor:\n",
        "      x = self.bn(F.leaky_relu(self.linear(torch.cat((z, cat, z_cat), axis=1)), negative_slope=LRELU_ALPHA))\n",
        "    else:\n",
        "      #x = self.bn(F.leaky_relu(self.linear(torch.cat((z, cat), axis=1)), negative_slope=LRELU_ALPHA))\n",
        "      x = self.bn(F.relu(self.linear(torch.cat((z, cat), axis=1))))\n",
        "    x = x.view(-1, self.hidden_channels, self.image_size // 4, self.image_size // 4)\n",
        "    x = F.interpolate(x, scale_factor=2)\n",
        "    # ITER 1:  x = self.bn2(F.leaky_relu(self.conv1(x), negative_slope=LRELU_ALPHA))\n",
        "    # ITER 2:  x = self.bn2(F.relu(self.conv1(x)))\n",
        "    # ITER 3:\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.interpolate(x, scale_factor=2)\n",
        "    x = torch.tanh(self.conv2(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGjpOHp1HF8x"
      },
      "source": [
        "class Generator2(nn.Module):\n",
        "  def __init__(self, noise_dim=50, categorical_dim=10, categorical_noise_dim=30, image_size=20):\n",
        "    super(Generator2, self).__init__()\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "    self.categorical_dim = categorical_dim\n",
        "    self.categorical_noise_dim = categorical_noise_dim\n",
        "\n",
        "    self.image_size = image_size #size of one of the sides of the image (we are working with square images)\n",
        "    self.hidden_channels = 128 #channels of the first convolutional layer\n",
        "    self.output_channels = 1 #the images we work with are black & white\n",
        "\n",
        "    self.hidden_layer = self.hidden_channels * (self.image_size // 4) ** 2\n",
        "\n",
        "    self.linear = nn.Linear(self.noise_dim + self.categorical_dim + self.categorical_noise_dim, self.hidden_layer)\n",
        "    self.bn = nn.BatchNorm1d(self.hidden_layer)\n",
        "    self.bn2 = nn.BatchNorm2d(self.hidden_channels//2)\n",
        "    self.bn3 = nn.BatchNorm2d(self.hidden_channels//4)\n",
        "    #self.upscale = F.upsample_bilinear(2)\n",
        "    #self.upscale = nn.PixelShuffle(upscale_factor=2) # Better than F.upsample_bilinear: training GANs means avoiding sparse gradients\n",
        "    self.conv0 = nn.Conv2d(self.hidden_channels, self.hidden_channels//2, (3,3), stride=1, padding=1)\n",
        "    self.conv1 = nn.Conv2d(self.hidden_channels//2, self.hidden_channels//4, (3,3), stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(self.hidden_channels//4, self.output_channels, (3,3), stride=1, padding=1)\n",
        "\n",
        "    #self.conv3 = nn.Conv2d(self.hidden_channels//4, self.hidden_channels//8, (3,3), stride=1, padding=1)\n",
        "\n",
        "\n",
        "  def forward(self, z, cat, z_cat):\n",
        "    if not z_cat == error_tensor:\n",
        "      x = self.bn(F.relu(self.linear(torch.cat((z, cat, z_cat), axis=1))))\n",
        "    else:\n",
        "      #x = self.bn(F.leaky_relu(self.linear(torch.cat((z, cat), axis=1)), negative_slope=LRELU_ALPHA))\n",
        "      x = self.bn(F.relu(self.linear(torch.cat((z, cat), axis=1))))\n",
        "    x = x.view(-1, self.hidden_channels, self.image_size // 4, self.image_size // 4)\n",
        "    x = F.relu(self.conv0(x))\n",
        "    x = self.bn2(F.interpolate(x, scale_factor=2))\n",
        "    # ITER 1:  x = self.bn2(F.leaky_relu(self.conv1(x), negative_slope=LRELU_ALPHA))\n",
        "    # ITER 2:  x = self.bn2(F.relu(self.conv1(x)))\n",
        "    # ITER 3:\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.interpolate(x, scale_factor=2)\n",
        "    x = self.bn3(x)\n",
        "    x = torch.tanh(self.conv2(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqGdW9FrjX9B"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, noise_dim=50, categorical_dim=10, categorical_noise_dim=30, image_size=20):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "    self.categorical_dim = categorical_dim\n",
        "    self.categorical_noise_dim = categorical_noise_dim\n",
        "\n",
        "    self.image_size = image_size #size of one of the sides of the image (we are working with square images)\n",
        "    self.hidden_channels = 128 #channels of the last convolutional layer\n",
        "    self.input_channels = 1 #the images we work with are black & white\n",
        "\n",
        "    self.hidden_layer = self.hidden_channels * (self.image_size // 4) ** 2\n",
        "\n",
        "    self.conv1 = nn.Conv2d(self.input_channels, self.hidden_channels//2, (3,3), stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(self.hidden_channels//2, self.hidden_channels, (3,3), stride=1, padding=1)\n",
        "\n",
        "    self.bn = nn.BatchNorm2d(self.hidden_channels//2)\n",
        "    self.bn2 = nn.BatchNorm2d(self.hidden_channels)\n",
        "    self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    self.true_dense = nn.Linear(self.hidden_layer, 1)\n",
        "    self.cat_dense = nn.Linear(self.hidden_layer, self.categorical_dim)\n",
        "    if categorical_noise_dim > 0:\n",
        "      self.noise_dense = nn.Linear(self.hidden_layer, self.categorical_noise_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn(self.avgpool(F.leaky_relu(self.conv1(x), negative_slope=LRELU_ALPHA)))\n",
        "    x = self.bn2(self.avgpool(F.leaky_relu(self.conv2(x), negative_slope=LRELU_ALPHA)))\n",
        "    x = self.flatten(x)\n",
        "    #true = F.sigmoid(self.true_dense(x))\n",
        "    true = torch.sigmoid(self.true_dense(x))\n",
        "    cat = F.softmax(self.cat_dense(x))\n",
        "    if self.categorical_noise_dim > 0:\n",
        "      cat_noise = self.noise_dense(x)\n",
        "    else:\n",
        "      cat_noise = error_tensor\n",
        "\n",
        "    return true, cat, cat_noise\n"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxtYQTXifZeR"
      },
      "source": [
        "class Discriminator2(nn.Module):\n",
        "  def __init__(self, noise_dim=50, categorical_dim=10, categorical_noise_dim=30, image_size=20):\n",
        "    super(Discriminator2, self).__init__()\n",
        "\n",
        "    self.noise_dim = noise_dim\n",
        "    self.categorical_dim = categorical_dim\n",
        "    self.categorical_noise_dim = categorical_noise_dim\n",
        "\n",
        "    self.image_size = image_size #size of one of the sides of the image (we are working with square images)\n",
        "    self.hidden_channels = 128 #channels of the last convolutional layer\n",
        "    self.input_channels = 1 #the images we work with are black & white\n",
        "\n",
        "    self.hidden_layer = self.hidden_channels * (self.image_size // 4) ** 2\n",
        "\n",
        "    self.conv1 = nn.Conv2d(self.input_channels, self.hidden_channels, (5,5), stride=1, padding=1)\n",
        "    #self.conv2 = nn.Conv2d(self.hidden_channels//2, self.hidden_channels, (3,3), stride=1, padding=1)\n",
        "\n",
        "    self.bn = nn.BatchNorm2d(self.hidden_channels)\n",
        "    #self.bn2 = nn.BatchNorm2d(self.hidden_channels)\n",
        "    self.avgpool = nn.AvgPool2d(kernel_size=4)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dropout = nn.Dropout(p=0.4)\n",
        "    \n",
        "    self.true_dense = nn.Linear(self.hidden_layer, 1)\n",
        "    self.cat_dense = nn.Linear(self.hidden_layer, self.categorical_dim)\n",
        "    if categorical_noise_dim > 0:\n",
        "      self.noise_dense = nn.Linear(self.hidden_layer, self.categorical_noise_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn(self.avgpool(F.leaky_relu(self.conv1(x), negative_slope=LRELU_ALPHA)))\n",
        "    #x = self.dropout(x)\n",
        "    #x = self.bn2(self.avgpool(F.leaky_relu(self.conv2(x), negative_slope=LRELU_ALPHA)))\n",
        "    x = self.flatten(x)\n",
        "    #true = F.sigmoid(self.true_dense(x))\n",
        "    true = torch.sigmoid(self.true_dense(x))\n",
        "    cat = F.softmax(self.cat_dense(x))\n",
        "    if self.categorical_noise_dim > 0:\n",
        "      cat_noise = self.noise_dense(x)\n",
        "    else:\n",
        "      cat_noise = error_tensor\n",
        "\n",
        "    return true, cat, cat_noise\n"
      ],
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw-GQzfVFULs"
      },
      "source": [
        "#Generation of the different datasets that we're going to send to the generator\n",
        "def generate_generator_data(batch_size, noise_dim=50, cat_dim=10, noise_cat_dim=30, same_class=True):\n",
        "\n",
        "  noise = torch.normal(mean=0, std=1, size=(batch_size, noise_dim))\n",
        "\n",
        "  #The categorical variable has to be one-hot encoded. This process does exactly that\n",
        "  cat = torch.zeros(size=(batch_size, cat_dim))\n",
        "  \n",
        "  #Generate all the data from the same class?\n",
        "  #TRUE\n",
        "  if same_class:\n",
        "    cat_result = torch.randint(low=0, high=cat_dim, size=(1,))\n",
        "    for i in range(batch_size):\n",
        "      cat[i, cat_result[0]] = 1\n",
        "  #FALSE\n",
        "  else:\n",
        "    cat_vector = torch.randint(low=0, high=cat_dim, size=(batch_size,))\n",
        "    for i in range(batch_size):\n",
        "      cat[i, cat_vector[i]] = 1\n",
        "\n",
        "  if noise_cat_dim > 0:\n",
        "    noise_cat = torch.normal(mean=0, std=0.2, size=(batch_size, noise_cat_dim))\n",
        "  else:\n",
        "    noise_cat = error_tensor\n",
        "\n",
        "  return noise, cat, noise_cat"
      ],
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_dOcxBKx2yR"
      },
      "source": [
        "# Dummification in torch\n",
        "def torch_dummies(variable, dim):\n",
        "  cat = torch.zeros(size=(len(variable), dim))\n",
        "  for i in range(len(variable)):\n",
        "    cat[i, variable[i]] = 1\n",
        "  return cat"
      ],
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgGk2WhCLwWl"
      },
      "source": [
        "# MODEL SETUP (Gen, Discr, Loss, Optimizer)\n",
        "\n",
        "# After defining the generator and discriminator classes, we define the rest of elements\n",
        "# that are necessary for the training: loss functions and optimizers\n",
        "\n",
        "mse = nn.MSELoss()\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "mse, cross_entropy = mse.to(device), cross_entropy.to(device)\n",
        "\n",
        "# Functions for the calculation of the loss function\n",
        "def rand_ones_like(tensor):\n",
        "  return torch.rand_like(tensor)*0.2 + 0.9\n",
        "\n",
        "def rand_zeros_like(tensor):\n",
        "  return torch.rand_like(tensor)*0.2\n",
        "\n",
        "### Info-GAN ###\n",
        "if GAN_STYLE == 0:\n",
        "\n",
        "  lambda_cat = 1\n",
        "  lambda_con = 0.1\n",
        "\n",
        "  noise_dim = 50\n",
        "  cat_dim = 10\n",
        "  noise_cat_dim = 30\n",
        "\n",
        "  generator = Generator(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "  discriminator = Discriminator(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "\n",
        "  generator = generator.to(device)\n",
        "  discriminator = discriminator.to(device)\n",
        "\n",
        "\n",
        "  # -- Definition for each of the losses\n",
        "  # Generator loss: MSE Loss\n",
        "  def gen_loss(pred):\n",
        "    #print(max(pred))\n",
        "    return mse(pred, rand_ones_like(pred))\n",
        "\n",
        "  # Discriminator loss: two batches of MSE loss: one for each batch (true images and )\n",
        "  def discr_loss(real_pred, fake_pred):\n",
        "    #print(max(real_pred), max(fake_pred))\n",
        "    real_loss = mse(real_pred, rand_ones_like(real_pred))\n",
        "    fake_loss = mse(fake_pred, rand_zeros_like(fake_pred))\n",
        "    return (real_loss + fake_loss) / 2\n",
        "\n",
        "  # Features loss: batch of noise and categories predicted by the discr.\n",
        "  #  noise is continuous (MSE loss) and categories are not (cross entropy loss)\n",
        "  def features_loss(cat_input, cat_pred, noise_input, noise_pred):\n",
        "    return lambda_cat * cross_entropy(cat_input, torch.argmax(cat_pred, axis=1)) + lambda_con * mse(noise_input, noise_pred)\n",
        "\n",
        "  # Optimizer definition\n",
        "  gen_optim = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "  discr_optim = torch.optim.SGD(discriminator.parameters(), lr=0.001)\n",
        "  features_optim = torch.optim.Adam(chain(generator.parameters(), discriminator.parameters()))\n",
        "\n",
        "### AC-GAN ###\n",
        "elif GAN_STYLE == 1:\n",
        "\n",
        "  noise_dim = 50\n",
        "  cat_dim = 10\n",
        "  noise_cat_dim = 0 \n",
        "\n",
        "  generator = Generator2(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "  discriminator = Discriminator2(noise_dim=noise_dim, categorical_dim=cat_dim, categorical_noise_dim=noise_cat_dim)\n",
        "\n",
        "  generator = generator.to(device)\n",
        "  discriminator = discriminator.to(device)\n",
        "\n",
        "  # -- Definition for each of the losses\n",
        "  # Generator loss: MSE Loss\n",
        "  def gen_loss(pred, cat_input, cat_pred):\n",
        "    #return 0.5 * (mse(pred, rand_ones_like(pred)) + cross_entropy(cat_input, torch.argmax(cat_pred, axis=1))/2.5)\n",
        "    #print('gen loss: ', mse(pred, rand_ones_like(pred)))\n",
        "    return (mse(pred, rand_ones_like(pred)))\n",
        "\n",
        "  # Discriminator loss: two batches of MSE loss: one for each batch (true images and )\n",
        "  def discr_loss(real_pred, fake_pred, real_cat_input, real_cat_labels, fake_cat_input, fake_cat_labels, invert=False):\n",
        "    #print(real_cat_input.shape, real_cat_labels.shape)\n",
        "    #print(real_cat_input[0,:], real_cat_labels[0,:])\n",
        "    #real_loss = 0.5 * (mse(real_pred, rand_ones_like(real_pred)) + cross_entropy(real_cat_input, torch.argmax(real_cat_labels, axis=1))/2.5)\n",
        "    #fake_loss = 0.5 * (mse(fake_pred, rand_zeros_like(fake_pred)) + cross_entropy(fake_cat_input, torch.argmax(fake_cat_labels, axis=1))/2.5)\n",
        "    if not invert or not FLIP_LABELS:\n",
        "      real_loss = (mse(real_pred, rand_ones_like(real_pred)))\n",
        "      fake_loss = (mse(fake_pred, rand_zeros_like(fake_pred)))\n",
        "    else:\n",
        "      real_loss = (mse(real_pred, rand_zeros_like(real_pred)))\n",
        "      fake_loss = (mse(fake_pred, rand_ones_like(fake_pred)))\n",
        "    #print('real loss: ', real_loss, '   fake loss: ', fake_loss)\n",
        "\n",
        "    return (real_loss + fake_loss) / 2\n",
        "\n",
        "  # Optimizer definition\n",
        "  gen_optim = torch.optim.Adam(generator.parameters(), lr=LR_G)\n",
        "  #discr_optim = torch.optim.SGD(discriminator.parameters(), lr=LR_D)\n",
        "  discr_optim = torch.optim.Adam(discriminator.parameters(), lr=LR_D)"
      ],
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rifj4u5v3ack"
      },
      "source": [
        "# Noise in images\r\n",
        "def noisy_image(image_batch, epsilon):\r\n",
        "  #In this problem, images are normalised in the [-1, 1] range\r\n",
        "  noise = torch.rand_like(image_batch) * epsilon\r\n",
        "  noisy_image_batch = torch.clamp(image_batch + noise, min=-1, max=1)\r\n",
        "  return noisy_image_batch"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJR7zG0TZCV"
      },
      "source": [
        "# Noise strength decay\r\n",
        "def epsilon_decay(epoch, epsilon_decay_factor, epsilon_decay_exponent):\r\n",
        "  return epsilon_decay_factor / (epoch + 3) ** epsilon_decay_exponent\r\n"
      ],
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6l4bkyuCS1D"
      },
      "source": [
        "# Discriminator label inversion frequency\r\n",
        "# This is done to fool the discriminator and let the generator learn\r\n",
        "def label_inversion(epoch, inversion_factor, inversion_exponent):\r\n",
        "  inversion_weight = 1 / (inversion_factor * epoch + 3) ** inversion_exponent\r\n",
        "  inversion = random.choices([0,1], weights=[1-inversion_weight, inversion_weight])\r\n",
        "  return inversion[0]"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmww8RUaQKim",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "88283aa1-2705-43a6-9469-a634ad97a0ba"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# --------\n",
        "# TRAINING\n",
        "# --------\n",
        "\n",
        "n_epochs = 5000\n",
        "dataloader = numbers_loader\n",
        "inversion_total = 0\n",
        "inversion_all = 0\n",
        "\n",
        "#images that we're going to use to see the performance of the Generator\n",
        "# at different epochs\n",
        "data_show = 36\n",
        "check_noise, check_cat, check_noise_cat = generate_generator_data(data_show, noise_dim=noise_dim, cat_dim=cat_dim, noise_cat_dim=noise_cat_dim, same_class=False)\n",
        "check_noise, check_cat, check_noise_cat = check_noise.to(device), check_cat.to(device), check_noise_cat.to(device)\n",
        "\n",
        "# We set up these variables that are designed for plotting\n",
        "gen_loss_acum = []\n",
        "discr_loss_acum = []\n",
        "features_loss_acum = []\n",
        "\n",
        "# This training is only performed on the numbers dataset\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  gen_loss_epoch = 0\n",
        "  discr_loss_epoch = 0\n",
        "  features_loss_epoch = 0\n",
        "\n",
        "  for batch_idx, (real_img_batch, real_img_label) in enumerate(dataloader):\n",
        "\n",
        "    #print(real_img_label.shape, real_img_label)\n",
        "    real_img_batch = real_img_batch.to(device)\n",
        "    real_img_label = torch_dummies(real_img_label, cat_dim).to(device)\n",
        "    \n",
        "    #real_img_real_img_batch.type(torch.cuda.FloatTensor)\n",
        "\n",
        "    ## ------------------\n",
        "    ## GENERATOR TRAINING\n",
        "    ## ------------------\n",
        "    \n",
        "    gen_optim.zero_grad()\n",
        "    #if GAN_STYLE == 0: #InfoGAN\n",
        "    noise, cat, noise_cat = generate_generator_data(BATCH_SIZE, noise_dim, cat_dim, noise_cat_dim, same_class=False)\n",
        "    noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "    #elif GAN_STYLE == 1: #AC-GAN\n",
        "    img_batch = generator(noise, cat, noise_cat)\n",
        "    img_batch = img_batch.to(device)\n",
        "    #plt.figure()\n",
        "    #ax = plt.subplot(2, 1, 1)\n",
        "    #plt.imshow(img_batch[0][0,:,:].cpu().detach().numpy())\n",
        "    with torch.no_grad():\n",
        "      if NOISE:\n",
        "        img_batch = noisy_image(img_batch, epsilon_decay(epoch, EPSILON, EPSILON_EXPONENT)) ##NOISE\n",
        "    #ax = plt.subplot(2, 1, 2)\n",
        "    #plt.imshow(img_batch[0][0,:,:].cpu().detach().numpy())\n",
        "    #plt.show()\n",
        "    veracity, cat_pred, _ = discriminator(img_batch)\n",
        "    veracity, cat_pred = veracity.to(device), cat_pred.to(device)\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      generator_loss = gen_loss(veracity)\n",
        "    elif GAN_STYLE == 1: #AC-GAN\n",
        "      generator_loss = gen_loss(veracity, cat, cat_pred)\n",
        "    generator_loss.backward()\n",
        "    gen_optim.step()\n",
        "\n",
        "\n",
        "    ## ----------------------\n",
        "    ## DISCRIMINATOR TRAINING\n",
        "    ## ----------------------\n",
        "    if batch_idx % GEN_PER_DISCR == 0:\n",
        "      discr_optim.zero_grad()\n",
        "\n",
        "      # First, we send a batch that has been created from the generator\n",
        "      noise, cat, noise_cat = generate_generator_data(BATCH_SIZE, noise_dim, cat_dim, noise_cat_dim, same_class=False)\n",
        "      noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "      img_batch = generator(noise, cat, noise_cat).detach()\n",
        "      img_batch = img_batch.to(device)\n",
        "      with torch.no_grad():\n",
        "        if NOISE:\n",
        "          img_batch = noisy_image(img_batch, epsilon_decay(epoch, EPSILON, EPSILON_EXPONENT)) ##NOISE\n",
        "      veracity_gen, cat_gen, _ = discriminator(img_batch)\n",
        "\n",
        "      # Then, we send a batch from the real images\n",
        "      real_img_batch = real_img_batch.type(torch.cuda.FloatTensor)\n",
        "      with torch.no_grad():\n",
        "        if NOISE:\n",
        "          real_img_batch = noisy_image(real_img_batch, epsilon_decay(epoch, EPSILON, EPSILON_EXPONENT)) ##NOISE\n",
        "      veracity_real, cat_real_pred, _ = discriminator(real_img_batch)\n",
        "\n",
        "      # We calculate the loss and apply backpropagation\n",
        "      #print(noise_cat.shape, cat_gen.shape)\n",
        "\n",
        "      inversion = label_inversion(epoch, DISCR_LABEL_INVERSION_FACTOR, DISCR_LABEL_INVERSION_PWR)\n",
        "      # variables that track the total inversions that have been performed (bad code)\n",
        "      inversion_total += inversion\n",
        "      inversion_all += 1\n",
        "      \n",
        "      #inversion = False ## COMMENT THIS LINE WHEN YOU WANT TO PRODUCE REAL LOSS LABEL INVERSION TO THE DISCRMINATOR\n",
        "      discriminator_loss = discr_loss(veracity_real, veracity_gen, real_img_label, cat_real_pred, cat, cat_gen, inversion)\n",
        "      discriminator_loss.backward()\n",
        "      discr_optim.step()\n",
        "\n",
        "\n",
        "    ## ----------------\n",
        "    ## FEATURE TRAINING\n",
        "    ## ----------------\n",
        "\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      features_optim.zero_grad()\n",
        "\n",
        "      noise, cat, noise_cat = generate_generator_data(BATCH_SIZE, noise_dim, cat_dim, noise_cat_dim)\n",
        "      noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "      img_batch = generator(noise, cat, noise_cat)\n",
        "      img_batch = img_batch.to(device)\n",
        "      _, cat_pred, noise_cat_pred = discriminator(img_batch)\n",
        "      loss = features_loss(cat, cat_pred, noise_cat, noise_cat_pred)\n",
        "      loss.backward()\n",
        "      features_optim.step()\n",
        "\n",
        "    #print(generator_loss, generator_loss.item(), gen_loss_epoch + generator_loss.item())\n",
        "\n",
        "    gen_loss_epoch = gen_loss_epoch + generator_loss.item()\n",
        "    discr_loss_epoch = discr_loss_epoch + discriminator_loss.item()\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      features_loss_epoch = features_loss_epoch + loss.item()\n",
        "\n",
        "\n",
        "    #END OF BATCH TEXT\n",
        "    if batch_idx % 300 == 0:\n",
        "      print(\n",
        "          f'Train Epoch: {epoch+1}/{n_epochs} [{batch_idx*len(real_img_batch)}/{len(dataloader.dataset)} ({round(100. * batch_idx / len(dataloader), 1)}%)]')\n",
        "      print('discr label inversions:',inversion_total, ' total batches:', inversion_all, ' inversion frequency:', inversion_total/inversion_all)\n",
        "      #print(generator_loss, discriminator_loss, gen_loss_epoch, discr_loss_epoch)\n",
        "\n",
        "  # LOSS PLOT\n",
        "  with torch.no_grad():\n",
        "    gen_loss_epoch = gen_loss_epoch / (batch_idx+1)\n",
        "    discr_loss_epoch = discr_loss_epoch / (batch_idx+1)\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      features_loss_epoch = features_loss_epoch / (batch_idx+1)\n",
        "    #gen_loss_acum.append(gen_loss_epoch.detach())\n",
        "    #discr_loss_acum.append(discr_loss_epoch.detach())\n",
        "    gen_loss_acum.append(gen_loss_epoch)\n",
        "    discr_loss_acum.append(discr_loss_epoch)\n",
        "    if GAN_STYLE == 0: #InfoGAN\n",
        "      #features_loss_acum.append(features_loss_epoch.detach())\n",
        "      features_loss_acum.append(features_loss_epoch)\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  print('GENERATOR loss: ',gen_loss_epoch, '\\nDISCRIMINATOR loss: ', discr_loss_epoch)\n",
        "  if GAN_STYLE == 0: #InfoGAN\n",
        "    plt.figure(figsize=(12,12))\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    plt.plot(gen_loss_acum, 'r')\n",
        "    plt.plot(discr_loss_acum, 'g')\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(features_loss_acum, 'b')\n",
        "    plt.show()\n",
        "  elif GAN_STYLE == 1: #AC_GAN\n",
        "    plt.figure(figsize=(12,6))\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([0,1])\n",
        "    plt.plot(gen_loss_acum, 'r')\n",
        "    plt.plot(discr_loss_acum, 'g')\n",
        "    plt.show()\n",
        "\n",
        "  # SHOW THE GENERATOR'S INNER WORKINGS\n",
        "  #data_show = 6\n",
        "  #noise, cat, noise_cat = generate_generator_data(data_show, noise_cat_dim=0)\n",
        "  #noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "  img_batch = generator(check_noise, check_cat, check_noise_cat)\n",
        "  plt.figure(figsize=(8,8))\n",
        "  for i in range(data_show):\n",
        "    plt.subplot(data_show // 6, 6, i+1)\n",
        "    plt.imshow(img_batch[i,0,:,:].cpu().detach().numpy())\n",
        "  \n",
        "\n",
        "  ##GENERAR EL MISMO RUIDO AL PRINCIPIO DEL ENTRENAMIENTO PARA LOS PLOTS DEL GENERADOR\n",
        "  # ASÍ SE VE LA EVOLUCIÓN DE LAS MIMSAS MUESTRAS DE RUIDO"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-401-9cd8f79255a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#plt.imshow(img_batch[0][0,:,:].cpu().detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mveracity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mveracity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mveracity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mGAN_STYLE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#InfoGAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-372-0bb59e630f2c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#true = F.sigmoid(self.true_dense(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_noise_dim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQNJcmFEMUO"
      },
      "source": [
        "noise, cat, noise_cat = generate_generator_data(20)\n",
        "noise, cat, noise_cat = noise.to(device), cat.to(device), noise_cat.to(device)\n",
        "img_batch = generator(noise, cat, noise_cat)\n",
        "plt.imshow(img_batch[14,0,:,:].cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}