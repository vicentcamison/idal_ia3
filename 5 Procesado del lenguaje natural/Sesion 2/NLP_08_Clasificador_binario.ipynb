{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NLP_08-Clasificador binario.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/5%20Procesado%20del%20lenguaje%20natural/Sesion%202/NLP_08_Clasificador_binario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeWwK0JClvba"
      },
      "source": [
        "# Clasificador binario\n",
        "Vamos a utilizar las librerías `spaCy` y `scikit-learn` para entrenar un clasificador binario sobre un conjunto de tweets en español etiquetados como positivos/negativos (análisis de sentimientos)\n",
        "\n",
        "## Carga y preparación de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfXO1Mlnlvbo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Leemos los datos\n",
        "df = pd.read_csv('tweets_all.csv', index_col=None)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2VcKoeqlvbp"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhV8g_pHlvbq"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbm9Z9jnlvbr"
      },
      "source": [
        "Tenemos 1514 tweets, de los cuales hay 637 positivos y 474 negativos. El resto son neutros o no tienen polaridad clasificada.\n",
        "Vamos a entrenar sólo con los positivos y negativos para utilizar un clasificador binario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou4F-aHtlvbs"
      },
      "source": [
        "df = df[(df['polarity']=='P') | (df['polarity']=='N')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EpA7x2ylvbt"
      },
      "source": [
        "### Análisis exploratorio de los datos (EDA)\n",
        "Estudiamos primero los datos para comprobar su adecuación.  \n",
        "Primero vemos si las clases están balanceadas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ln2NsA_lvbt"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLQTBkvnlvbu"
      },
      "source": [
        "Creamos una nueva columna con la longitud de cada tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MhPuTi-lvbw"
      },
      "source": [
        "df['pre_clean_len'] = df.content.str.len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEnzmhvlvby"
      },
      "source": [
        "Lo representamos como diagrama de cajas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofwAn3xUlvb2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "plt.boxplot(df.pre_clean_len)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBYhHtkvlvb4"
      },
      "source": [
        "## Limpieza de texto\n",
        "Hacemos un pequeño pre-procesado del texto antes de extraer las características:  \n",
        "- Quitamos las menciones y las URL del texto porque no aportan valor para el análisis de sentimientos.\n",
        "- Los hashtag sí que pueden aportar valor así que simplemente quitamos el #.\n",
        "- Quitamos los signos de puntuación y palabras menores de 3 caracteres.\n",
        "- Por último quitamos todos los símbolos de puntuación del texto (que forman parte de un token).\n",
        "- Lematizamos el texto y lo guardamos en otra columna para comparar resultados del clasificador. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2FuNhi3lvb4"
      },
      "source": [
        "import re, string, spacy\n",
        "nlp=spacy.load('es_core_news_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CY4qmiklvb5"
      },
      "source": [
        "#lista de stop-words específicos de nuestro corpus (aproximación)\n",
        "stop_words = ['unos', 'unas', 'algún', 'alguna', 'algunos', 'algunas', 'ese', 'eso', 'así']\n",
        "\n",
        "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
        "\n",
        "def clean_text(text, lemas=False):\n",
        "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos en tokens\n",
        "    y eliminamos signos de puntuación.\n",
        "    Si lemas=True extraemos el lema, si no dejamos en minúsculas solamente.\n",
        "    Como salida volvemos a convertir los tokens en cadena de texto\"\"\"\n",
        "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text) #elimina menciones y URL\n",
        "    tokens = nlp(text)\n",
        "    tokens = [tok.lemma_.lower() if lemas else tok.lower_ for tok in tokens if not tok.is_punct]\n",
        "    filtered_tokens = [pattern2.sub('', tok) for tok in tokens if not (tok in stop_words) and len(tok)>2]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return filtered_text\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDHH5o7ylvb5"
      },
      "source": [
        "Probamos el funcionamiento de estas funciones sobre un tweet de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mbkHDY0lvb6"
      },
      "source": [
        "print('Original:\\n',df.content[702])\n",
        "print('\\nLimpiado:\\n',clean_text(df.content[702]))\n",
        "print('\\nLematizado:\\n',clean_text(df.content[702], lemas=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZxQnWSKlvb6"
      },
      "source": [
        "Aplicamos limpieza a todos los tweets del DataFrame y creamos columna nueva con los lemas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-qAZbglvb6"
      },
      "source": [
        "df[\"limpio\"]=df['content'].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn_v9AGRlvb7"
      },
      "source": [
        "#Quitamos tweets vacíos después de la limpieza\n",
        "df=df[df.limpio!='']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0UefhgYlvb7"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBae_lrflvb7"
      },
      "source": [
        "### Ejercicio 1\n",
        "Crea una nueva columna \"lemas\" con el texto lematizado de cada tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCDGLLLblvb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL-d37Qelvb8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crSGp5Pglvb8"
      },
      "source": [
        "#Contamos el nº de palabras por tweet\n",
        "df['words'] = [len(t.split(' ')) for t in df.limpio]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6TZa99Dlvb8"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4Qz1Sdlvb9"
      },
      "source": [
        "## Entrenamiento del Clasificador\n",
        "Vamos a usar la librería scikit-learn para aplicar un clasificador binario sobre la polaridad usando una extracción de características Bag-of-Words (BoW)\n",
        "\n",
        "Primero dividimos en conjunto de entrenamiento y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qITo2k0lvb9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "# Asignamos un 70% a training y un 30% a test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['limpio'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz8BI_Iblvb9"
      },
      "source": [
        "print('Primera entrada de train:\\n', X_train.iloc[0])\n",
        "print('Polaridad:', y_train.iloc[0])\n",
        "print('\\nX_train shape:', X_train.shape)\n",
        "print('\\nX_test shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg2x6beslvb_"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J_3BthYlvb_"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX8pVl4zlvcA"
      },
      "source": [
        "X_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oLZKq5VlvcB"
      },
      "source": [
        "type(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD_klIculvcB"
      },
      "source": [
        "y_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwzUOyutlvcC"
      },
      "source": [
        "### Extracción de características BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOBUVFV2lvcC"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# aprendemos el modelo CountVectorizer sobre el conjunto de train\n",
        "vect = CountVectorizer()\n",
        "\n",
        "X_train_vectorized = vect.fit_transform(X_train)\n",
        "X_train_vectorized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggKJyLYVlvcD"
      },
      "source": [
        "Vemos el número de términos distintos que tiene el diccionario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpVv1_d-lvcE"
      },
      "source": [
        "len(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OTMDscDlvcF"
      },
      "source": [
        "np.random.choice(vect.get_feature_names(), 5, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkErTG2wlvcF"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "Vamos a probar un clasificador Logistic Regression de scikit-learn para entrenar nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpEuLf9olvcG"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "modelLR = LogisticRegression(solver='liblinear')\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelLR.fit(X_train_vectorized, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotl5q1rlvcH"
      },
      "source": [
        "### Verificación del modelo\n",
        "Para ver el rendimiento del modelo usamos el conjunto de test. Primero transformamos el conjunto de test a su matriz BoW mediante el vectorizador aprendido en TRAIN y aplicamos el modelo entrenado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PDHAE5dlvcH"
      },
      "source": [
        "# Predecimos sobre el conjunto de test\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "X_test_vectorized.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzSmj-NlvcH"
      },
      "source": [
        "prediccion = modelLR.predict(X_test_vectorized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QOrr6CrlvcI"
      },
      "source": [
        "prediccion.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0pFlWkjlvcI"
      },
      "source": [
        "Vemos el resultado de la predicción y calculamos su precisión con distintas métricas.  \n",
        "Ejemplo de predicción de algunas muestras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM0FS9IilvcI"
      },
      "source": [
        "pd.DataFrame({\n",
        "    'texto':X_test,\n",
        "    'polaridad':y_test,\n",
        "    'predicción':prediccion\n",
        "}).sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jneo5ZLElvcI"
      },
      "source": [
        "### Exactitud del modelo\n",
        "(Núm. predicciones correctas / Total de muestras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yuYOOHlvcJ"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Exactitud: ', accuracy_score(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaxcgobalvcJ"
      },
      "source": [
        "Matriz de confusión (predicción -columnas- frente a etiquetas reales -filas-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoOdt6J1lvcJ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm=confusion_matrix(y_test, prediccion)\n",
        "pd.DataFrame(cm, index=('N_true','P_true'), columns=('N_pred','P_pred'))\n",
        "#filas: True, columnas: Prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUrCPA5ylvcK"
      },
      "source": [
        "Podemos ver un informe más completo del clasificador con la métrica `classification_report`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUpRfNk-lvcK"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi64HUbylvcL"
      },
      "source": [
        "`precision` es la precisión: TP/(TP+FP) (positivos reales sobre el total de positivos detectados)  \n",
        "`recall` es la sensibilidad: TP/(TP+FN) (positivos detectados sobre el total de positivos reales) cantidad de positivos que son detectados  \n",
        "`support` indica el número de muestras en cada clase en el conjunto de test (suma por filas en la matriz de confusión)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4hFrU-elvcL"
      },
      "source": [
        "Área bajo la curva ROC:  \n",
        "Para calcular el área bajo la curva ROC (AUC) es necesario obtener la probabilidad de salida del clasificador con `predict_proba`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KCkqEGmlvcL"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "prediccion_prob = modelLR.predict_proba(vect.transform(X_test))\n",
        "#la primera columna corresponde a la etiqueta 'N'\n",
        "#Es necesario convertir los True Labels a un array lógico (1 para etiqueta N)\n",
        "roc_auc_score((y_test=='N'), prediccion_prob[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYPIEFQElvcM"
      },
      "source": [
        "### Veamos qué palabras son las más relevantes en el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIkch8AUlvcM"
      },
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoO2gG_RlvcM"
      },
      "source": [
        "### Optimización del código\n",
        "Combinamos la extracción de características y clasificación en un `pipeline` de scikit-learn (https://scikit-learn.org/stable/modules/compose.html#)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxruyadnlvcN"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vect = CountVectorizer()\n",
        "modelLR = LogisticRegression(solver='liblinear')\n",
        "\n",
        "modelo = make_pipeline(vect, modelLR)\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelo.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDmlS4lvlvcN"
      },
      "source": [
        "Podemos acceder a cada etapa del pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsu3wcX3lvcN"
      },
      "source": [
        "modelo.steps[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OaA-rFllvcN"
      },
      "source": [
        "len(modelo['countvectorizer'].get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rosrh9OlvcO"
      },
      "source": [
        "# Predecimos sobre el conjunto de test\n",
        "prediccion = modelo.predict(X_test)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHNXVzCklvcO"
      },
      "source": [
        "## Otros modelos\n",
        "Probamos con los modelos Naïve Bayes y un SVM lineal para ver si mejora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwax1XFnlvcO"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "modelNB = MultinomialNB()\n",
        "\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelNB.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predecimos sobre el conjunto de test\n",
        "prediccion = modelNB.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asXxETMFlvcO"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "modelSVM = SGDClassifier(loss='hinge', max_iter=10000, tol=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOKh0z94lvcP"
      },
      "source": [
        "### Ejercicio 2\n",
        "Entrena el modelo de clasificador SVM y comprueba su rendimiento en test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS4vKwnalvcP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1oGRkaElvcP"
      },
      "source": [
        "## Modelo con vectores TF-IDF\n",
        "Cambiamos el vectorizador para ver si hay mejoría"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERIXeH7zlvcP"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer()\n",
        "\n",
        "modelo = make_pipeline(vect, modelLR)\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelo.fit(X_train, y_train)\n",
        "\n",
        "# Predecimos sobre el conjunto de test\n",
        "prediccion = modelo.predict(X_test)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlE288UplvcQ"
      },
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(modelo['tfidfvectorizer'].get_feature_names())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelo['logisticregression'].coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpfLiBnSlvcQ"
      },
      "source": [
        "### Otros modelos con TF-IDF\n",
        "### Ejercicio 3\n",
        "Repete el entretenamiento sobre la matriz TF-IDF con los modelos NB y SVM para ver si hay alguna mejoría"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiyyIEyQlvcR"
      },
      "source": [
        "#modelo NB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF8AHS2AlvcS"
      },
      "source": [
        "#modelo SVM\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI0OMsDSlvcS"
      },
      "source": [
        "## Modelos con texto lematizado\n",
        "Repetimos con el texto lematizado para ver si hay alguna mejoría en el rendimiento del clasificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VhoqUphlvcS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "# Asignamos un 70% a training y un 30% a test\n",
        "X_train_lema, X_test_lema, y_train, y_test = train_test_split(df['lemas'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wggcI_sAlvcT"
      },
      "source": [
        "X_train_lema.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec_ApfBwlvcU"
      },
      "source": [
        "### Modelos BoW con texto lematizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA7xChaPlvcU"
      },
      "source": [
        "#vectorizamos\n",
        "vect = CountVectorizer()\n",
        "\n",
        "X_train_vectorized = vect.fit_transform(X_train_lema)\n",
        "X_test_vectorized = vect.transform(X_test_lema)\n",
        "X_train_vectorized.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbVjGZOPlvcU"
      },
      "source": [
        "#Modelo BoW-LR\n",
        "modelLR.fit(X_train_vectorized, y_train)\n",
        "prediccion = modelLR.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWiOVROalvcV"
      },
      "source": [
        "#Modelo BoW-NB\n",
        "modelNB.fit(X_train_vectorized, y_train)\n",
        "prediccion = modelNB.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbRzDna-lvcV"
      },
      "source": [
        "#Modelo BoW-SVM\n",
        "modelSVM.fit(X_train_vectorized, y_train)\n",
        "prediccion = modelSVM.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QNBcNZslvcV"
      },
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUjwPHOLlvcW"
      },
      "source": [
        "### Modelos TF-IDF con texto lematizado\n",
        "### Ejercicio 4\n",
        "Calcula las matrices de características para el conjunto de textos lematizado y aplica los clasificadores LR, NB y SVM para comparar sus resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa2asgHLlvcX"
      },
      "source": [
        "#vectorizamos\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZADjQ4rglvcX"
      },
      "source": [
        "#Modelo BoW-LR\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM5vwSTSlvcX"
      },
      "source": [
        "#Modelo BoW-NB\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4jSQ1S3lvcX"
      },
      "source": [
        "#Modelo BoW-SVM\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FvWvQXAlvcY"
      },
      "source": [
        "Interpretamos el modelo LR para ver la importancia de los términos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyouCugHlvcY"
      },
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bzcq32XlvcY"
      },
      "source": [
        "## Modelos n-gramas\n",
        "Podemos probar a utilizar las características de unigramas y bigramas como entradas al modelo de clasificador a ver si aumenta el rendimiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EurgnhANlvcY"
      },
      "source": [
        "#vectorizamos\n",
        "vect = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
        "\n",
        "X_train_vectorized = vect.fit_transform(X_train_lema)\n",
        "X_test_vectorized = vect.transform(X_test_lema)\n",
        "X_train_vectorized.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psc3s9-slvcZ"
      },
      "source": [
        "np.random.choice(vect.get_feature_names(), 5, replace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dyfj44olvcZ"
      },
      "source": [
        "#Entrenamos los 3 clasificadores con las características BoW-bigramas\n",
        "modelos = [('Logistic Regression', modelLR),\n",
        "           ('Naive Bayes', modelNB),\n",
        "           ('Linear SVM', modelSVM)]\n",
        "for m, clf in modelos:\n",
        "    clf.fit(X_train_vectorized, y_train)\n",
        "    prediccion = clf.predict(X_test_vectorized)\n",
        "    print(f'Modelo {m}: {accuracy_score(y_test, prediccion):.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKfxnCHZlvcZ"
      },
      "source": [
        "Faltaría probar otras combinaciones:  \n",
        "- TF-IDF con bigramas\n",
        "- Bigramas con texto sin lematizar\n",
        "- Reducción del vocabulario con `min_df` y `max_df`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67b4ECW4lvcZ"
      },
      "source": [
        "#Quitamos palabras presentes en más del 10% de documentos\n",
        "vect = CountVectorizer(max_df=0.1)\n",
        "X_train_vectorized = vect.fit_transform(X_train_lema)\n",
        "X_test_vectorized = vect.transform(X_test_lema)\n",
        "print(len(vect.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7A_UWPplvca"
      },
      "source": [
        "vect.stop_words_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu0hJNGPlvca"
      },
      "source": [
        "#Entrenamos los 3 clasificadores con las características BoW quitando stop-words\n",
        "modelos = [('Logistic Regression', modelLR),\n",
        "           ('Naive Bayes', modelNB),\n",
        "           ('Linear SVM', modelSVM)]\n",
        "for m, clf in modelos:\n",
        "    clf.fit(X_train_vectorized, y_train)\n",
        "    prediccion = clf.predict(X_test_vectorized)\n",
        "    print(f'Modelo {m}: {accuracy_score(y_test, prediccion):.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0poICknlvcb"
      },
      "source": [
        "### Modelo sin preprocesado\n",
        "Por comparar, probamos un modelo Bow-LR sin realizar ningún tipo de limpieza y pre-procesado en el texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED3x4gi7lvcc"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO42pCLulvcd"
      },
      "source": [
        "#vectorizamos\n",
        "vect = CountVectorizer()\n",
        "\n",
        "X_train_vectorized = vect.fit_transform(X_train)\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "X_train_vectorized.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXSubO9lvce"
      },
      "source": [
        "#Modelo BoW-LR\n",
        "modelLR.fit(X_train_vectorized, y_train)\n",
        "prediccion = modelLR.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgBibiTlvcf"
      },
      "source": [
        "Baja la exactitud del modelo de un 76% a un 72%. Vemos ahora cuáles son los términos más importantes en este caso:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clwZpeMGlvcf"
      },
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIGm9bFdlvcf"
      },
      "source": [
        "## Modelo con word embeddings\n",
        "Ahora vamos a usar como espacio de características los *word vectors* de las palabras de nuestro corpus.  \n",
        "Como cada palabra tiene un vector de longitud fija, tenemos que obtener un único vector como promedio de todas las palabras del tweet.  \n",
        "En spaCy, el vector de cada palabra es el atributo `vector`.  \n",
        "El atributo `vector` del objeto `Doc` del texto procesado en spaCy contiene el vector promedio de todos los tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLr83bEblvcf"
      },
      "source": [
        "Vemos el tamaño del vector del modelo `Spacy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBeTDCCIlvcf"
      },
      "source": [
        "nlp.vocab.vectors_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEp25lYWlvcg"
      },
      "source": [
        "Es el tamaño del vector de cada token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpjuyoqIlvcg"
      },
      "source": [
        "doc=nlp(df.content[1])\n",
        "doc[1].vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pucSGv_Dlvcg"
      },
      "source": [
        "Que coincide con el tamaño del vector del documento entero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UXcgwOFlvcg"
      },
      "source": [
        "doc.vector.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONZERMkSlvch"
      },
      "source": [
        "Este vector corresponde al promedio de los vectores de todos los tokens del documento que tienen un vector definido en `spaCy`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWt7I1Jclvch"
      },
      "source": [
        "#Cada vector tiene un tamaño de 50, por tanto hay que crear una matriz de\n",
        "#tamaño (nº documentos,50) para guardar el promedio de los vectores de cada tweet\n",
        "#y guardar en cada fila el correspondiente vector promedio\n",
        "word_embeddings=np.zeros((len(df), nlp.vocab.vectors_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS_xxjCflvci"
      },
      "source": [
        "word_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrAYZAS_lvci"
      },
      "source": [
        "#Spacy ya calcula el promedio de los vectores de un documento en Doc.vector\n",
        "vectors = [nlp(tweet).vector for tweet in df.limpio]\n",
        "for i,vector in enumerate(vectors):\n",
        "    word_embeddings[i,:]=vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3kM9H9lvci"
      },
      "source": [
        "type(word_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALoaWzztlvcj"
      },
      "source": [
        "Generamos los conjuntos de entrenamiento con word embeddings de cada tweet y volvemos a aplicar los mismos clasificadores de antes.\n",
        "\n",
        "### Ejercicio 5\n",
        "Divide las entradas y salidas del modelo en entrenamiento y test respetando la misma división que hemos empleado hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhKKrj7olvcj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi_hZS_hlvcj"
      },
      "source": [
        "Aplicamos un clasificador a esta matriz de características. En este caso la matriz conviene valores decimales, por lo que el clasificador `MultinomialNB` se tiene que sustituir por un `GaussianNB` para usar un modelo Naïve Bayes, pero también podemos probar otros modelos más complejos (p. ej. un SVM con un kernel RFB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvOSB0Ylvcj"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10s4ptMAlvck"
      },
      "source": [
        "#entrenamos clasificadores con modelos word embeddings\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "modelos = [('Logistic Regression', modelLR),\n",
        "           ('Naive Bayes', GaussianNB()),\n",
        "           ('Linear SVM', modelSVM),\n",
        "           ('RFB SVM', SVC(gamma='scale', C=2))]\n",
        "\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con características Word Embeddings promediados'.format(m))\n",
        "    #entrenamos sobre train\n",
        "    clf.fit(X_train, y_train)\n",
        "    # Predecimos sobre el conjunto de test\n",
        "    prediccion = clf.predict(X_test)\n",
        "    print(classification_report(y_test, prediccion))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfI3P16llvck"
      },
      "source": [
        "Los modelos con word embedding promediado para todo el tweet funcionan un poco peor que modelos más simples (BoW, TF-IDF). Para usar word embeddings conviene irse a un modelo de aprendizaje profundo (por ejemplo un modelo CNN o un modelo secuencial con LSTM), para lo que es necesario entrenar con un conjunto de datos mucho mayor."
      ]
    }
  ]
}