{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NLP_07-extracción de características Big Data.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/5%20Procesado%20del%20lenguaje%20natural/Sesion%202/NLP_07_extraccio%CC%81n_de_caracteri%CC%81sticas_Big_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "retired-velvet"
      },
      "source": [
        "# Extracción de características en grandes volúmenes de datos\n",
        "Para extraer las características de un corpus muy grande de textos, conviene procesarlo mediante objetos *stream*. Así no se carga todo el corpus en memoria sino que se analiza documento a documento."
      ],
      "id": "retired-velvet"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "infinite-cattle"
      },
      "source": [
        "## Conjunto de textos de \"mundocine\"\n",
        "Vamos a utilizar un conjunto de datos formado por una serie de críticas de películas de cine, almacenadas en formato XML en el directorio `'criticas'` (una crítica por archivo). Hemos preparado una función de tipo `generator` que procesa el directorio donde están los archivos de las críticas y devuelve por cada archivo XML una tupla con 4 valores:\n",
        " - Nombre de la película (*string*)\n",
        " - Resumen breve de la crítica (*string*)\n",
        " - Texto de la crítica (*string*)\n",
        " - Valoración de la película (*int* de 1 a 5)"
      ],
      "id": "infinite-cattle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "round-imperial"
      },
      "source": [
        "import os, re\n",
        "from xml.dom.minidom import parseString\n",
        "\n",
        "def parse_folder(path):\n",
        "    \"\"\"generator that reads the contents of XML files in a folder\n",
        "    Returns the <body> of the <review> in each XML file.\n",
        "    XML files encoded as 'latin-1'\"\"\"\n",
        "    for file in sorted([f for f in os.listdir(path) if f.endswith('.xml')],\n",
        "                        key=lambda x: int(re.match(r'\\d+',x).group())):\n",
        "        with open(os.path.join(path, file), encoding='latin-1') as f:\n",
        "            doc=parseString(f.read())\n",
        "\n",
        "            titulo = doc.documentElement.attributes[\"title\"].value\n",
        "\n",
        "            btxt = \"\"\n",
        "            review_bod = doc.getElementsByTagName(\"body\")\n",
        "            if len(review_bod) > 0:\n",
        "                for node in review_bod[0].childNodes:\n",
        "                    if node.nodeType == node.TEXT_NODE:\n",
        "                        btxt += node.data + \" \"\n",
        "\n",
        "            rtxt = \"\"\n",
        "            review_summ = doc.getElementsByTagName(\"summary\")\n",
        "            if len(review_summ) > 0:\n",
        "                for node in review_summ[0].childNodes:\n",
        "                    if node.nodeType == node.TEXT_NODE:\n",
        "                        rtxt += node.data + \" \"\n",
        "                        \n",
        "            rank = int(doc.documentElement.attributes[\"rank\"].value)\n",
        "            \n",
        "            yield titulo, rtxt, btxt, rank\n"
      ],
      "id": "round-imperial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diverse-fairy"
      },
      "source": [
        "### Ejercicio 1\n",
        "Carga la primera crítica del directorio usando el método `next` sobre la función `parse_folder` en el objeto `critica`. Muestra sus 4 valores."
      ],
      "id": "diverse-fairy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "democratic-topic"
      },
      "source": [
        ""
      ],
      "id": "democratic-topic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "characteristic-converter"
      },
      "source": [
        "## Extracción de características básicas\n",
        "Utilizando la librería `textaCy` procesamos cada documento y generamos una serie de estadísticos. \\\n",
        "Guardaremos los resultados en un objeto `DataFrame` de Pandas.\\\n",
        "Como característica de cada crítica vamos a extraer:\n",
        "- Título de la película\n",
        "- Longitud (en caracteres) del resumen\n",
        "- Longitud (en caracteres) del texto de la crítica\n",
        "- Puntuación de la crítica\n",
        "\n",
        "### Ejercicio 2\n",
        "Completa el código siguiente para generar el `DataFrame`"
      ],
      "id": "characteristic-converter"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adapted-bacteria"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#creamos una lista en blanco\n",
        "datos = \n",
        "\n",
        "#recorremos las críticas y calculamos sus métricas\n",
        "for c in parse_folder(\"criticas\"):\n",
        "    datos.append({\n",
        "        'título': ___,\n",
        "        'LongResumen': ___,\n",
        "        'LongCritica': ___,\n",
        "        'puntuación': ___\n",
        "    })\n",
        "\n",
        "resumen = pd.DataFrame(datos)"
      ],
      "id": "adapted-bacteria",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recognized-paraguay"
      },
      "source": [
        "resumen"
      ],
      "id": "recognized-paraguay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bored-acting"
      },
      "source": [
        "## Limpieza de texto\n",
        "Para poder extraer las características vectoriales del corpus es conveniente realizar primero una limpieza y pre-procedado de cada documento.\\\n",
        "Vamos a suponer que queremos preparar este conjunto de textos para entrenar un modelo que prediga la puntuación de cada crítica a partir del texto de la crítica.\\\n",
        "Realizaremos el siguiente procesado:\n",
        "- Introducir un espacio después de determinados signos de puntuación (\".\", \"?\") para que el tokenizado sea correcto\n",
        "- Separar el texto en *tokens*\n",
        "- Eliminar los *tokens* de tipo *stop-word*, signos de puntuación o espacios o de longitud 1\n",
        "- Convertir las entidades de tipo `PER` al token *persona*\n",
        "- Lematizar el texto y pasarlo a minúsculas"
      ],
      "id": "bored-acting"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surface-traveler"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "def normaliza(texto):\n",
        "    #separamos después de ciertos signos de puntuación\n",
        "    texto = re.sub(r\"([\\.\\?])\", r\"\\1 \", texto)\n",
        "    doc = nlp(texto)\n",
        "    tokens = [t for t in doc if not t.is_punct and not t.is_stop and not t.is_space and len(t.text)>1]\n",
        "    palabras = []\n",
        "    for t in tokens:\n",
        "        if t.ent_iob_=='B' and t.ent_type_=='PER':\n",
        "            palabras.append('persona')\n",
        "        elif t.ent_iob_=='I' and t.ent_type_=='PER':\n",
        "            continue\n",
        "        else:\n",
        "            palabras.append(t.lemma_.lower())\n",
        "    salida = ' '.join(palabras)\n",
        "    \n",
        "    return salida"
      ],
      "id": "surface-traveler",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced-recommendation"
      },
      "source": [
        "### Ejercicio 3\n",
        "Comprueba su funcionamiento en la crítica previamente descargada (variable `critica`)"
      ],
      "id": "enhanced-recommendation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aging-training"
      },
      "source": [
        ""
      ],
      "id": "aging-training",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visible-detective"
      },
      "source": [
        "### Análisis morfológico\n",
        "En una crítica tiene mucha importancia los adjetivos utilizados.\\\n",
        "Crea una función para filtrar sólo los adjetivos utilizados en cada crítica (utiliza el lema de cada adjetivo)."
      ],
      "id": "visible-detective"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "under-elimination"
      },
      "source": [
        "def extraer_adj(texto):\n",
        "    texto = re.sub(r\"([\\.\\?])\", r\"\\1 \", texto)\n",
        "    doc = nlp(texto)\n",
        "    tokens = [t.lemma_ for t in doc if t.pos_==\"ADJ\"]\n",
        "    \n",
        "    return ' '.join(tokens)"
      ],
      "id": "under-elimination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assigned-forge"
      },
      "source": [
        "Comprobamos su funcionamiento en la crítica previamente descargada (variable `critica`)"
      ],
      "id": "assigned-forge"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dramatic-colleague"
      },
      "source": [
        "extraer_adj(critica[2])"
      ],
      "id": "dramatic-colleague",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "indian-whale"
      },
      "source": [
        "## Extracción de características *sparse* con `scikit-learn`\n",
        "Vamos a calculas las matrices de características *bag-of-words* y *tfidf* del conjunto de textos anterior.\\\n",
        "Vamos a usar la librería `scikit-learn` para vectorizar los documentos."
      ],
      "id": "indian-whale"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "express-explorer"
      },
      "source": [
        "#Para no tener que cargar todas las críticas en memoria,\n",
        "#creamos un generador que devuelve iterativamente el\n",
        "#texto procesado de cada crítica\n",
        "\n",
        "def generaCritica(criticas):\n",
        "    \"\"\"Función de tipo generator que devuelve el\n",
        "    texto normalizado de cada crítica.\n",
        "    Entrada:\n",
        "    criticas: objeto 'parse_folder' que itera\n",
        "    sobre el directio de las críticas\n",
        "    Salida:\n",
        "    texto normalizado de cada crítica\"\"\"\n",
        "    for c in criticas:\n",
        "        yield normaliza(c[2])"
      ],
      "id": "express-explorer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "typical-polymer"
      },
      "source": [
        "Comprobamos su funcionamiento generando el texto normalizado de la primera crítica"
      ],
      "id": "typical-polymer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sized-means"
      },
      "source": [
        "next(generaCritica(parse_folder(\"criticas\")))"
      ],
      "id": "sized-means",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surrounded-headquarters"
      },
      "source": [
        "Vectorizamos todo el conjunto de datos usando las funciones de `scikit-learn`.\\\n",
        "Estas funciones admiten un objeto `generator` como argumento de entrada."
      ],
      "id": "surrounded-headquarters"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "limited-bloom"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer()\n",
        "\n",
        "criticas = generaCritica(parse_folder(\"criticas\"))\n",
        "BoW_criticas = vect.fit_transform(criticas)\n",
        "BoW_criticas"
      ],
      "id": "limited-bloom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pleased-number"
      },
      "source": [
        "### Ejercicio 4\n",
        "Genera distintas variantes de matrices de características para el conjunto de las críticas. Prueba con:\n",
        "- Matriz TF-IDF\n",
        "- Matriz BoW con unigramas y bigramas\n",
        "- Matriz TF-IDF eliminando las palabras menos frecuentes y las más frecuentes (mínimo de 2 y máximo de 5 documentos)\n",
        "- Muestra cuáles son las palabras más frecuentes eliminadas"
      ],
      "id": "pleased-number"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "russian-twenty"
      },
      "source": [
        "#matriz TF-IDF\n"
      ],
      "id": "russian-twenty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "innocent-virtue"
      },
      "source": [
        "#BoW de unigramas y bigramas\n"
      ],
      "id": "innocent-virtue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pending-samba"
      },
      "source": [
        "#Matriz TF-IDF eliminando las palabras menos frecuentes y las más frecuentes (mínimo de 2 y máximo de 5 documentos)\n"
      ],
      "id": "pending-samba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "certain-skating"
      },
      "source": [
        "#Palabras más frecuencias eliminadas (atributo 'stop_words_' del vectorizador)\n"
      ],
      "id": "certain-skating",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "occasional-contractor"
      },
      "source": [
        "## Extracción de Word embeddings\n",
        "Ahora vamos a calcular los *word vectors* de las palabras de nuestro conjunto de datos, usando la clase `word2vec` de la librería `gensim`.\\\n",
        "Esta librería acepta como argumento de entrada un objeto `iterador` que generará el texto pre-procesado de la siguiente crítica en la secuencia.\\\n",
        "Vamos a usar las funciones de pre-procesado de la librería `gensim`.\\\n",
        "Primero definimos un objeto de tipo `iterator` para recorrer las críticas. Se diferencia de un simple `generator` en que se puede reiniciar la generación de la secuencia (necesario para el modelo `word2vec`)"
      ],
      "id": "occasional-contractor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amber-greeting"
      },
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "        \n",
        "class PreprocesaCriticas(object):\n",
        "    \"\"\"Pre-procesa el corpus de críticas con la función 'simple_preprocess'\n",
        "    de la librería gensim\n",
        "    Entrada: directorio de críticas\n",
        "    Salida: iterador sobre las críticas (como lista de tokens)\"\"\"\n",
        "    def __init__(self, dirname):\n",
        "        self.dirname = dirname\n",
        " \n",
        "    def __iter__(self):\n",
        "        for c in parse_folder(self.dirname):\n",
        "            yield normaliza(c[2]).split(' ')"
      ],
      "id": "amber-greeting",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "measured-surrey"
      },
      "source": [
        "#instanciamos un objeto para nuestro directorio\n",
        "criticas = PreprocesaCriticas(\"criticas\")"
      ],
      "id": "measured-surrey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rational-carrier"
      },
      "source": [
        "Para probar su funcionamiento con la primera crítica lo convertimos en iterable y usamos el método `next`"
      ],
      "id": "rational-carrier"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hispanic-cholesterol"
      },
      "source": [
        "criticas_iter = iter(criticas)\n",
        "print(next(criticas_iter))"
      ],
      "id": "hispanic-cholesterol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fitted-psychiatry"
      },
      "source": [
        "Al contrario que el objeto generado con la función `generaCriticas` el objeto de `PreprocesaCriticas` se reinicia cada vez que lo convertimos en *iterable*:"
      ],
      "id": "fitted-psychiatry"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minimal-outdoors"
      },
      "source": [
        "Calculamos los vectores de palabras de todo el corpus con el modelo `word2vec` que acepta como argumento de entrada un objeto de tipo `iterator` como el creado"
      ],
      "id": "minimal-outdoors"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raised-joseph"
      },
      "source": [
        "#Cálculo de los vectores de palabras\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(criticas, #iterador con los documentos\n",
        "                               size=10,          #tamaño del vector\n",
        "                               window=5,         #nº de términos adyacentes que usamos para el cálculo\n",
        "                               min_count=5,      #nº mínimo de apariciones del término para contarlo\n",
        "                               iter=100\n",
        "                              )\n",
        "\n",
        "#una vez entrenado el modelo nos quedamos con los vectores calculados\n",
        "#si no se van a actualizar los vectores con nuevos documentos\n",
        "model = model.wv\n",
        "len(model.vocab)"
      ],
      "id": "raised-joseph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surface-stake"
      },
      "source": [
        "Seleccinamos aleatoriamente 25 palabras del conjunto calculado"
      ],
      "id": "surface-stake"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "optimum-empty"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "palabras_sample = np.random.choice(model.index2word, 25, replace=False)"
      ],
      "id": "optimum-empty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "filled-multiple"
      },
      "source": [
        "palabras_sample"
      ],
      "id": "filled-multiple",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hydraulic-avatar"
      },
      "source": [
        "### Ejercicio 5\n",
        "Comprueba cómo funciona el modelo buscando las palabras más similares semánticamente a \"trama\", \"peli\" y \"película\""
      ],
      "id": "hydraulic-avatar"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "other-netscape"
      },
      "source": [
        ""
      ],
      "id": "other-netscape",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worst-parliament"
      },
      "source": [
        "## Extracción de características *sparse* con `gensim`\n",
        "Vamos a generar las matrices TF-IDF y BoW del corpus con la librería Gensim mediante *data streming* para no tener que cargar en memoria los textos."
      ],
      "id": "worst-parliament"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "macro-guidance"
      },
      "source": [
        ""
      ],
      "id": "macro-guidance",
      "execution_count": null,
      "outputs": []
    }
  ]
}