{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_06b-Extracción características BoW.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/5%20Procesado%20del%20lenguaje%20natural/Sesion%202/NLP_06b_Extraccio%CC%81n_caracteri%CC%81sticas_BoW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sChEkALUkwyG"
      },
      "source": [
        "# Extracción de características *Bag of Words*\n",
        "\n",
        "Primero importamos todas las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FfQ5hjMkwyU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import gensim\n",
        "\n",
        "pd.options.display.max_colwidth = None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQyYTixIkwyW"
      },
      "source": [
        "Creamos un pequeño cuerpo de textos de ejemplo *(CORPUS)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8AuycvBkwyX"
      },
      "source": [
        "corpus = ['El cielo es azul y bonito',\n",
        "          'Me encanta el cielo azul, pero no el cielo plomizo',\n",
        "          'Bonito cielo hacía ese día',\n",
        "          'Hoy he desayunado huevos con jamón y tostadas',\n",
        "          'Juan odia las tostadas y los huevos con jamón',\n",
        "          'las tostadas de jamón están muy buenas']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFsAzYDPkwyY"
      },
      "source": [
        "## Limpieza del texto\n",
        "Definimos una función simple de limpieza y normalización del texto y la aplicamos a nuestro corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D4suBl_kwya"
      },
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "def normalizar_doc(doc):\n",
        "    '''Función que normaliza un texto cogiendo sólo\n",
        "    las palabras en minúsculas mayores de 3 caracteres'''\n",
        "    # separamos en tokens\n",
        "    tokens = nlp(doc)\n",
        "    # filtramos stopwords\n",
        "    filtered_tokens = [t.lower_ for t in tokens if\n",
        "                       len(t.text)>3 and\n",
        "                       not t.is_space and\n",
        "                       not t.is_punct]\n",
        "    # juntamos de nuevo en una cadena\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvsh-G3gkwyd",
        "outputId": "afed40eb-94b3-4dac-c858-f30e5c0113b8"
      },
      "source": [
        "#probamos la función\n",
        "normalizar_doc(corpus[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cielo azul bonito'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDonEyi6kwyh",
        "outputId": "f84c3c05-7878-48e6-d620-3a91ffc9eaad"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'El cielo es azul y bonito'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvBrKG4Ikwyi",
        "outputId": "a917023b-011c-44de-c68b-83621f96e976"
      },
      "source": [
        "#aplicamos a todo el corpus\n",
        "norm_corpus = [normalizar_doc(doc) for doc in corpus]\n",
        "norm_corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cielo azul bonito',\n",
              " 'encanta cielo azul pero cielo plomizo',\n",
              " 'bonito cielo hacía',\n",
              " 'desayunado huevos jamón tostadas',\n",
              " 'juan odia tostadas huevos jamón',\n",
              " 'tostadas jamón están buenas']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WRvnEpLkwyj",
        "outputId": "802df59b-0bc5-4e8b-8691-546c7eff4388"
      },
      "source": [
        "#alternativamente\n",
        "list(map(normalizar_doc, corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cielo azul bonito',\n",
              " 'encanta cielo azul pero cielo plomizo',\n",
              " 'bonito cielo hacía',\n",
              " 'desayunado huevos jamón tostadas',\n",
              " 'juan odia tostadas huevos jamón',\n",
              " 'tostadas jamón están buenas']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FlAh310kwyk"
      },
      "source": [
        "# Librería `scikit-learn`\n",
        "Implementamos el modelo Bag-of-Word (BoW) con `scikit-learn`\n",
        "\n",
        "Contamos la frecuencia de aparición de los términos en cada documento, usando un vocabulario común. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmypPpTfkwym",
        "outputId": "46059e5c-2ff1-4835-d966-bc59b6e845a2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "cv.fit(norm_corpus) #también funcionaría cv.fit(map(normalizar_doc, corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sCYnOFekwyn",
        "outputId": "4e0d6f23-6c1d-4b79-df37-801066592712"
      },
      "source": [
        "type(cv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.feature_extraction.text.CountVectorizer"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jebPR6BWkwyo"
      },
      "source": [
        "El modelo genera un diccionario con todas las palabras del vocabulario y asigna un índice único a cada palabra:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVnNyDYlkwyo",
        "outputId": "df3f1dbc-85ae-43b4-ddd8-a7168e6d6927"
      },
      "source": [
        "cv.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['azul',\n",
              " 'bonito',\n",
              " 'buenas',\n",
              " 'cielo',\n",
              " 'desayunado',\n",
              " 'encanta',\n",
              " 'están',\n",
              " 'hacía',\n",
              " 'huevos',\n",
              " 'jamón',\n",
              " 'juan',\n",
              " 'odia',\n",
              " 'pero',\n",
              " 'plomizo',\n",
              " 'tostadas']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbT8K3QEkwyq",
        "outputId": "847142a4-14c2-4ae9-c500-a9097091d0ec"
      },
      "source": [
        "cv.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cielo': 3,\n",
              " 'azul': 0,\n",
              " 'bonito': 1,\n",
              " 'encanta': 5,\n",
              " 'pero': 12,\n",
              " 'plomizo': 13,\n",
              " 'hacía': 7,\n",
              " 'desayunado': 4,\n",
              " 'huevos': 8,\n",
              " 'jamón': 9,\n",
              " 'tostadas': 14,\n",
              " 'juan': 10,\n",
              " 'odia': 11,\n",
              " 'están': 6,\n",
              " 'buenas': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwR398qMkwyr"
      },
      "source": [
        "A partir del vocabulario aprendido, generamos el vector BoW de cada documento creando una matriz:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGc4Opu0kwyr"
      },
      "source": [
        "cv_matrix = cv.transform(norm_corpus)\n",
        "cv_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Rwz1-Xkwyu"
      },
      "source": [
        "#matriz sparse\n",
        "cv_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORiggkhSkwyv"
      },
      "source": [
        "#sólo guarda info de las celdas no vacías\n",
        "print(cv_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me7Y7c-ukwyw"
      },
      "source": [
        "cv_matrix = cv_matrix.toarray()\n",
        "cv_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fs5Nm_tkwyx"
      },
      "source": [
        "Cada término único es una característica de la matriz generada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClZkCqXJkwyz"
      },
      "source": [
        "# obtenemos palabras únicas en el corpus\n",
        "vocab = cv.get_feature_names()\n",
        "# mostramos vectores de características BoW del corpus\n",
        "pd.DataFrame(cv_matrix, columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xf26OCCkwyz"
      },
      "source": [
        "#id de las palabras del vocabulario\n",
        "cv.vocabulary_.get('cielo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkajD9YOkwyz"
      },
      "source": [
        "#si una palabra no está en el vocabulario...\n",
        "cv.vocabulary_.get('lluvia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MxcYmSikwy0"
      },
      "source": [
        "### Aplicando el modelo a nuevos documentos\n",
        "Cuando calculamos el vector BoW de un texto nuevo con el modelo no hay que volver a ajustar el vocabulario, por lo que los términos nuevos no se tendrán en cuenta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi-jkNN6kwy0"
      },
      "source": [
        "nuevo_corpus = ['El Cielo amenaza lluvia', 'Pedro desayuna tostadas de jamón con tomate']\n",
        "cv_matrix_nueva = cv.transform(map(normalizar_doc, nuevo_corpus))\n",
        "cv_matrix_nueva"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdi010hhkwy0"
      },
      "source": [
        "pd.DataFrame(cv_matrix_nueva.toarray(), columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwM-wIOJkwy1"
      },
      "source": [
        "### Modelos N-grams\n",
        "Considera como términos del vocabulario cada secuencia de N palabras consecutivas que aparece en el texto (*n-gramas*).  \n",
        "Por ejemplo para los *bigrams* del corpus (N=2):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kE_hsgJkwy1"
      },
      "source": [
        "bv = CountVectorizer(ngram_range=(2,2))\n",
        "bv_matrix = bv.fit_transform(norm_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOdwIN5Jkwy1"
      },
      "source": [
        "bv.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaWYCOFwkwy1"
      },
      "source": [
        "len(bv.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "javk30Z6kwy1"
      },
      "source": [
        "bv_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__ewVSLfkwy2"
      },
      "source": [
        "bv_matrix = bv_matrix.toarray()\n",
        "vocab_bigram = bv.get_feature_names()\n",
        "pd.DataFrame(bv_matrix, columns=vocab_bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spwbyodtkwy2"
      },
      "source": [
        "bv_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9r6ggI9kwy2"
      },
      "source": [
        "bv.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVGM6B-Lkwy2"
      },
      "source": [
        "Se puede establecer el rango de n-grams a `(1,2)` para obtener el conjunto de unigramas y bigramas del corpus.  \n",
        "Para limitar el número de términos en el vocabulario del modelo BoW se puede limitar a los términos que aparecen en un mínimo de documentos con el parámetro `min_df`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjOhW-7hkwy3"
      },
      "source": [
        "bv = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
        "bv_matrix = bv.fit_transform(norm_corpus)\n",
        "\n",
        "bv_matrix = bv_matrix.toarray()\n",
        "vocab_bigram = bv.get_feature_names()\n",
        "pd.DataFrame(bv_matrix, columns=vocab_bigram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfuEcFBkwy3"
      },
      "source": [
        "bv_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc0zYAMtkwy3"
      },
      "source": [
        "vocab_bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLi8hsxakwy4"
      },
      "source": [
        "### Ejercicio 1\n",
        "Aplica el modelo de BoW con bigramas al nuevo corpus de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtjJEcRpkwy4"
      },
      "source": [
        "#completar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHDAzy4Bkwy4"
      },
      "source": [
        "# Librería `Gensim`\n",
        "Para trabajar con la librería `Gensim` es necesario transformar los documentos en una lista de tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HejghHWekwy4"
      },
      "source": [
        "def word_tokenize(text):\n",
        "    return [token.text for token in nlp.make_doc(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvyN1eZokwy5"
      },
      "source": [
        "Convertimos nuestros texto de ejemplo en una lista de tokens y visualizamos el primer documento como ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WERZRfpkwy6"
      },
      "source": [
        "tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
        "tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHmayEkDkwy6"
      },
      "source": [
        "def normalizar_doc_tokenize(doc):\n",
        "    '''Función que normaliza un texto cogiendo sólo\n",
        "    las palabras en minúsculas mayores de 3 caracteres'''\n",
        "    # separamos en tokens\n",
        "    tokens = nlp(doc)\n",
        "    # filtramos stopwords\n",
        "    filtered_tokens = [t.lower_ for t in tokens if\n",
        "                       len(t.text)>3 and\n",
        "                       not t.is_space and\n",
        "                       not t.is_punct]\n",
        "\n",
        "    return filtered_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lao4YfkCkwy8"
      },
      "source": [
        "normalizar_doc_tokenize(corpus[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7BYXBa-kwy8"
      },
      "source": [
        "tokenized_corpus = [normalizar_doc_tokenize(doc) for doc in corpus]\n",
        "tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA-P-B09kwy8"
      },
      "source": [
        "## Modelo Bag of Words\n",
        "Se pasará al modelo de Gensim como:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2h53sCJkwy-"
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "diccionario = Dictionary(tokenized_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8LtxAQLkwy_"
      },
      "source": [
        "diccionario"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ8gdGwRkwzA"
      },
      "source": [
        "El ID de cada palabra del diccionario se obtiene con:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAfqIjmskwzB"
      },
      "source": [
        "diccionario.token2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLHP4rkkwzB"
      },
      "source": [
        "La librería `gensim` crea la matriz BoW con otro formato. A cada palabra distinta del corpus se le asigna un ID único. Por cada documento se genera una lista de tuplas (ID, frecuencia) con la frecuencia de aparición de cada palabra:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LONtG5jkkwzC"
      },
      "source": [
        "diccionario.doc2bow(tokenized_corpus[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZVKJiaRkwzC"
      },
      "source": [
        "diccionario.token2id['plomizo'] #ID de cada término"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyC8hzhLkwzC"
      },
      "source": [
        "diccionario[5] #término correspondiente a una ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCdfVghikwzC"
      },
      "source": [
        "diccionario.id2token #diccionario de palabras para cada ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdjct1VPkwzD"
      },
      "source": [
        "mapped_corpus = [diccionario.doc2bow(text)\n",
        "                 for text in tokenized_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn-E6y1kkwzD"
      },
      "source": [
        "mapped_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXe6tk6FkwzD"
      },
      "source": [
        "for (i, tf) in mapped_corpus[1]:\n",
        "    print(f\"{diccionario[i]}: {tf}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8D2juVPkwzD"
      },
      "source": [
        "#frec. de documentos de cada token\n",
        "diccionario.dfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW-F7BNkkwzE"
      },
      "source": [
        "for i in diccionario.dfs:\n",
        "    print(f\"{diccionario[i]}: {diccionario.dfs[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf1LWHu1kwzE"
      },
      "source": [
        "#frec. aparición total de cada token\n",
        "diccionario.cfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DmG_wuMkwzE"
      },
      "source": [
        "### Ejercicio 2\n",
        "Recorre el diccionario `cfs` mostrando el término correspondiente para cada ID y su frecuencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgiLP3LikwzE"
      },
      "source": [
        "#completar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKPfhtUfkwzF"
      },
      "source": [
        "## Aplicación de los modelos a nuevos textos\n",
        "Para aplicar un modelo BoW o TF-IDF a un nuevo documento hay que utilizar los modelos ya entrenados en `gensim` sobre el corpus original\n",
        "### Modelo BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvS9HfRvkwzG"
      },
      "source": [
        "tokenized_nuevo_corpus = [normalizar_doc_tokenize(doc) for doc in nuevo_corpus]\n",
        "\n",
        "mapped_nuevo_corpus = [diccionario.doc2bow(text)\n",
        "                 for text in tokenized_nuevo_corpus]\n",
        "\n",
        "mapped_nuevo_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFc6e7N8kwzG"
      },
      "source": [
        "tokenized_nuevo_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6DJL63AkwzG"
      },
      "source": [
        "for (i, tf) in mapped_nuevo_corpus[1]:\n",
        "    print(f\"{diccionario[i]}: {tf}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORUQ29gZkwzG"
      },
      "source": [
        "#Más pythonico con 'map'\n",
        "list(map(diccionario.doc2bow, map(normalizar_doc_tokenize, nuevo_corpus)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJAKWHXpkwzG"
      },
      "source": [
        "#o mejor incluso\n",
        "list(map(lambda x: diccionario.doc2bow(normalizar_doc_tokenize(x)), nuevo_corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGkLRSDkwzG"
      },
      "source": [
        "### Ejercicio 3\n",
        "Define una función que devuelva el BoW de una lista de nuevos textos (pasada como lista de *strings*) usando el diccionario y la función de normalización creadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YgefFBQkwzH"
      },
      "source": [
        "def bow(corpus, diccionario=diccionario, normalizacion=normalizar_doc_tokenize):\n",
        "    \"\"\"Genera la matriz BoW de la lista de texto en 'corpus'\n",
        "    usando el diccionario y la función de normalización\n",
        "    pasados como argumentos\"\"\"\n",
        "    \n",
        "    #COMPLETAR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QOPf6R4kwzH"
      },
      "source": [
        "bow(nuevo_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}