{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_08c-Clasificador binario-LSA.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/5%20Procesado%20del%20lenguaje%20natural/Sesion%203/NLP_08c_Clasificador_binario_LSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJtgDFmrmmVm"
      },
      "source": [
        "# Análisis de sentimientos con modelo LSA\n",
        "Vamos a utilizar un modelo de reducción de la dimensionalidad LSA para clasificar con conjunto de tweets en español\n",
        "\n",
        "## Carga y preparación de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoJlMAXpmmVy",
        "outputId": "2702cebb-bbcd-4c67-faa3-6e6cec00b3a0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Leemos los datos\n",
        "df = pd.read_csv('tweets_all.csv', index_col=None)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1514 entries, 0 to 1513\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   content   1514 non-null   object\n",
            " 1   polarity  1514 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 23.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIbNhT1hmmV1"
      },
      "source": [
        "Tenemos 1514 tweets, de los cuales hay 474 positivos y 637 negativos. El resto son neutros o no tienen polaridad.\n",
        "Vamos a entrenar sólo con los positivos y negativos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNC71j9bmmV2"
      },
      "source": [
        "df = df[(df['polarity']=='P') | (df['polarity']=='N')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYY72ftBmmV3"
      },
      "source": [
        "El conjunto no está balanceado pero casi.\n",
        "Quitamos las columnas que no usamos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsmbibXKmmV4",
        "outputId": "96dc8812-c17b-47b9-fa7b-53afd4515e86"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N    637\n",
              "P    474\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maKYWpglmmV5"
      },
      "source": [
        "## Limpieza de texto\n",
        "Hacemos un pequeño pre-procesado del texto antes de extraer las características:  \n",
        "- Quitamos las menciones y las URL del texto porque no aportan valor para el análisis de sentimientos.\n",
        "- Los hashtag sí que pueden aportar valor así que simplemente quitamos el #.\n",
        "- Quitamos los signos de puntuación y palabras menores de 3 caracteres.\n",
        "- Por último quitamos todos los símbolos de puntuación del texto (que forman parte de un token).\n",
        "- Lematizamos el texto y lo guardamos en otra columna para comparar resultados del clasificador. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBqZzcYmmV5",
        "outputId": "f827b0f3-1ffc-4bf0-c09f-94dedac5eea0"
      },
      "source": [
        "import re, string, spacy\n",
        "nlp=spacy.load('es_core_news_md')\n",
        "\n",
        "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
        "\n",
        "def clean_text(text, lemas=False):\n",
        "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos en tokens\n",
        "    y eliminamos signos de puntuación.\n",
        "    Si lemas=True extraemos el lema, si no dejamos en minúsculas solamente.\n",
        "    Como salida volvemos a convertir los tokens en cadena de texto\"\"\"\n",
        "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text) #elimina menciones y URL\n",
        "    tokens = nlp(text)\n",
        "    tokens = [tok.lemma_.lower() if lemas else tok.lower_ for tok in tokens if not tok.is_punct]\n",
        "    filtered_tokens = [pattern2.sub('', tok) for tok in tokens] #no quitamos stop-words\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return filtered_text\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'es_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b5d109f38278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpattern2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#elimina símbolos de puntuación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, disable, exclude, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_md'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Wae6iVmmV6"
      },
      "source": [
        "Aplicamos limpieza a todos los tweets del Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DG_v_o_mmV8"
      },
      "source": [
        "df['limpio']=df.content.apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PShqw3EBmmV8"
      },
      "source": [
        "#Quitamos tweets vacíos después de la limpieza\n",
        "df=df[df.content!='']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msU2Jwf6mmV-"
      },
      "source": [
        "df[\"lemas\"]=df.content.apply(clean_text, lemas=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-gZe9aGmmV-"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxvTOmOBmmV-"
      },
      "source": [
        "### Clasificador\n",
        "Vamos a usar la librería scikit-learn para aplicar un clasificador sobre la polaridad. Las características de cada Tweet las extraemos en una matriz TF-IDF y luego aplicamos una reducción de dimensiones con un modelo LSA de *topic modeling*.  \n",
        "\n",
        "Primero dividimos en conjunto de entrenamiento y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWkEgDW9mmV_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividmos entre conjunto de entrenamiento y test\n",
        "# Asignamos un 70% a training y un 30% a test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['limpio'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfjPZbXymmV_"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#definimos una función para entrenar y validar cada clasificador\n",
        "def train_predict_evaluate_model(classifier, \n",
        "                                 train_features, train_labels, \n",
        "                                 test_features, test_labels):\n",
        "    # build model    \n",
        "    classifier.fit(train_features, train_labels)\n",
        "    # predict using model\n",
        "    predictions = classifier.predict(test_features) \n",
        "    # evaluate model prediction performance   \n",
        "    print(classification_report(test_labels, predictions))\n",
        "    return predictions \n",
        "\n",
        "# creamos los modelos\n",
        "modelos = [('Logistic Regression', LogisticRegression()),\n",
        "           ('Naive Bayes', GaussianNB()),\n",
        "           ('Linear SVM', SGDClassifier(loss='hinge', max_iter=10000, tol=1e-5)),\n",
        "           ('RFB VSM', SVC(gamma='scale', C=2))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOpDWC3CmmWA"
      },
      "source": [
        "### Modelo LSA\n",
        "Creamos un procesado `pipeline` para calcular la matriz TF-IDF y a partir de esta el modelo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTx5sgnhmmWA"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "vect = TfidfVectorizer()\n",
        "svd = TruncatedSVD(n_components=500)\n",
        "\n",
        "modelo = make_pipeline(vect, svd, Normalizer(copy=False))\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "lsa_train = modelo.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAjllv0hmmWA"
      },
      "source": [
        "lsa_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmD_yGKnmmWB"
      },
      "source": [
        "lsa_test = modelo.transform(X_test)\n",
        "lsa_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy6TIjClmmWB"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "modelos = [('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
        "           ('Naive Bayes', GaussianNB()),\n",
        "           ('Linear SVM', SGDClassifier(loss='hinge', max_iter=10000, tol=1e-5)),\n",
        "           ('RFB SVM', SVC(gamma='scale', C=2))]\n",
        "\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con características LSA 500 dims'.format(m))\n",
        "    #entrenamos sobre train\n",
        "    clf.fit(lsa_train, y_train)\n",
        "    # Predecimos sobre el conjunto de test\n",
        "    prediccion = clf.predict(lsa_test)\n",
        "    print(classification_report(y_test, prediccion))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2yGSe5SmmWB"
      },
      "source": [
        "Una vez establecido el modelo de clasificación a utilizar podemos integrar todos los paso en el `pipeline`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw-a3aDSmmWB"
      },
      "source": [
        "vect = TfidfVectorizer()\n",
        "svd = TruncatedSVD(n_components=500)\n",
        "modelSVM = SVC(gamma='scale', C=2)\n",
        "\n",
        "modelo = make_pipeline(vect, svd, Normalizer(copy=False), modelSVM)\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelo.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovrdRoeEmmWB"
      },
      "source": [
        "prediccion = modelo.predict(X_test)\n",
        "print(classification_report(y_test, prediccion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZJB4unLmmWB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}