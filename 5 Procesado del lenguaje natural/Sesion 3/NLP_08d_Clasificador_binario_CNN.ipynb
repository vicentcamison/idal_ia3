{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_08d-Clasificador binario-CNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicentcamison/idal_ia3/blob/main/5%20Procesado%20del%20lenguaje%20natural/Sesion%203/NLP_08d_Clasificador_binario_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isRWjj7km1X-"
      },
      "source": [
        "# Clasificador de texto con CNN\n",
        "Implementemos un clasificador con Convolutional Neural Networks aplicado al análisis de sentimiento en Twitter usando la librería `Keras`.  \n",
        "Aplicamos una primera capa de embeddings para convertir las palabras en vectores y luego entrenamos con una red CNN (seleccionando el max-pooling de cada filtro para obtener un vector por tweet).  \n",
        "Para calcular los embeddings usamos:  \n",
        "- Una capa de embeddings propia sobre los tweets\n",
        "- Transfer Learning con los word embeddings de spaCy \n",
        "- Transfer Learning con los word embeddings de GloVe \n",
        "\n",
        "Implementado según el modelo planteado en [Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1408.5882)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6FUfyh-m1YK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string, spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Dropout, Activation, Conv1D, GlobalMaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.display.max_colwidth = None\n",
        "np.random.seed(123)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke3qQOSFm1YN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4e9e112f-d650-4441-a129-47dea2d24d0b"
      },
      "source": [
        "# Leemos los datos\n",
        "#LOCAL:\n",
        "#df = pd.read_csv('tweets_all.csv', index_col=None)\n",
        "#GITHUB:\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/vicentcamison/idal_ia3/main/5%20Procesado%20del%20lenguaje%20natural/Sesion%203/tweets_all.csv', index_col=None)\n",
        "\n",
        "#seleccionamos columnas de interés\n",
        "df = df[['content', 'polarity']]\n",
        "\n",
        "#dejamos polaridades definidas\n",
        "df = df[(df['polarity']=='P') | (df['polarity']=='N')]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@myendlesshazza a. que puto mal escribo\\n\\nb. me sigo surrando help \\n\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vale he visto la tia bebiendose su regla y me hs dado muchs grima</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@Yulian_Poe @guillermoterry1 Ah. mucho más por supuesto! solo que lo incluyo. Me habías entendido mal</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                 content polarity\n",
              "1  @myendlesshazza a. que puto mal escribo\\n\\nb. me sigo surrando help \\n\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA        N\n",
              "2                     @estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero         N\n",
              "3                                Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps         P\n",
              "4                                                     Vale he visto la tia bebiendose su regla y me hs dado muchs grima         N\n",
              "5                 @Yulian_Poe @guillermoterry1 Ah. mucho más por supuesto! solo que lo incluyo. Me habías entendido mal         P"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYzV9CwTm1YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91fb610-9326-4376-c236-c7c028ab2227"
      },
      "source": [
        "df.polarity.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N    637\n",
              "P    474\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHvSoL1em1YU"
      },
      "source": [
        "## Limpieza de texto\n",
        "Usamos Spacy para separar el texto en tokens y mantener sólo las palabras importantes, dejando su lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_XfbTx05-L2",
        "outputId": "17c4ecf8-9bcb-44a6-f62a-3ac4ac90a8b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#(ejecutar esto sólo si estás en Colab)\n",
        "!python -m spacy download es_core_news_md"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.2.5/es_core_news_md-2.2.5.tar.gz#egg=es_core_news_md==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_md==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrqsNoVtm1YU"
      },
      "source": [
        "import re, string, spacy\n",
        "nlp=spacy.load('es_core_news_md')\n",
        "\n",
        "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
        "\n",
        "def clean_text(text, lemas=False):\n",
        "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos en tokens\n",
        "    y eliminamos signos de puntuación.\n",
        "    Si lemas=True extraemos el lema, si no dejamos en minúsculas solamente.\n",
        "    Como salida volvemos a convertir los tokens en cadena de texto\"\"\"\n",
        "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text) #elimina menciones y URL\n",
        "    tokens = nlp(text)\n",
        "    tokens = [tok.lemma_.lower() if lemas else tok.lower_ for tok in tokens if not tok.is_punct]\n",
        "    filtered_tokens = [pattern2.sub('', tok) for tok in tokens] #no quitamos stop-words\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return filtered_text\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV-9QfvLm1YV"
      },
      "source": [
        "## Preparamos el conjunto de datos\n",
        "Convertimos el texto en *tokens* y asignamos una ID numérica a cada token.  \n",
        "Convertimos a secuencias de longitud fija.  \n",
        "La longitud de la secuencia viene dada por la longitud en tokens del tweet más largo. Sólo se conservan los tokens de las palabras en el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d212RKpO9xKD",
        "outputId": "b31646c3-f186-4e05-c495-9f9c6319117d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tweets_train.iloc[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  estar enamorar y a vez me hacer dañar porque se ir a chupar culo de otro perro y no me decir nadar   crisis'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqrIC6MHBKu1",
        "outputId": "dcb7bc2a-2eff-47d7-becc-ec26224d29b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train[0]\n",
        "X_train[1]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  5,  46,   1,  26, 806,  11, 807,   2, 492,   3, 808,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmts0-j9m1YW"
      },
      "source": [
        "#limpiamos texto y quitamos tweets que se han quedado vacíos\n",
        "df.content=df.content.apply(clean_text, lemas=True)\n",
        "df = df[df['content']!='']\n",
        "\n",
        "#el conjunto de salida es la polaridad, hay que convertir a numérico para Keras\n",
        "#codificamos 'P' como 1 y 'N' se queda como 0\n",
        "Y=(df.polarity=='P').values*1\n",
        "\n",
        "#Separamos entrenamiento y test\n",
        "tweets_train, tweets_test, Y_train, Y_test = train_test_split(df.content,Y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFP8pygCm1YX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac2914d-13a9-45e5-fe2b-a3ef10d906e0"
      },
      "source": [
        "tokenizer = Tokenizer(split=' ')\n",
        "tokenizer.fit_on_texts(tweets_train.values)\n",
        "X_train = tokenizer.texts_to_sequences(tweets_train.values)\n",
        "X_train = pad_sequences(X_train, padding='post')\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Número de tokens distintos: {len(word_index)}')\n",
        "MAX_SEQUENCE_LENGTH = X_train.shape[1]\n",
        "max_features = len(word_index)+1\n",
        "X_test = tokenizer.texts_to_sequences(tweets_test.values)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de tokens distintos: 2606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4AG7tvim1YX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb8d34e-78f8-44b4-9a93-406839205ed7"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(777, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlkmKD_dm1YY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac8e2ca-9eb0-4fe6-f8f4-173e72a3e82c"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 15, 375,   6,   8,  61,   9,  26, 490,  37,  18,  22,   8, 804,\n",
              "       491,   2,  64, 216,   6,   7,   9,  40,  65, 805,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhSmMZvtm1Ya"
      },
      "source": [
        "tweets_train.values[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOhVsMTBm1Ya"
      },
      "source": [
        "word_index['después']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1k1ND-9m1Yb"
      },
      "source": [
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLW2R67Nm1Yb"
      },
      "source": [
        "## Word embeddins propios\n",
        "Entrenamos una capa de embedding para aprender los WE con los textos de nuestro problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R96rc5Wm1Yc"
      },
      "source": [
        "#Creamos el modelo CNN en Keras\n",
        "#Usamos como referencia el ejemplo de Keras: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
        "#pero quitamos la capa oculta intermedia para simplificar el modelo y dejarlo como en el artículo\n",
        "\n",
        "#Parámetros de la red\n",
        "embed_dim = 50\n",
        "filters = 64\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim, input_length = MAX_SEQUENCE_LENGTH))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# añadimos una capa de convolución 1D que aprende\n",
        "# filtros de grupos de palabras de tamaño kernel_size\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "\n",
        "# calculamos el max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# conectamos a una capa de salida de una unidad con activación sigmoide\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# compilamos el modelo\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6MC-Ryom1Yd"
      },
      "source": [
        "### Pregunta:\n",
        "¿de dónde vienen los tamaños de los parámetros de cada capa?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXRqlqKIm1Ye"
      },
      "source": [
        "#solución\n",
        "print(max_features * embed_dim)\n",
        "print(filters*embed_dim*kernel_size+filters)\n",
        "print(filters+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITjTPA9Xm1Yf"
      },
      "source": [
        "batch_size = 16\n",
        "history = model.fit(X_train, Y_train, epochs=20, batch_size=batch_size, verbose=2, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF-Ish42m1Yf"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('WE propios')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sY9GYg1m1Yf"
      },
      "source": [
        "#los word embeddings aprendidos son los pesos de la primera capa\n",
        "embeddings=model.get_weights()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFRGuDqm1Yg"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFa5jSOJm1Yg"
      },
      "source": [
        "embeddings[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgu8Seqbm1Yg"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG4lacBpm1Yh"
      },
      "source": [
        "predict=model.predict(X_test, batch_size=1)\n",
        "prediccion=(predict>0.5).tolist()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0spPUPXm1Yh"
      },
      "source": [
        "### Ejercicio 1\n",
        "Introduce una capa densa de 50 neuronas, con un Dropout con p=0.4 y una función de activación 'ReLU' entre la salida de la capa convolucional (después del MaxPooling) y la capa de salida. Compila y entrena con un tamaño de batch de 16 y 20 épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH-zluONm1Yi"
      },
      "source": [
        "# Solución\n",
        "#Creamos el modelo CNN en Keras\n",
        "#Usamos como referencia el ejemplo de Keras: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
        "#pero quitamos la capa oculta intermedia para simplificar el modelo y dejarlo como en el artículo\n",
        "\n",
        "#Parámetros de la red\n",
        "embed_dim = 50\n",
        "filters = 64\n",
        "kernel_size = 3\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim, input_length = MAX_SEQUENCE_LENGTH))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# añadimos una capa de convolución 1D que aprende\n",
        "# filtros de grupos de palabras de tamaño kernel_size\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "\n",
        "# calculamos el max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# conectamos a una capa de salida de una unidad con activación sigmoide\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# compilamos el modelo\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7-TQLwXm1Yj"
      },
      "source": [
        "Compara los resultados con los anteriores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYwmWA0Zm1Yj"
      },
      "source": [
        "batch_size = 16\n",
        "history = model.fit(X_train, Y_train, epochs=20, batch_size=batch_size, verbose=2, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I2xfYEem1Yj"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyRPhKdTm1Yj"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('WE propios 2 capas')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cADEO2aFm1Yk"
      },
      "source": [
        "predict=model.predict(X_test, batch_size=1)\n",
        "prediccion=(predict>0.5).tolist()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYaNdn2Ym1Yk"
      },
      "source": [
        "## Word embeddings de spaCy\n",
        "Aplicamos Transfer Learning usando los embeddings GloVe incluidos en el modelo de spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dCP6-CJm1Yl"
      },
      "source": [
        "nlp=spacy.load('es_core_news_md')\n",
        "#Rellenamos los vectores con el valor en spaCy para nuestro vocabulario\n",
        "EMBEDDING_DIM = nlp.vocab.vectors_length\n",
        "embedding_matrix = np.zeros((max_features, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if(i>max_features):\n",
        "        break\n",
        "    embedding_vector = nlp.vocab[word].vector\n",
        "    if embedding_vector is not None:\n",
        "        # las palabras que no están en spaCy serán cero.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v91D_aZQm1Ym"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZ6UEf9m1Ym"
      },
      "source": [
        "#Creamos el modelo CNN en Keras\n",
        "\n",
        "#parámetros de la red\n",
        "filters = 64\n",
        "kernel_size = 3\n",
        "\n",
        "embedding_layer = Embedding(max_features,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq1gJPaDm1Ym"
      },
      "source": [
        "batch_size = 16\n",
        "history = model.fit(X_train, Y_train, epochs=20, batch_size=batch_size, verbose=2, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49864FO2m1Yn"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('TF con WE spaCy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7swNmJkm1Yn"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-uRtOw3m1Yn"
      },
      "source": [
        "predict=model.predict(X_test, batch_size=1)\n",
        "prediccion=(predict>0.5).tolist()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLdT8oGm1Yn"
      },
      "source": [
        "### Word embeddings de FastText\n",
        "Podemos usar cualquier conjunto de *word embeddings* con el formato `KeyedVectors` de Gensim para hacer Transfer Learning.  \n",
        "WE descargados desde https://fasttext.cc/docs/en/crawl-vectors.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVAVR65rm1Yo"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "modelWE = KeyedVectors.load_word2vec_format('/Users/jovifran/Downloads/fasttext-sbwc.100k.vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNGTWqK-m1Yo"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "#https://github.com/mquezada/starsconf2018-word-embeddings\n",
        "modelWE = KeyedVectors.load_word2vec_format('~/Downloads/fasttext-sbwc.100k.vec')\n",
        "\n",
        "EMBEDDING_DIM = modelWE.vector_size\n",
        "\n",
        "embedding_matrix = np.zeros((max_features, EMBEDDING_DIM))\n",
        "vectores = 0\n",
        "for word, i in word_index.items():\n",
        "    if(i<max_features):\n",
        "        try:\n",
        "            embedding_vector = modelWE[word]\n",
        "        except:\n",
        "            embedding_vector = np.zeros(EMBEDDING_DIM)\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        vectores += 1\n",
        "        \n",
        "print(\"Cargados {} vectores en la matriz\".format(vectores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJVnL695m1Yo"
      },
      "source": [
        "max_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgegNivkm1Yp"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2urjHZrm1Yp"
      },
      "source": [
        "#Creamos el modelo CNN en Keras\n",
        "\n",
        "#parámetros de la red\n",
        "filters = 64\n",
        "kernel_size = 3\n",
        "\n",
        "embedding_layer = Embedding(max_features,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RY81JFXm1Yp"
      },
      "source": [
        "batch_size = 16\n",
        "history = model.fit(X_train, Y_train, epochs=20, batch_size=batch_size, verbose=2, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68zWy5EYm1Yp"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('TF con WE FastText')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA9pjK47m1Yq"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7yiBWidm1Yq"
      },
      "source": [
        "predict=model.predict(X_test, batch_size=1)\n",
        "prediccion=(predict>0.5).tolist()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(Y_test, prediccion, target_names=['N','P']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1FXatP1m1Ys"
      },
      "source": [
        "### Inferencia en nuevos textos\n",
        "\n",
        "Si queremos utilizar el clasificador con un texto nuevo hay que procesar el texto con la misma secuencia: limpieza, tokenizado y conversión en secuencia de la longitud adecuada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnsBg0tlm1Ys"
      },
      "source": [
        "twt = 'estoy triste con el partido'\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt = tokenizer.texts_to_sequences([clean_text(twt, lemas=True)]) #hay que pasar el texto a array\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OprP5Z-hm1Ys"
      },
      "source": [
        "twt = pad_sequences(twt, maxlen=X_train.shape[1], dtype='int32', padding='post', truncating='post', value=0)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)\n",
        "if(np.round(sentiment) == 0):\n",
        "    print(\"negativo\")\n",
        "elif (np.round(sentiment) == 1):\n",
        "    print(\"positivo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxfxU3Vwm1Yt"
      },
      "source": [
        "### Ejercicio 2\n",
        "Repite el entrenamiento con la capa de embeddings de spaCy con `trainable = True` y compara los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3T_90Sim1Yt"
      },
      "source": [
        "## Solución"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}